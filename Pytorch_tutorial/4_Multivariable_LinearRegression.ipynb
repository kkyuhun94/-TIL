{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x114d7d190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 w1: 0.294 w2: 0.297 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
      "Epoch  100/1000 w1: 0.674 w2: 0.676 w3: 0.676 b: 0.008 Cost: 1.563634\n",
      "Epoch  200/1000 w1: 0.679 w2: 0.677 w3: 0.677 b: 0.008 Cost: 1.497607\n",
      "Epoch  300/1000 w1: 0.684 w2: 0.677 w3: 0.677 b: 0.008 Cost: 1.435026\n",
      "Epoch  400/1000 w1: 0.689 w2: 0.678 w3: 0.678 b: 0.008 Cost: 1.375730\n",
      "Epoch  500/1000 w1: 0.694 w2: 0.678 w3: 0.678 b: 0.009 Cost: 1.319511\n",
      "Epoch  600/1000 w1: 0.699 w2: 0.679 w3: 0.679 b: 0.009 Cost: 1.266222\n",
      "Epoch  700/1000 w1: 0.704 w2: 0.679 w3: 0.679 b: 0.009 Cost: 1.215696\n",
      "Epoch  800/1000 w1: 0.709 w2: 0.679 w3: 0.679 b: 0.009 Cost: 1.167818\n",
      "Epoch  900/1000 w1: 0.713 w2: 0.680 w3: 0.680 b: 0.009 Cost: 1.122429\n",
      "Epoch 1000/1000 w1: 0.718 w2: 0.680 w3: 0.680 b: 0.009 Cost: 1.079378\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, w1.item(), w3.item(), w3.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost: 9298.520508\n",
      "Epoch    2/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost: 2915.712402\n",
      "Epoch    3/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost: 915.040527\n",
      "Epoch    4/20 hypothesis: tensor([137.7968, 165.6247, 163.1911, 177.7112, 126.3307]) Cost: 287.936005\n",
      "Epoch    5/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost: 91.371010\n",
      "Epoch    6/20 hypothesis: tensor([148.1035, 178.0144, 175.3980, 191.0042, 135.7812]) Cost: 29.758139\n",
      "Epoch    7/20 hypothesis: tensor([150.1744, 180.5042, 177.8508, 193.6753, 137.6805]) Cost: 10.445305\n",
      "Epoch    8/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost: 4.391228\n",
      "Epoch    9/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost: 2.493135\n",
      "Epoch   10/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost: 1.897688\n",
      "Epoch   11/20 hypothesis: tensor([152.5485, 183.3610, 180.6640, 196.7389, 139.8602]) Cost: 1.710541\n",
      "Epoch   12/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost: 1.651412\n",
      "Epoch   13/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost: 1.632387\n",
      "Epoch   14/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost: 1.625923\n",
      "Epoch   15/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost: 1.623412\n",
      "Epoch   16/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost: 1.622141\n",
      "Epoch   17/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost: 1.621253\n",
      "Epoch   18/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0662, 140.0963]) Cost: 1.620500\n",
      "Epoch   19/20 hypothesis: tensor([152.8014, 183.6715, 180.9666, 197.0686, 140.0985]) Cost: 1.619770\n",
      "Epoch   20/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.1000]) Cost: 1.619033\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train.matmul(W) + b # or .mm or @\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level Implementation with nn.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# nn모듈을 사용\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1) # 입력 3, 출력 1\n",
    "\n",
    "    def forward(self, x):         # hypothesis 계산 : forward()에서, backward는 알아서 해줌 \n",
    "        return self.linear(x)\n",
    "\n",
    "model = MultivariateLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer, # (model.parameters(), lr)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 73.,  80.,  75.],\n",
       "        [ 93.,  88.,  93.],\n",
       "        [ 89.,  91.,  90.],\n",
       "        [ 96.,  98., 100.],\n",
       "        [ 73.,  66.,  70.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/200 hypothesis : tensor([-20.5930, -30.8318, -27.2626, -29.8314, -24.8466]) Cost: 39633.414062\n",
      "Epoch    1/200 hypothesis : tensor([57.1418, 62.6022, 64.7982, 70.4206, 46.4205]) Cost: 12431.273438\n",
      "Epoch    2/200 hypothesis : tensor([100.6621, 114.9128, 116.3395, 126.5479,  86.3209]) Cost: 3904.840332\n",
      "Epoch    3/200 hypothesis : tensor([125.0270, 144.2000, 145.1954, 157.9714, 108.6602]) Cost: 1232.251709\n",
      "Epoch    4/200 hypothesis : tensor([138.6674, 160.5973, 161.3506, 175.5641, 121.1676]) Cost: 394.532684\n",
      "Epoch    5/200 hypothesis : tensor([146.3035, 169.7779, 170.3951, 185.4135, 128.1707]) Cost: 131.948273\n",
      "Epoch    6/200 hypothesis : tensor([150.5782, 174.9182, 175.4586, 190.9277, 132.0919]) Cost: 49.637566\n",
      "Epoch    7/200 hypothesis : tensor([152.9708, 177.7964, 178.2933, 194.0147, 134.2878]) Cost: 23.833128\n",
      "Epoch    8/200 hypothesis : tensor([154.3098, 179.4082, 179.8802, 195.7429, 135.5177]) Cost: 15.740547\n",
      "Epoch    9/200 hypothesis : tensor([155.0588, 180.3111, 180.7684, 196.7103, 136.2069]) Cost: 13.199559\n",
      "Epoch   10/200 hypothesis : tensor([155.4776, 180.8169, 181.2656, 197.2518, 136.5932]) Cost: 12.398743\n",
      "Epoch   11/200 hypothesis : tensor([155.7115, 181.1005, 181.5437, 197.5548, 136.8101]) Cost: 12.143396\n",
      "Epoch   12/200 hypothesis : tensor([155.8419, 181.2597, 181.6993, 197.7243, 136.9320]) Cost: 12.059056\n",
      "Epoch   13/200 hypothesis : tensor([155.9143, 181.3492, 181.7862, 197.8191, 137.0008]) Cost: 12.028257\n",
      "Epoch   14/200 hypothesis : tensor([155.9542, 181.3997, 181.8347, 197.8720, 137.0398]) Cost: 12.014270\n",
      "Epoch   15/200 hypothesis : tensor([155.9760, 181.4284, 181.8616, 197.9015, 137.0622]) Cost: 12.005560\n",
      "Epoch   16/200 hypothesis : tensor([155.9876, 181.4448, 181.8765, 197.9178, 137.0753]) Cost: 11.998510\n",
      "Epoch   17/200 hypothesis : tensor([155.9935, 181.4544, 181.8847, 197.9268, 137.0831]) Cost: 11.991979\n",
      "Epoch   18/200 hypothesis : tensor([155.9963, 181.4603, 181.8891, 197.9318, 137.0881]) Cost: 11.985615\n",
      "Epoch   19/200 hypothesis : tensor([155.9973, 181.4639, 181.8914, 197.9344, 137.0914]) Cost: 11.979303\n",
      "Epoch   20/200 hypothesis : tensor([155.9972, 181.4663, 181.8925, 197.9357, 137.0937]) Cost: 11.973015\n",
      "Epoch   21/200 hypothesis : tensor([155.9966, 181.4681, 181.8930, 197.9363, 137.0956]) Cost: 11.966729\n",
      "Epoch   22/200 hypothesis : tensor([155.9957, 181.4694, 181.8930, 197.9365, 137.0972]) Cost: 11.960443\n",
      "Epoch   23/200 hypothesis : tensor([155.9946, 181.4706, 181.8929, 197.9365, 137.0986]) Cost: 11.954148\n",
      "Epoch   24/200 hypothesis : tensor([155.9935, 181.4717, 181.8927, 197.9364, 137.0999]) Cost: 11.947878\n",
      "Epoch   25/200 hypothesis : tensor([155.9922, 181.4727, 181.8923, 197.9361, 137.1012]) Cost: 11.941603\n",
      "Epoch   26/200 hypothesis : tensor([155.9909, 181.4736, 181.8920, 197.9359, 137.1024]) Cost: 11.935335\n",
      "Epoch   27/200 hypothesis : tensor([155.9896, 181.4745, 181.8916, 197.9356, 137.1036]) Cost: 11.929104\n",
      "Epoch   28/200 hypothesis : tensor([155.9883, 181.4754, 181.8912, 197.9353, 137.1048]) Cost: 11.922812\n",
      "Epoch   29/200 hypothesis : tensor([155.9870, 181.4763, 181.8908, 197.9350, 137.1060]) Cost: 11.916579\n",
      "Epoch   30/200 hypothesis : tensor([155.9857, 181.4773, 181.8904, 197.9347, 137.1073]) Cost: 11.910291\n",
      "Epoch   31/200 hypothesis : tensor([155.9844, 181.4782, 181.8900, 197.9344, 137.1084]) Cost: 11.904028\n",
      "Epoch   32/200 hypothesis : tensor([155.9831, 181.4791, 181.8896, 197.9341, 137.1097]) Cost: 11.897758\n",
      "Epoch   33/200 hypothesis : tensor([155.9818, 181.4800, 181.8892, 197.9338, 137.1109]) Cost: 11.891546\n",
      "Epoch   34/200 hypothesis : tensor([155.9805, 181.4809, 181.8888, 197.9334, 137.1120]) Cost: 11.885311\n",
      "Epoch   35/200 hypothesis : tensor([155.9792, 181.4818, 181.8884, 197.9331, 137.1132]) Cost: 11.879084\n",
      "Epoch   36/200 hypothesis : tensor([155.9778, 181.4827, 181.8880, 197.9328, 137.1144]) Cost: 11.872824\n",
      "Epoch   37/200 hypothesis : tensor([155.9766, 181.4836, 181.8876, 197.9325, 137.1156]) Cost: 11.866629\n",
      "Epoch   38/200 hypothesis : tensor([155.9753, 181.4845, 181.8873, 197.9322, 137.1169]) Cost: 11.860384\n",
      "Epoch   39/200 hypothesis : tensor([155.9739, 181.4854, 181.8868, 197.9319, 137.1180]) Cost: 11.854124\n",
      "Epoch   40/200 hypothesis : tensor([155.9726, 181.4863, 181.8864, 197.9316, 137.1192]) Cost: 11.847937\n",
      "Epoch   41/200 hypothesis : tensor([155.9713, 181.4872, 181.8861, 197.9313, 137.1204]) Cost: 11.841735\n",
      "Epoch   42/200 hypothesis : tensor([155.9700, 181.4881, 181.8857, 197.9310, 137.1216]) Cost: 11.835442\n",
      "Epoch   43/200 hypothesis : tensor([155.9687, 181.4890, 181.8853, 197.9306, 137.1228]) Cost: 11.829267\n",
      "Epoch   44/200 hypothesis : tensor([155.9674, 181.4899, 181.8849, 197.9303, 137.1240]) Cost: 11.823064\n",
      "Epoch   45/200 hypothesis : tensor([155.9661, 181.4908, 181.8845, 197.9300, 137.1252]) Cost: 11.816870\n",
      "Epoch   46/200 hypothesis : tensor([155.9648, 181.4917, 181.8841, 197.9297, 137.1264]) Cost: 11.810653\n",
      "Epoch   47/200 hypothesis : tensor([155.9635, 181.4925, 181.8837, 197.9294, 137.1276]) Cost: 11.804495\n",
      "Epoch   48/200 hypothesis : tensor([155.9622, 181.4934, 181.8833, 197.9291, 137.1288]) Cost: 11.798292\n",
      "Epoch   49/200 hypothesis : tensor([155.9609, 181.4944, 181.8829, 197.9288, 137.1300]) Cost: 11.792064\n",
      "Epoch   50/200 hypothesis : tensor([155.9596, 181.4952, 181.8825, 197.9285, 137.1312]) Cost: 11.785908\n",
      "Epoch   51/200 hypothesis : tensor([155.9583, 181.4961, 181.8821, 197.9282, 137.1324]) Cost: 11.779722\n",
      "Epoch   52/200 hypothesis : tensor([155.9570, 181.4970, 181.8817, 197.9279, 137.1336]) Cost: 11.773572\n",
      "Epoch   53/200 hypothesis : tensor([155.9557, 181.4979, 181.8813, 197.9276, 137.1348]) Cost: 11.767378\n",
      "Epoch   54/200 hypothesis : tensor([155.9543, 181.4988, 181.8809, 197.9272, 137.1360]) Cost: 11.761144\n",
      "Epoch   55/200 hypothesis : tensor([155.9531, 181.4997, 181.8805, 197.9269, 137.1372]) Cost: 11.755028\n",
      "Epoch   56/200 hypothesis : tensor([155.9518, 181.5006, 181.8801, 197.9266, 137.1384]) Cost: 11.748831\n",
      "Epoch   57/200 hypothesis : tensor([155.9505, 181.5015, 181.8797, 197.9263, 137.1396]) Cost: 11.742681\n",
      "Epoch   58/200 hypothesis : tensor([155.9491, 181.5024, 181.8793, 197.9260, 137.1408]) Cost: 11.736500\n",
      "Epoch   59/200 hypothesis : tensor([155.9479, 181.5033, 181.8789, 197.9257, 137.1420]) Cost: 11.730381\n",
      "Epoch   60/200 hypothesis : tensor([155.9465, 181.5042, 181.8785, 197.9254, 137.1431]) Cost: 11.724192\n",
      "Epoch   61/200 hypothesis : tensor([155.9453, 181.5051, 181.8781, 197.9251, 137.1443]) Cost: 11.718084\n",
      "Epoch   62/200 hypothesis : tensor([155.9440, 181.5060, 181.8777, 197.9248, 137.1455]) Cost: 11.711910\n",
      "Epoch   63/200 hypothesis : tensor([155.9427, 181.5069, 181.8773, 197.9244, 137.1467]) Cost: 11.705760\n",
      "Epoch   64/200 hypothesis : tensor([155.9414, 181.5078, 181.8769, 197.9241, 137.1479]) Cost: 11.699600\n",
      "Epoch   65/200 hypothesis : tensor([155.9401, 181.5086, 181.8766, 197.9238, 137.1491]) Cost: 11.693489\n",
      "Epoch   66/200 hypothesis : tensor([155.9388, 181.5095, 181.8761, 197.9235, 137.1503]) Cost: 11.687338\n",
      "Epoch   67/200 hypothesis : tensor([155.9375, 181.5104, 181.8758, 197.9232, 137.1515]) Cost: 11.681249\n",
      "Epoch   68/200 hypothesis : tensor([155.9362, 181.5113, 181.8754, 197.9229, 137.1527]) Cost: 11.675077\n",
      "Epoch   69/200 hypothesis : tensor([155.9349, 181.5122, 181.8750, 197.9226, 137.1539]) Cost: 11.668975\n",
      "Epoch   70/200 hypothesis : tensor([155.9336, 181.5131, 181.8746, 197.9223, 137.1550]) Cost: 11.662874\n",
      "Epoch   71/200 hypothesis : tensor([155.9323, 181.5140, 181.8742, 197.9220, 137.1562]) Cost: 11.656710\n",
      "Epoch   72/200 hypothesis : tensor([155.9310, 181.5149, 181.8738, 197.9217, 137.1574]) Cost: 11.650628\n",
      "Epoch   73/200 hypothesis : tensor([155.9297, 181.5158, 181.8734, 197.9213, 137.1586]) Cost: 11.644472\n",
      "Epoch   74/200 hypothesis : tensor([155.9284, 181.5166, 181.8730, 197.9210, 137.1598]) Cost: 11.638399\n",
      "Epoch   75/200 hypothesis : tensor([155.9271, 181.5176, 181.8726, 197.9208, 137.1610]) Cost: 11.632280\n",
      "Epoch   76/200 hypothesis : tensor([155.9258, 181.5184, 181.8722, 197.9204, 137.1622]) Cost: 11.626202\n",
      "Epoch   77/200 hypothesis : tensor([155.9245, 181.5193, 181.8718, 197.9201, 137.1634]) Cost: 11.620093\n",
      "Epoch   78/200 hypothesis : tensor([155.9232, 181.5202, 181.8714, 197.9198, 137.1646]) Cost: 11.614006\n",
      "Epoch   79/200 hypothesis : tensor([155.9219, 181.5211, 181.8710, 197.9195, 137.1657]) Cost: 11.607894\n",
      "Epoch   80/200 hypothesis : tensor([155.9206, 181.5220, 181.8706, 197.9192, 137.1669]) Cost: 11.601775\n",
      "Epoch   81/200 hypothesis : tensor([155.9193, 181.5229, 181.8703, 197.9189, 137.1681]) Cost: 11.595687\n",
      "Epoch   82/200 hypothesis : tensor([155.9180, 181.5238, 181.8699, 197.9186, 137.1693]) Cost: 11.589630\n",
      "Epoch   83/200 hypothesis : tensor([155.9168, 181.5247, 181.8695, 197.9183, 137.1705]) Cost: 11.583591\n",
      "Epoch   84/200 hypothesis : tensor([155.9155, 181.5256, 181.8691, 197.9180, 137.1717]) Cost: 11.577477\n",
      "Epoch   85/200 hypothesis : tensor([155.9142, 181.5264, 181.8687, 197.9176, 137.1728]) Cost: 11.571445\n",
      "Epoch   86/200 hypothesis : tensor([155.9129, 181.5273, 181.8683, 197.9173, 137.1740]) Cost: 11.565356\n",
      "Epoch   87/200 hypothesis : tensor([155.9116, 181.5282, 181.8679, 197.9171, 137.1752]) Cost: 11.559293\n",
      "Epoch   88/200 hypothesis : tensor([155.9103, 181.5291, 181.8675, 197.9168, 137.1764]) Cost: 11.553207\n",
      "Epoch   89/200 hypothesis : tensor([155.9090, 181.5300, 181.8671, 197.9164, 137.1776]) Cost: 11.547141\n",
      "Epoch   90/200 hypothesis : tensor([155.9077, 181.5309, 181.8667, 197.9161, 137.1788]) Cost: 11.541090\n",
      "Epoch   91/200 hypothesis : tensor([155.9064, 181.5318, 181.8663, 197.9158, 137.1800]) Cost: 11.535046\n",
      "Epoch   92/200 hypothesis : tensor([155.9051, 181.5326, 181.8659, 197.9155, 137.1811]) Cost: 11.529009\n",
      "Epoch   93/200 hypothesis : tensor([155.9038, 181.5335, 181.8655, 197.9152, 137.1823]) Cost: 11.522936\n",
      "Epoch   94/200 hypothesis : tensor([155.9026, 181.5344, 181.8652, 197.9149, 137.1835]) Cost: 11.516939\n",
      "Epoch   95/200 hypothesis : tensor([155.9013, 181.5353, 181.8648, 197.9146, 137.1847]) Cost: 11.510862\n",
      "Epoch   96/200 hypothesis : tensor([155.9000, 181.5362, 181.8644, 197.9143, 137.1859]) Cost: 11.504853\n",
      "Epoch   97/200 hypothesis : tensor([155.8987, 181.5371, 181.8640, 197.9140, 137.1871]) Cost: 11.498816\n",
      "Epoch   98/200 hypothesis : tensor([155.8974, 181.5379, 181.8636, 197.9137, 137.1882]) Cost: 11.492776\n",
      "Epoch   99/200 hypothesis : tensor([155.8961, 181.5388, 181.8632, 197.9134, 137.1894]) Cost: 11.486753\n",
      "Epoch  100/200 hypothesis : tensor([155.8949, 181.5397, 181.8628, 197.9131, 137.1906]) Cost: 11.480746\n",
      "Epoch  101/200 hypothesis : tensor([155.8936, 181.5406, 181.8624, 197.9128, 137.1918]) Cost: 11.474712\n",
      "Epoch  102/200 hypothesis : tensor([155.8923, 181.5415, 181.8620, 197.9124, 137.1929]) Cost: 11.468690\n",
      "Epoch  103/200 hypothesis : tensor([155.8910, 181.5424, 181.8616, 197.9121, 137.1941]) Cost: 11.462704\n",
      "Epoch  104/200 hypothesis : tensor([155.8897, 181.5433, 181.8612, 197.9118, 137.1953]) Cost: 11.456689\n",
      "Epoch  105/200 hypothesis : tensor([155.8884, 181.5441, 181.8608, 197.9115, 137.1965]) Cost: 11.450670\n",
      "Epoch  106/200 hypothesis : tensor([155.8872, 181.5450, 181.8605, 197.9113, 137.1977]) Cost: 11.444660\n",
      "Epoch  107/200 hypothesis : tensor([155.8859, 181.5459, 181.8601, 197.9109, 137.1988]) Cost: 11.438675\n",
      "Epoch  108/200 hypothesis : tensor([155.8846, 181.5468, 181.8597, 197.9106, 137.2000]) Cost: 11.432662\n",
      "Epoch  109/200 hypothesis : tensor([155.8833, 181.5477, 181.8593, 197.9103, 137.2012]) Cost: 11.426676\n",
      "Epoch  110/200 hypothesis : tensor([155.8820, 181.5485, 181.8589, 197.9100, 137.2024]) Cost: 11.420700\n",
      "Epoch  111/200 hypothesis : tensor([155.8807, 181.5494, 181.8585, 197.9097, 137.2035]) Cost: 11.414718\n",
      "Epoch  112/200 hypothesis : tensor([155.8794, 181.5503, 181.8581, 197.9094, 137.2047]) Cost: 11.408726\n",
      "Epoch  113/200 hypothesis : tensor([155.8782, 181.5512, 181.8577, 197.9091, 137.2059]) Cost: 11.402731\n",
      "Epoch  114/200 hypothesis : tensor([155.8769, 181.5521, 181.8573, 197.9088, 137.2071]) Cost: 11.396750\n",
      "Epoch  115/200 hypothesis : tensor([155.8756, 181.5530, 181.8569, 197.9085, 137.2083]) Cost: 11.390738\n",
      "Epoch  116/200 hypothesis : tensor([155.8743, 181.5538, 181.8566, 197.9082, 137.2094]) Cost: 11.384827\n",
      "Epoch  117/200 hypothesis : tensor([155.8730, 181.5547, 181.8562, 197.9079, 137.2106]) Cost: 11.378830\n",
      "Epoch  118/200 hypothesis : tensor([155.8717, 181.5556, 181.8558, 197.9076, 137.2118]) Cost: 11.372869\n",
      "Epoch  119/200 hypothesis : tensor([155.8705, 181.5565, 181.8554, 197.9073, 137.2130]) Cost: 11.366904\n",
      "Epoch  120/200 hypothesis : tensor([155.8692, 181.5574, 181.8550, 197.9070, 137.2141]) Cost: 11.360960\n",
      "Epoch  121/200 hypothesis : tensor([155.8679, 181.5582, 181.8546, 197.9067, 137.2153]) Cost: 11.354998\n",
      "Epoch  122/200 hypothesis : tensor([155.8666, 181.5591, 181.8542, 197.9064, 137.2165]) Cost: 11.349064\n",
      "Epoch  123/200 hypothesis : tensor([155.8654, 181.5600, 181.8538, 197.9061, 137.2176]) Cost: 11.343089\n",
      "Epoch  124/200 hypothesis : tensor([155.8641, 181.5609, 181.8534, 197.9057, 137.2188]) Cost: 11.337156\n",
      "Epoch  125/200 hypothesis : tensor([155.8628, 181.5617, 181.8530, 197.9054, 137.2200]) Cost: 11.331228\n",
      "Epoch  126/200 hypothesis : tensor([155.8615, 181.5626, 181.8527, 197.9051, 137.2211]) Cost: 11.325295\n",
      "Epoch  127/200 hypothesis : tensor([155.8603, 181.5635, 181.8523, 197.9048, 137.2223]) Cost: 11.319349\n",
      "Epoch  128/200 hypothesis : tensor([155.8590, 181.5644, 181.8519, 197.9045, 137.2235]) Cost: 11.313421\n",
      "Epoch  129/200 hypothesis : tensor([155.8577, 181.5652, 181.8515, 197.9042, 137.2247]) Cost: 11.307467\n",
      "Epoch  130/200 hypothesis : tensor([155.8564, 181.5661, 181.8511, 197.9039, 137.2258]) Cost: 11.301535\n",
      "Epoch  131/200 hypothesis : tensor([155.8551, 181.5670, 181.8507, 197.9036, 137.2270]) Cost: 11.295645\n",
      "Epoch  132/200 hypothesis : tensor([155.8539, 181.5679, 181.8503, 197.9033, 137.2282]) Cost: 11.289711\n",
      "Epoch  133/200 hypothesis : tensor([155.8526, 181.5688, 181.8500, 197.9030, 137.2293]) Cost: 11.283808\n",
      "Epoch  134/200 hypothesis : tensor([155.8513, 181.5696, 181.8496, 197.9027, 137.2305]) Cost: 11.277873\n",
      "Epoch  135/200 hypothesis : tensor([155.8501, 181.5705, 181.8492, 197.9024, 137.2317]) Cost: 11.271957\n",
      "Epoch  136/200 hypothesis : tensor([155.8488, 181.5714, 181.8488, 197.9021, 137.2328]) Cost: 11.266077\n",
      "Epoch  137/200 hypothesis : tensor([155.8475, 181.5723, 181.8484, 197.9018, 137.2340]) Cost: 11.260157\n",
      "Epoch  138/200 hypothesis : tensor([155.8462, 181.5731, 181.8480, 197.9015, 137.2352]) Cost: 11.254252\n",
      "Epoch  139/200 hypothesis : tensor([155.8450, 181.5740, 181.8476, 197.9012, 137.2364]) Cost: 11.248350\n",
      "Epoch  140/200 hypothesis : tensor([155.8437, 181.5749, 181.8472, 197.9009, 137.2375]) Cost: 11.242441\n",
      "Epoch  141/200 hypothesis : tensor([155.8424, 181.5758, 181.8469, 197.9006, 137.2387]) Cost: 11.236581\n",
      "Epoch  142/200 hypothesis : tensor([155.8412, 181.5766, 181.8465, 197.9003, 137.2398]) Cost: 11.230708\n",
      "Epoch  143/200 hypothesis : tensor([155.8399, 181.5775, 181.8461, 197.9000, 137.2410]) Cost: 11.224836\n",
      "Epoch  144/200 hypothesis : tensor([155.8386, 181.5784, 181.8457, 197.8997, 137.2422]) Cost: 11.218908\n",
      "Epoch  145/200 hypothesis : tensor([155.8374, 181.5793, 181.8453, 197.8994, 137.2434]) Cost: 11.213056\n",
      "Epoch  146/200 hypothesis : tensor([155.8361, 181.5801, 181.8449, 197.8991, 137.2445]) Cost: 11.207186\n",
      "Epoch  147/200 hypothesis : tensor([155.8348, 181.5810, 181.8445, 197.8988, 137.2457]) Cost: 11.201270\n",
      "Epoch  148/200 hypothesis : tensor([155.8335, 181.5819, 181.8442, 197.8985, 137.2468]) Cost: 11.195432\n",
      "Epoch  149/200 hypothesis : tensor([155.8323, 181.5827, 181.8438, 197.8981, 137.2480]) Cost: 11.189571\n",
      "Epoch  150/200 hypothesis : tensor([155.8310, 181.5836, 181.8434, 197.8978, 137.2492]) Cost: 11.183702\n",
      "Epoch  151/200 hypothesis : tensor([155.8297, 181.5845, 181.8430, 197.8976, 137.2504]) Cost: 11.177838\n",
      "Epoch  152/200 hypothesis : tensor([155.8285, 181.5854, 181.8426, 197.8972, 137.2515]) Cost: 11.171998\n",
      "Epoch  153/200 hypothesis : tensor([155.8272, 181.5862, 181.8422, 197.8970, 137.2527]) Cost: 11.166127\n",
      "Epoch  154/200 hypothesis : tensor([155.8259, 181.5871, 181.8418, 197.8966, 137.2538]) Cost: 11.160304\n",
      "Epoch  155/200 hypothesis : tensor([155.8247, 181.5880, 181.8415, 197.8964, 137.2550]) Cost: 11.154439\n",
      "Epoch  156/200 hypothesis : tensor([155.8234, 181.5888, 181.8411, 197.8960, 137.2561]) Cost: 11.148600\n",
      "Epoch  157/200 hypothesis : tensor([155.8221, 181.5897, 181.8407, 197.8957, 137.2573]) Cost: 11.142764\n",
      "Epoch  158/200 hypothesis : tensor([155.8209, 181.5906, 181.8403, 197.8955, 137.2585]) Cost: 11.136885\n",
      "Epoch  159/200 hypothesis : tensor([155.8196, 181.5915, 181.8399, 197.8951, 137.2596]) Cost: 11.131087\n",
      "Epoch  160/200 hypothesis : tensor([155.8183, 181.5923, 181.8396, 197.8949, 137.2608]) Cost: 11.125294\n",
      "Epoch  161/200 hypothesis : tensor([155.8171, 181.5932, 181.8391, 197.8945, 137.2620]) Cost: 11.119418\n",
      "Epoch  162/200 hypothesis : tensor([155.8158, 181.5941, 181.8388, 197.8942, 137.2631]) Cost: 11.113620\n",
      "Epoch  163/200 hypothesis : tensor([155.8145, 181.5949, 181.8384, 197.8939, 137.2643]) Cost: 11.107818\n",
      "Epoch  164/200 hypothesis : tensor([155.8133, 181.5958, 181.8380, 197.8936, 137.2654]) Cost: 11.102010\n",
      "Epoch  165/200 hypothesis : tensor([155.8120, 181.5967, 181.8376, 197.8933, 137.2666]) Cost: 11.096151\n",
      "Epoch  166/200 hypothesis : tensor([155.8107, 181.5975, 181.8372, 197.8930, 137.2678]) Cost: 11.090319\n",
      "Epoch  167/200 hypothesis : tensor([155.8095, 181.5984, 181.8369, 197.8927, 137.2689]) Cost: 11.084534\n",
      "Epoch  168/200 hypothesis : tensor([155.8082, 181.5993, 181.8365, 197.8924, 137.2701]) Cost: 11.078753\n",
      "Epoch  169/200 hypothesis : tensor([155.8069, 181.6001, 181.8361, 197.8921, 137.2712]) Cost: 11.072903\n",
      "Epoch  170/200 hypothesis : tensor([155.8057, 181.6010, 181.8357, 197.8918, 137.2724]) Cost: 11.067109\n",
      "Epoch  171/200 hypothesis : tensor([155.8044, 181.6019, 181.8353, 197.8915, 137.2735]) Cost: 11.061322\n",
      "Epoch  172/200 hypothesis : tensor([155.8032, 181.6027, 181.8349, 197.8912, 137.2747]) Cost: 11.055531\n",
      "Epoch  173/200 hypothesis : tensor([155.8019, 181.6036, 181.8345, 197.8909, 137.2759]) Cost: 11.049764\n",
      "Epoch  174/200 hypothesis : tensor([155.8007, 181.6045, 181.8342, 197.8906, 137.2770]) Cost: 11.043946\n",
      "Epoch  175/200 hypothesis : tensor([155.7994, 181.6053, 181.8338, 197.8903, 137.2782]) Cost: 11.038189\n",
      "Epoch  176/200 hypothesis : tensor([155.7981, 181.6062, 181.8334, 197.8900, 137.2793]) Cost: 11.032434\n",
      "Epoch  177/200 hypothesis : tensor([155.7969, 181.6071, 181.8330, 197.8897, 137.2805]) Cost: 11.026615\n",
      "Epoch  178/200 hypothesis : tensor([155.7956, 181.6079, 181.8326, 197.8894, 137.2816]) Cost: 11.020873\n",
      "Epoch  179/200 hypothesis : tensor([155.7944, 181.6088, 181.8322, 197.8891, 137.2828]) Cost: 11.015104\n",
      "Epoch  180/200 hypothesis : tensor([155.7931, 181.6097, 181.8319, 197.8888, 137.2839]) Cost: 11.009312\n",
      "Epoch  181/200 hypothesis : tensor([155.7919, 181.6105, 181.8315, 197.8885, 137.2851]) Cost: 11.003571\n",
      "Epoch  182/200 hypothesis : tensor([155.7906, 181.6114, 181.8311, 197.8882, 137.2863]) Cost: 10.997774\n",
      "Epoch  183/200 hypothesis : tensor([155.7893, 181.6123, 181.8307, 197.8879, 137.2874]) Cost: 10.992035\n",
      "Epoch  184/200 hypothesis : tensor([155.7881, 181.6131, 181.8303, 197.8876, 137.2886]) Cost: 10.986271\n",
      "Epoch  185/200 hypothesis : tensor([155.7868, 181.6140, 181.8300, 197.8873, 137.2897]) Cost: 10.980502\n",
      "Epoch  186/200 hypothesis : tensor([155.7856, 181.6148, 181.8296, 197.8870, 137.2909]) Cost: 10.974782\n",
      "Epoch  187/200 hypothesis : tensor([155.7843, 181.6157, 181.8292, 197.8867, 137.2920]) Cost: 10.969017\n",
      "Epoch  188/200 hypothesis : tensor([155.7830, 181.6166, 181.8288, 197.8864, 137.2932]) Cost: 10.963267\n",
      "Epoch  189/200 hypothesis : tensor([155.7818, 181.6174, 181.8284, 197.8861, 137.2943]) Cost: 10.957544\n",
      "Epoch  190/200 hypothesis : tensor([155.7805, 181.6183, 181.8280, 197.8858, 137.2955]) Cost: 10.951852\n",
      "Epoch  191/200 hypothesis : tensor([155.7793, 181.6192, 181.8277, 197.8855, 137.2966]) Cost: 10.946048\n",
      "Epoch  192/200 hypothesis : tensor([155.7780, 181.6200, 181.8273, 197.8852, 137.2978]) Cost: 10.940328\n",
      "Epoch  193/200 hypothesis : tensor([155.7768, 181.6209, 181.8269, 197.8849, 137.2989]) Cost: 10.934636\n",
      "Epoch  194/200 hypothesis : tensor([155.7755, 181.6217, 181.8265, 197.8846, 137.3001]) Cost: 10.928889\n",
      "Epoch  195/200 hypothesis : tensor([155.7743, 181.6226, 181.8261, 197.8843, 137.3012]) Cost: 10.923173\n",
      "Epoch  196/200 hypothesis : tensor([155.7730, 181.6235, 181.8258, 197.8840, 137.3024]) Cost: 10.917437\n",
      "Epoch  197/200 hypothesis : tensor([155.7718, 181.6243, 181.8254, 197.8837, 137.3035]) Cost: 10.911736\n",
      "Epoch  198/200 hypothesis : tensor([155.7705, 181.6252, 181.8250, 197.8834, 137.3047]) Cost: 10.906008\n",
      "Epoch  199/200 hypothesis : tensor([155.7693, 181.6261, 181.8246, 197.8831, 137.3058]) Cost: 10.900321\n",
      "Epoch  200/200 hypothesis : tensor([155.7680, 181.6269, 181.8242, 197.8828, 137.3070]) Cost: 10.894592\n"
     ]
    }
   ],
   "source": [
    "# cost 계산\n",
    "nb_epochs = 200\n",
    "for epoch in range(nb_epochs + 1) :\n",
    "    # H(x) 계산\n",
    "    hypothesis = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(hypothesis, y_train)\n",
    "    \n",
    "    # cost로 H(x)개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 20번마다 로그 출력\n",
    "    print('Epoch {:4d}/{} hypothesis : {} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading Data\n",
    "\n",
    "* 대부분 데이터 셋은 적어도 수십만 개의 데이터를 제공\n",
    "* 너무 많은 데이터를 한번에 학습시키기에는 너무 느리고 하드웨어적으로 불가능 -> 균일하게 나눠서 학습 : minibatch gradient descent\n",
    "\n",
    "### mini-batch gradient descent \n",
    "* 업데이트를 좀 더 빠르게 할 수 있다.\n",
    "* 전체 데이터를 쓰지 않아서 잘못된 방향으로 업데이트를 할 수도 있다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch dataset 생성\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset) :\n",
    "    def __init__(self) : \n",
    "        self.x_data = [[73, 80, 75],\n",
    "                       [93, 88, 93],\n",
    "                       [89, 91, 90],\n",
    "                       [96, 98, 100],\n",
    "                       [73, 66, 70]]\n",
    "        self.y_data = [[152], [185], [180], [196], [142]]\n",
    "    \n",
    "    # 길이를 반환하는 매직 method 설정\n",
    "    def __len__(self) :\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    # 인덱스를 받았을 때 그에 맞는 데이터를 반환하는 method\n",
    "    def __getitem__(self, idx) :\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "dataset = CustomDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset을 만들면 dataloader를 사용할 수 있음\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=2,           # minibatch의 크기\n",
    "    shuffle=True,           # Epoch마다 데이터셋을 섞어서, 데이터가 학습되는 순서를 바꿈 \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch : 1/3 Cost: 16.692860\n",
      "Epoch    0/20 Batch : 2/3 Cost: 13.284204\n",
      "Epoch    0/20 Batch : 3/3 Cost: 13.311319\n",
      "Epoch    1/20 Batch : 1/3 Cost: 2.854760\n",
      "Epoch    1/20 Batch : 2/3 Cost: 20.381283\n",
      "Epoch    1/20 Batch : 3/3 Cost: 17.847067\n",
      "Epoch    2/20 Batch : 1/3 Cost: 12.155276\n",
      "Epoch    2/20 Batch : 2/3 Cost: 13.486388\n",
      "Epoch    2/20 Batch : 3/3 Cost: 6.275967\n",
      "Epoch    3/20 Batch : 1/3 Cost: 6.307155\n",
      "Epoch    3/20 Batch : 2/3 Cost: 29.522331\n",
      "Epoch    3/20 Batch : 3/3 Cost: 6.708002\n",
      "Epoch    4/20 Batch : 1/3 Cost: 1.793839\n",
      "Epoch    4/20 Batch : 2/3 Cost: 25.916840\n",
      "Epoch    4/20 Batch : 3/3 Cost: 21.124273\n",
      "Epoch    5/20 Batch : 1/3 Cost: 9.469073\n",
      "Epoch    5/20 Batch : 2/3 Cost: 12.709829\n",
      "Epoch    5/20 Batch : 3/3 Cost: 16.598015\n",
      "Epoch    6/20 Batch : 1/3 Cost: 26.860384\n",
      "Epoch    6/20 Batch : 2/3 Cost: 7.953699\n",
      "Epoch    6/20 Batch : 3/3 Cost: 10.997133\n",
      "Epoch    7/20 Batch : 1/3 Cost: 13.737206\n",
      "Epoch    7/20 Batch : 2/3 Cost: 13.789694\n",
      "Epoch    7/20 Batch : 3/3 Cost: 14.364049\n",
      "Epoch    8/20 Batch : 1/3 Cost: 19.967022\n",
      "Epoch    8/20 Batch : 2/3 Cost: 9.490628\n",
      "Epoch    8/20 Batch : 3/3 Cost: 2.974962\n",
      "Epoch    9/20 Batch : 1/3 Cost: 4.718835\n",
      "Epoch    9/20 Batch : 2/3 Cost: 31.288233\n",
      "Epoch    9/20 Batch : 3/3 Cost: 6.000466\n",
      "Epoch   10/20 Batch : 1/3 Cost: 13.707636\n",
      "Epoch   10/20 Batch : 2/3 Cost: 9.205295\n",
      "Epoch   10/20 Batch : 3/3 Cost: 20.861126\n",
      "Epoch   11/20 Batch : 1/3 Cost: 8.610811\n",
      "Epoch   11/20 Batch : 2/3 Cost: 12.786097\n",
      "Epoch   11/20 Batch : 3/3 Cost: 23.048233\n",
      "Epoch   12/20 Batch : 1/3 Cost: 11.624879\n",
      "Epoch   12/20 Batch : 2/3 Cost: 7.634616\n",
      "Epoch   12/20 Batch : 3/3 Cost: 20.658257\n",
      "Epoch   13/20 Batch : 1/3 Cost: 4.816610\n",
      "Epoch   13/20 Batch : 2/3 Cost: 30.862289\n",
      "Epoch   13/20 Batch : 3/3 Cost: 6.030031\n",
      "Epoch   14/20 Batch : 1/3 Cost: 13.601557\n",
      "Epoch   14/20 Batch : 2/3 Cost: 7.171433\n",
      "Epoch   14/20 Batch : 3/3 Cost: 16.179211\n",
      "Epoch   15/20 Batch : 1/3 Cost: 0.253545\n",
      "Epoch   15/20 Batch : 2/3 Cost: 14.832930\n",
      "Epoch   15/20 Batch : 3/3 Cost: 27.776644\n",
      "Epoch   16/20 Batch : 1/3 Cost: 6.701545\n",
      "Epoch   16/20 Batch : 2/3 Cost: 17.736496\n",
      "Epoch   16/20 Batch : 3/3 Cost: 7.336282\n",
      "Epoch   17/20 Batch : 1/3 Cost: 21.187786\n",
      "Epoch   17/20 Batch : 2/3 Cost: 16.235601\n",
      "Epoch   17/20 Batch : 3/3 Cost: 1.916308\n",
      "Epoch   18/20 Batch : 1/3 Cost: 13.836434\n",
      "Epoch   18/20 Batch : 2/3 Cost: 1.415984\n",
      "Epoch   18/20 Batch : 3/3 Cost: 31.001019\n",
      "Epoch   19/20 Batch : 1/3 Cost: 10.730365\n",
      "Epoch   19/20 Batch : 2/3 Cost: 17.211079\n",
      "Epoch   19/20 Batch : 3/3 Cost: 14.275358\n",
      "Epoch   20/20 Batch : 1/3 Cost: 11.484745\n",
      "Epoch   20/20 Batch : 2/3 Cost: 6.837453\n",
      "Epoch   20/20 Batch : 3/3 Cost: 32.810471\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs + 1) :\n",
    "    for batch_idx, samples in enumerate(dataloader) :\n",
    "        x_train, y_train = samples\n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        # cost 계산\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "        \n",
    "        # cost로 H(x) 개선\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        print('Epoch {:4d}/{} Batch : {}/{} Cost: {:.6f}'.format(\n",
    "            epoch, epochs, batch_idx+1, len(dataloader), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
