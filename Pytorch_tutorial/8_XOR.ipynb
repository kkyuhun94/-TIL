{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR : 두개의 입력이 다른경우에 1을, 같은 경우에 0을 출력하는 논리 연산\n",
    "* 단일 퍼셉트론으로는 계산이 불가능하기 때문에 Multi layer perceptrons을 사용한다.\n",
    "* 역전파를 사용해서 MLP를 사용이 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR\n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device) # 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 하기 위해 Layer를 구성하는 weight와 bias \n",
    "# nn Layers\n",
    "w1 = torch.Tensor(2,2).to(device)\n",
    "b1 = torch.Tensor(2).to(device)\n",
    "w2 = torch.Tensor(2,1).to(device)\n",
    "b2 = torch.Tensor(1).to(device)\n",
    "\n",
    "# nn.Linear 두개로 이루어 진것과 같음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid함수 구현\n",
    "def sigmoid(x) :\n",
    "    # sigmoid function\n",
    "    return 1.0 / (1.0 + torch.exp(-x))\n",
    "\n",
    "# sigmoid의 미분함수 \n",
    "def sigmoid_prime(x) :\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(nan)\n",
      "100 tensor(nan)\n",
      "200 tensor(nan)\n",
      "300 tensor(nan)\n",
      "400 tensor(nan)\n",
      "500 tensor(nan)\n",
      "600 tensor(nan)\n",
      "700 tensor(nan)\n",
      "800 tensor(nan)\n",
      "900 tensor(nan)\n",
      "1000 tensor(nan)\n",
      "1100 tensor(nan)\n",
      "1200 tensor(nan)\n",
      "1300 tensor(nan)\n",
      "1400 tensor(nan)\n",
      "1500 tensor(nan)\n",
      "1600 tensor(nan)\n",
      "1700 tensor(nan)\n",
      "1800 tensor(nan)\n",
      "1900 tensor(nan)\n",
      "2000 tensor(nan)\n",
      "2100 tensor(nan)\n",
      "2200 tensor(nan)\n",
      "2300 tensor(nan)\n",
      "2400 tensor(nan)\n",
      "2500 tensor(nan)\n",
      "2600 tensor(nan)\n",
      "2700 tensor(nan)\n",
      "2800 tensor(nan)\n",
      "2900 tensor(nan)\n",
      "3000 tensor(nan)\n",
      "3100 tensor(nan)\n",
      "3200 tensor(nan)\n",
      "3300 tensor(nan)\n",
      "3400 tensor(nan)\n",
      "3500 tensor(nan)\n",
      "3600 tensor(nan)\n",
      "3700 tensor(nan)\n",
      "3800 tensor(nan)\n",
      "3900 tensor(nan)\n",
      "4000 tensor(nan)\n",
      "4100 tensor(nan)\n",
      "4200 tensor(nan)\n",
      "4300 tensor(nan)\n",
      "4400 tensor(nan)\n",
      "4500 tensor(nan)\n",
      "4600 tensor(nan)\n",
      "4700 tensor(nan)\n",
      "4800 tensor(nan)\n",
      "4900 tensor(nan)\n",
      "5000 tensor(nan)\n",
      "5100 tensor(nan)\n",
      "5200 tensor(nan)\n",
      "5300 tensor(nan)\n",
      "5400 tensor(nan)\n",
      "5500 tensor(nan)\n",
      "5600 tensor(nan)\n",
      "5700 tensor(nan)\n",
      "5800 tensor(nan)\n",
      "5900 tensor(nan)\n",
      "6000 tensor(nan)\n",
      "6100 tensor(nan)\n",
      "6200 tensor(nan)\n",
      "6300 tensor(nan)\n",
      "6400 tensor(nan)\n",
      "6500 tensor(nan)\n",
      "6600 tensor(nan)\n",
      "6700 tensor(nan)\n",
      "6800 tensor(nan)\n",
      "6900 tensor(nan)\n",
      "7000 tensor(nan)\n",
      "7100 tensor(nan)\n",
      "7200 tensor(nan)\n",
      "7300 tensor(nan)\n",
      "7400 tensor(nan)\n",
      "7500 tensor(nan)\n",
      "7600 tensor(nan)\n",
      "7700 tensor(nan)\n",
      "7800 tensor(nan)\n",
      "7900 tensor(nan)\n",
      "8000 tensor(nan)\n",
      "8100 tensor(nan)\n",
      "8200 tensor(nan)\n",
      "8300 tensor(nan)\n",
      "8400 tensor(nan)\n",
      "8500 tensor(nan)\n",
      "8600 tensor(nan)\n",
      "8700 tensor(nan)\n",
      "8800 tensor(nan)\n",
      "8900 tensor(nan)\n",
      "9000 tensor(nan)\n",
      "9100 tensor(nan)\n",
      "9200 tensor(nan)\n",
      "9300 tensor(nan)\n",
      "9400 tensor(nan)\n",
      "9500 tensor(nan)\n",
      "9600 tensor(nan)\n",
      "9700 tensor(nan)\n",
      "9800 tensor(nan)\n",
      "9900 tensor(nan)\n",
      "10000 tensor(nan)\n"
     ]
    }
   ],
   "source": [
    "# 학습 코드 \n",
    "\n",
    "learning_rate = 0.1\n",
    "for step in range(10001) :\n",
    "    # forward\n",
    "    l1 = torch.add(torch.matmul(X, w1), b1)\n",
    "    a1 = sigmoid(l1)\n",
    "    l2 = torch.add(torch.matmul(a1, w2), b2)\n",
    "    Y_pred = sigmoid(l2)\n",
    "    \n",
    "    cost = -torch.mean(Y * torch.log(Y_pred) + (1-Y)* torch.log(1 - Y_pred))\n",
    "    \n",
    "    # Backprop\n",
    "    # loss(binary cross entropy loss)에 대한 미분 \n",
    "    d_Y_pred = (Y_pred - Y) / (Y_pred * (1.0 - Y_pred) + 1e-7)\n",
    "    \n",
    "    # Layer 2\n",
    "    d_l2 = d_Y_pred * sigmoid_prime(l2)\n",
    "    d_b2 = d_l2 # bias\n",
    "    d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2) # transpose : a1 의 차원 (n x m) 을 (m x n) 으로 바꿈 \n",
    "    \n",
    "    # Layer 1 \n",
    "    d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1))\n",
    "    d_l1 = d_a1 * sigmoid_prime(l1)\n",
    "    d_b1 = d_l1 # bias\n",
    "    d_w1 = torch.matmul(torch.transpose(X, 0 ,1), d_b1)\n",
    "    \n",
    "    # Weight update (gradient descent)\n",
    "    w1 = w1 - learning_rate * d_w1\n",
    "    b1 = b1 - learning_rate * torch.mean(d_b1, 0)\n",
    "    w2 = w2 - learning_rate * d_w2\n",
    "    b2 = b2 - learning_rate * torch.mean(d_b2, 0)\n",
    "    \n",
    "    if step % 100 == 0 :\n",
    "        print(step, cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xor -nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR\n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device) # 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn layers\n",
    "linear1 = torch.nn.Linear(2, 2, bias = True)\n",
    "linear2 = torch.nn.Linear(2, 1, bias = True)\n",
    "sigmoid = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 Cost: 0.7434073090553284\n",
      "step: 100 Cost: 0.693165123462677\n",
      "step: 200 Cost: 0.6931577920913696\n",
      "step: 300 Cost: 0.6931517124176025\n",
      "step: 400 Cost: 0.6931463479995728\n",
      "step: 500 Cost: 0.6931411027908325\n",
      "step: 600 Cost: 0.6931357383728027\n",
      "step: 700 Cost: 0.6931294798851013\n",
      "step: 800 Cost: 0.6931220889091492\n",
      "step: 900 Cost: 0.6931126117706299\n",
      "step: 1000 Cost: 0.6930999755859375\n",
      "step: 1100 Cost: 0.693082332611084\n",
      "step: 1200 Cost: 0.6930569410324097\n",
      "step: 1300 Cost: 0.6930190324783325\n",
      "step: 1400 Cost: 0.6929606199264526\n",
      "step: 1500 Cost: 0.6928660273551941\n",
      "step: 1600 Cost: 0.6927032470703125\n",
      "step: 1700 Cost: 0.6923960447311401\n",
      "step: 1800 Cost: 0.6917301416397095\n",
      "step: 1900 Cost: 0.6899653673171997\n",
      "step: 2000 Cost: 0.683831512928009\n",
      "step: 2100 Cost: 0.6561665534973145\n",
      "step: 2200 Cost: 0.431100070476532\n",
      "step: 2300 Cost: 0.1348930448293686\n",
      "step: 2400 Cost: 0.0663042739033699\n",
      "step: 2500 Cost: 0.04216815158724785\n",
      "step: 2600 Cost: 0.03045385330915451\n",
      "step: 2700 Cost: 0.0236658975481987\n",
      "step: 2800 Cost: 0.01927773468196392\n",
      "step: 2900 Cost: 0.01622403785586357\n",
      "step: 3000 Cost: 0.01398373395204544\n",
      "step: 3100 Cost: 0.01227390207350254\n",
      "step: 3200 Cost: 0.010928118601441383\n",
      "step: 3300 Cost: 0.009842472150921822\n",
      "step: 3400 Cost: 0.008949032984673977\n",
      "step: 3500 Cost: 0.008201336488127708\n",
      "step: 3600 Cost: 0.0075667379423975945\n",
      "step: 3700 Cost: 0.007021686062216759\n",
      "step: 3800 Cost: 0.006548595614731312\n",
      "step: 3900 Cost: 0.006134253926575184\n",
      "step: 4000 Cost: 0.005768344737589359\n",
      "step: 4100 Cost: 0.0054430365562438965\n",
      "step: 4200 Cost: 0.005151905119419098\n",
      "step: 4300 Cost: 0.004889918025583029\n",
      "step: 4400 Cost: 0.0046528722159564495\n",
      "step: 4500 Cost: 0.004437457304447889\n",
      "step: 4600 Cost: 0.004240859299898148\n",
      "step: 4700 Cost: 0.004060687031596899\n",
      "step: 4800 Cost: 0.003895031288266182\n",
      "step: 4900 Cost: 0.0037421947345137596\n",
      "step: 5000 Cost: 0.0036007347516715527\n",
      "step: 5100 Cost: 0.003469479735940695\n",
      "step: 5200 Cost: 0.0033473046496510506\n",
      "step: 5300 Cost: 0.0032333978451788425\n",
      "step: 5400 Cost: 0.0031268750317394733\n",
      "step: 5500 Cost: 0.0030270610004663467\n",
      "step: 5600 Cost: 0.0029333550482988358\n",
      "step: 5700 Cost: 0.0028452035039663315\n",
      "step: 5800 Cost: 0.00276215560734272\n",
      "step: 5900 Cost: 0.002683777129277587\n",
      "step: 6000 Cost: 0.0026096487417817116\n",
      "step: 6100 Cost: 0.0025394847616553307\n",
      "step: 6200 Cost: 0.0024729417636990547\n",
      "step: 6300 Cost: 0.0024097643326967955\n",
      "step: 6400 Cost: 0.002349698217585683\n",
      "step: 6500 Cost: 0.0022925634402781725\n",
      "step: 6600 Cost: 0.002238075714558363\n",
      "step: 6700 Cost: 0.002186085097491741\n",
      "step: 6800 Cost: 0.0021364721469581127\n",
      "step: 6900 Cost: 0.002089011948555708\n",
      "step: 7000 Cost: 0.00204361486248672\n",
      "step: 7100 Cost: 0.0020001311786472797\n",
      "step: 7200 Cost: 0.0019584265537559986\n",
      "step: 7300 Cost: 0.0019184107659384608\n",
      "step: 7400 Cost: 0.0018799942918121815\n",
      "step: 7500 Cost: 0.0018430722411721945\n",
      "step: 7600 Cost: 0.0018075549742206931\n",
      "step: 7700 Cost: 0.0017733527347445488\n",
      "step: 7800 Cost: 0.0017404207028448582\n",
      "step: 7900 Cost: 0.0017087137093767524\n",
      "step: 8000 Cost: 0.001678097527474165\n",
      "step: 8100 Cost: 0.0016485571395605803\n",
      "step: 8200 Cost: 0.0016200175741687417\n",
      "step: 8300 Cost: 0.0015924491453915834\n",
      "step: 8400 Cost: 0.0015657918993383646\n",
      "step: 8500 Cost: 0.0015400308184325695\n",
      "step: 8600 Cost: 0.0015150614781305194\n",
      "step: 8700 Cost: 0.0014909137971699238\n",
      "step: 8800 Cost: 0.0014674977865070105\n",
      "step: 8900 Cost: 0.001444813678972423\n",
      "step: 9000 Cost: 0.0014228165382519364\n",
      "step: 9100 Cost: 0.0014014765620231628\n",
      "step: 9200 Cost: 0.0013806892093271017\n",
      "step: 9300 Cost: 0.0013606036081910133\n",
      "step: 9400 Cost: 0.0013410557294264436\n",
      "step: 9500 Cost: 0.001322030322626233\n",
      "step: 9600 Cost: 0.001303557539358735\n",
      "step: 9700 Cost: 0.001285637030377984\n",
      "step: 9800 Cost: 0.0012681199004873633\n",
      "step: 9900 Cost: 0.0012511102249845862\n",
      "step: 10000 Cost: 0.0012345188297331333\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    # cost/loss function\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(\"step:\", step,\"Cost:\", cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis:  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), '\\nCorrect: ', predicted.detach().cpu().numpy(), '\\nAccuracy: ', accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 층을 늘림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn layers\n",
    "linear1 = torch.nn.Linear(2, 2, bias=True)\n",
    "linear2 = torch.nn.Linear(2, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model2 = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model2.parameters(), lr=1)  # modified learning rate from 0.1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7434073090553284\n",
      "100 0.693165123462677\n",
      "200 0.6931577920913696\n",
      "300 0.6931517124176025\n",
      "400 0.6931463479995728\n",
      "500 0.6931411027908325\n",
      "600 0.6931357383728027\n",
      "700 0.6931294798851013\n",
      "800 0.6931220889091492\n",
      "900 0.6931126117706299\n",
      "1000 0.6930999755859375\n",
      "1100 0.693082332611084\n",
      "1200 0.6930569410324097\n",
      "1300 0.6930190324783325\n",
      "1400 0.6929606199264526\n",
      "1500 0.6928660273551941\n",
      "1600 0.6927032470703125\n",
      "1700 0.6923960447311401\n",
      "1800 0.6917301416397095\n",
      "1900 0.6899653673171997\n",
      "2000 0.683831512928009\n",
      "2100 0.6561665534973145\n",
      "2200 0.431100070476532\n",
      "2300 0.1348930448293686\n",
      "2400 0.0663042739033699\n",
      "2500 0.04216815158724785\n",
      "2600 0.03045385330915451\n",
      "2700 0.0236658975481987\n",
      "2800 0.01927773468196392\n",
      "2900 0.01622403785586357\n",
      "3000 0.01398373395204544\n",
      "3100 0.01227390207350254\n",
      "3200 0.010928118601441383\n",
      "3300 0.009842472150921822\n",
      "3400 0.008949032984673977\n",
      "3500 0.008201336488127708\n",
      "3600 0.0075667379423975945\n",
      "3700 0.007021686062216759\n",
      "3800 0.006548595614731312\n",
      "3900 0.006134253926575184\n",
      "4000 0.005768344737589359\n",
      "4100 0.0054430365562438965\n",
      "4200 0.005151905119419098\n",
      "4300 0.004889918025583029\n",
      "4400 0.0046528722159564495\n",
      "4500 0.004437457304447889\n",
      "4600 0.004240859299898148\n",
      "4700 0.004060687031596899\n",
      "4800 0.003895031288266182\n",
      "4900 0.0037421947345137596\n",
      "5000 0.0036007347516715527\n",
      "5100 0.003469479735940695\n",
      "5200 0.0033473046496510506\n",
      "5300 0.0032333978451788425\n",
      "5400 0.0031268750317394733\n",
      "5500 0.0030270610004663467\n",
      "5600 0.0029333550482988358\n",
      "5700 0.0028452035039663315\n",
      "5800 0.00276215560734272\n",
      "5900 0.002683777129277587\n",
      "6000 0.0026096487417817116\n",
      "6100 0.0025394847616553307\n",
      "6200 0.0024729417636990547\n",
      "6300 0.0024097643326967955\n",
      "6400 0.002349698217585683\n",
      "6500 0.0022925634402781725\n",
      "6600 0.002238075714558363\n",
      "6700 0.002186085097491741\n",
      "6800 0.0021364721469581127\n",
      "6900 0.002089011948555708\n",
      "7000 0.00204361486248672\n",
      "7100 0.0020001311786472797\n",
      "7200 0.0019584265537559986\n",
      "7300 0.0019184107659384608\n",
      "7400 0.0018799942918121815\n",
      "7500 0.0018430722411721945\n",
      "7600 0.0018075549742206931\n",
      "7700 0.0017733527347445488\n",
      "7800 0.0017404207028448582\n",
      "7900 0.0017087137093767524\n",
      "8000 0.001678097527474165\n",
      "8100 0.0016485571395605803\n",
      "8200 0.0016200175741687417\n",
      "8300 0.0015924491453915834\n",
      "8400 0.0015657918993383646\n",
      "8500 0.0015400308184325695\n",
      "8600 0.0015150614781305194\n",
      "8700 0.0014909137971699238\n",
      "8800 0.0014674977865070105\n",
      "8900 0.001444813678972423\n",
      "9000 0.0014228165382519364\n",
      "9100 0.0014014765620231628\n",
      "9200 0.0013806892093271017\n",
      "9300 0.0013606036081910133\n",
      "9400 0.0013410557294264436\n",
      "9500 0.001322030322626233\n",
      "9600 0.001303557539358735\n",
      "9700 0.001285637030377984\n",
      "9800 0.0012681199004873633\n",
      "9900 0.0012511102249845862\n",
      "10000 0.0012345188297331333\n"
     ]
    }
   ],
   "source": [
    "for step in range(10001):\n",
    "    optimizer.zero_grad() # 파라미터 초기화 \n",
    "    hypothesis = model2(X) \n",
    "\n",
    "    # cost/loss function\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis:  [[0.00106364]\n",
      " [0.99889404]\n",
      " [0.99889404]\n",
      " [0.00165861]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy computation\n",
    "# 정확도 증가 \n",
    "# True if hypothesis>0.5 else False\n",
    "with torch.no_grad():  # update된 gradient\n",
    "    hypothesis = model2(X)\n",
    "    predicted = (hypothesis > 0.5).float() # 확률이 0.5 보다 클때 1 , 작을때 0\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), '\\nCorrect: ', predicted.detach().cpu().numpy(), '\\nAccuracy: ', accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn layers\n",
    "linear1 = torch.nn.Linear(2, 10, bias=True)\n",
    "linear2 = torch.nn.Linear(10, 10, bias=True)\n",
    "linear3 = torch.nn.Linear(10, 10, bias=True)\n",
    "linear4 = torch.nn.Linear(10, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model3 = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid, linear3, sigmoid, linear4, sigmoid).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)  # modified learning rate from 0.1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0012343396665528417\n",
      "100 0.0012182408245280385\n",
      "200 0.0012025302276015282\n",
      "300 0.0011871629394590855\n",
      "400 0.0011722433846443892\n",
      "500 0.0011576821561902761\n",
      "600 0.0011434644693508744\n",
      "700 0.0011295899748802185\n",
      "800 0.0011160289868712425\n",
      "900 0.0011027962900698185\n",
      "1000 0.0010899071348831058\n",
      "1100 0.001077286433428526\n",
      "1200 0.0010649494361132383\n",
      "1300 0.0010528811253607273\n",
      "1400 0.0010410815011709929\n",
      "1500 0.0010295654647052288\n",
      "1600 0.0010182731784880161\n",
      "1700 0.0010072496952489018\n",
      "1800 0.0009964201599359512\n",
      "1900 0.000985888997092843\n",
      "2000 0.0009755371138453484\n",
      "2100 0.0009653493762016296\n",
      "2200 0.0009554600110277534\n",
      "2300 0.0009457049891352654\n",
      "2400 0.0009361589327454567\n",
      "2500 0.0009268366266041994\n",
      "2600 0.0009177083848044276\n",
      "2700 0.0009086995269171894\n",
      "2800 0.0008998995763249695\n",
      "2900 0.0008912637713365257\n",
      "3000 0.0008827920537441969\n",
      "3100 0.0008744547376409173\n",
      "3200 0.0008662814507260919\n",
      "3300 0.0008582425070926547\n",
      "3400 0.000850397627800703\n",
      "3500 0.0008426720160059631\n",
      "3600 0.0008350807474926114\n",
      "3700 0.0008276087464764714\n",
      "3800 0.0008203010074794292\n",
      "3900 0.000813097576610744\n",
      "4000 0.0008060433901846409\n",
      "4100 0.0007990488666109741\n",
      "4200 0.0007922782097011805\n",
      "4300 0.0007855223957449198\n",
      "4400 0.0007789605297148228\n",
      "4500 0.000772458384744823\n",
      "4600 0.0007660457631573081\n",
      "4700 0.000759737566113472\n",
      "4800 0.000753578613512218\n",
      "4900 0.0007474941667169333\n",
      "5000 0.0007415290456265211\n",
      "5100 0.0007356087444350123\n",
      "5200 0.0007298376294784248\n",
      "5300 0.0007241410203278065\n",
      "5400 0.0007184743299148977\n",
      "5500 0.0007129717851057649\n",
      "5600 0.0007075138855725527\n",
      "5700 0.0007021008059382439\n",
      "5800 0.0006968070520088077\n",
      "5900 0.0006916326237842441\n",
      "6000 0.0006865476607345045\n",
      "6100 0.0006814478547312319\n",
      "6200 0.0006764822755940259\n",
      "6300 0.0006715613999404013\n",
      "6400 0.0006667450070381165\n",
      "6500 0.0006619584746658802\n",
      "6600 0.0006572165875695646\n",
      "6700 0.0006525642820633948\n",
      "6800 0.0006480163428932428\n",
      "6900 0.0006434982642531395\n",
      "7000 0.000639084551949054\n",
      "7100 0.0006346559966914356\n",
      "7200 0.0006303616100922227\n",
      "7300 0.0006261120433919132\n",
      "7400 0.0006218773778527975\n",
      "7500 0.0006177321774885058\n",
      "7600 0.0006136168376542628\n",
      "7700 0.0006095313001424074\n",
      "7800 0.000605535227805376\n",
      "7900 0.0006016436382196844\n",
      "8000 0.0005977072869427502\n",
      "8100 0.0005938604008406401\n",
      "8200 0.0005900880787521601\n",
      "8300 0.0005863604601472616\n",
      "8400 0.0005826478591188788\n",
      "8500 0.0005789650604128838\n",
      "8600 0.0005754015292041004\n",
      "8700 0.0005718081956729293\n",
      "8800 0.000568319286685437\n",
      "8900 0.0005648601800203323\n",
      "9000 0.0005614159745164216\n",
      "9100 0.0005579867283813655\n",
      "9200 0.0005546469474211335\n",
      "9300 0.0005513369687832892\n",
      "9400 0.0005480419495142996\n",
      "9500 0.0005448065348900855\n",
      "9600 0.0005416307831183076\n",
      "9700 0.0005384401883929968\n",
      "9800 0.0005353539017960429\n",
      "9900 0.0005322825745679438\n",
      "10000 0.0005292111891321838\n"
     ]
    }
   ],
   "source": [
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    # cost/loss function\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis:  [[4.5869316e-04]\n",
      " [9.9952638e-01]\n",
      " [9.9952638e-01]\n",
      " [7.1005197e-04]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), '\\nCorrect: ', predicted.detach().cpu().numpy(), '\\nAccuracy: ', accuracy.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
