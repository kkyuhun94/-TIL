{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12d27b230>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 같은 결과를 내기 위해 \n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy Loss with torch.nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.rand(3, 5, requires_grad = True)\n",
    "hypothesis = F.softmax(z, dim=1)\n",
    "y = torch.randint(5, (3,)).long() \n",
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
       "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
       "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]], grad_fn=<LogBackward>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Low level\n",
    "torch.log(F.softmax(z, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
       "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
       "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High level\n",
    "F.log_softmax(z, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Low level\n",
    "(y_one_hot * -torch.log(F.softmax(z,dim=1))).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High level\n",
    "# F.nll_loss : negative loss likelihood\n",
    "# nll_loss에는 long type을 인풋으로 받음 \n",
    "\n",
    "F.nll_loss(F.log_softmax(z, dim=1), y)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('data-04-zoo.csv', delimiter = ',', dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 1., 0.],\n",
       "       [1., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 3.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 6.],\n",
       "       [0., 1., 1., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
       "        [0., 0., 1.,  ..., 1., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 1.,  ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train =  torch.FloatTensor(xy[:, 0:-1])\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.LongTensor(xy[:, [-1]]).squeeze() # [-1] : 벡터로 하기위해 (그냥 -1은 안됨), 마지막 컬럼만 추출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 3, 0, 0, 0, 0, 3, 3, 0, 0, 1, 3, 6, 6, 6, 1, 0, 3, 0, 1, 1, 0, 1,\n",
       "        5, 4, 4, 0, 0, 0, 5, 0, 0, 1, 3, 0, 0, 1, 3, 5, 5, 1, 5, 1, 0, 0, 6, 0,\n",
       "        0, 0, 0, 5, 4, 6, 0, 0, 1, 1, 1, 1, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        6, 3, 0, 0, 2, 6, 1, 1, 2, 6, 3, 1, 0, 6, 3, 1, 5, 4, 2, 2, 3, 0, 0, 1,\n",
       "        0, 5, 0, 6, 1])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 17)\n",
      "torch.Size([101, 16])\n",
      "torch.Size([101])\n"
     ]
    }
   ],
   "source": [
    "print(xy.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 7\n",
    "y_one_hot = torch.zeros((len(y_train), nb_classes))\n",
    "y_one_hot = y_one_hot.scatter(1, y_train.unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = F.softmax(x_train.matmul(W) + b, dim = 1) # or .mm or @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = (y_one_hot * -torch.log(z)).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.945910\n",
      "Epoch  100/1000 Cost: 0.471836\n",
      "Epoch  200/1000 Cost: 0.326327\n",
      "Epoch  300/1000 Cost: 0.257839\n",
      "Epoch  400/1000 Cost: 0.215762\n",
      "Epoch  500/1000 Cost: 0.186603\n",
      "Epoch  600/1000 Cost: 0.164898\n",
      "Epoch  700/1000 Cost: 0.147955\n",
      "Epoch  800/1000 Cost: 0.134279\n",
      "Epoch  900/1000 Cost: 0.122962\n",
      "Epoch 1000/1000 Cost: 0.113422\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((16, 7), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad= True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W,b], lr = 0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1) :\n",
    "    \n",
    "    # Cost 계산 (2)\n",
    "    # z = x_train.matmul(W) + b # or .mm or @\n",
    "    # cost = F.cross_entropy(z.squeeze(), y_one_hot.squeeze()) # 실제 코드로 하니 안돌아감 계속 \n",
    "    \n",
    "    z = F.softmax(x_train.matmul(W) + b, dim = 1) # or .mm or @\n",
    "    cost = (y_one_hot * -torch.log(z)).sum(dim=1).mean()  # cross entropy low level\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번 마다 로그 출력\n",
    "    if epoch % 100 == 0 :\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level Implementation with `nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101, 16])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(16, 7)\n",
    "    \n",
    "    def forward(self, x) :     #forward spell 틀리지 말기;; 틀리면 안돌아감  \n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.5460e-01, -8.3686e-02, -6.0767e-01, -5.7760e-01,  6.3988e-01,\n",
       "          1.8320e-01,  1.1782e-01],\n",
       "        [ 6.1679e-01, -1.9700e-01, -6.4860e-01, -4.3452e-01,  7.6939e-01,\n",
       "          3.9976e-01,  2.4992e-01],\n",
       "        [ 3.6386e-01,  5.6284e-01, -1.5396e-01,  3.0666e-01,  6.3417e-01,\n",
       "         -6.0098e-02,  2.5796e-03],\n",
       "        [ 4.5460e-01, -8.3686e-02, -6.0767e-01, -5.7760e-01,  6.3988e-01,\n",
       "          1.8320e-01,  1.1782e-01],\n",
       "        [ 4.0503e-01, -1.3607e-01, -8.1359e-01, -4.5682e-01,  8.8770e-01,\n",
       "          1.8565e-01,  3.3355e-01],\n",
       "        [ 6.1679e-01, -1.9700e-01, -6.4860e-01, -4.3452e-01,  7.6939e-01,\n",
       "          3.9976e-01,  2.4992e-01],\n",
       "        [ 5.3877e-01, -4.2266e-01, -4.2010e-01, -4.0881e-01,  7.4327e-01,\n",
       "          2.5530e-01,  1.1877e-01],\n",
       "        [ 4.9760e-01,  2.7626e-01,  2.3952e-01,  3.5466e-01,  4.8974e-01,\n",
       "          9.5542e-03, -2.1220e-01],\n",
       "        [ 3.6386e-01,  5.6284e-01, -1.5396e-01,  3.0666e-01,  6.3417e-01,\n",
       "         -6.0098e-02,  2.5796e-03],\n",
       "        [ 5.4592e-01, -2.4074e-01, -2.6179e-01, -7.0202e-01,  2.5496e-01,\n",
       "          3.1673e-01,  9.9736e-02],\n",
       "        [ 4.0503e-01, -1.3607e-01, -8.1359e-01, -4.5682e-01,  8.8770e-01,\n",
       "          1.8565e-01,  3.3355e-01],\n",
       "        [ 1.6517e-01, -6.3473e-01,  9.4258e-02, -3.3114e-01,  6.2039e-01,\n",
       "         -5.0853e-01,  6.0487e-01],\n",
       "        [ 3.6386e-01,  5.6284e-01, -1.5396e-01,  3.0666e-01,  6.3417e-01,\n",
       "         -6.0098e-02,  2.5796e-03],\n",
       "        [ 2.4414e-01,  4.7660e-02, -9.6002e-02,  2.0358e-01,  2.6989e-01,\n",
       "         -2.5656e-01,  7.7751e-02],\n",
       "        [ 4.4360e-01, -5.5743e-01, -3.8531e-01, -5.6185e-01,  8.3485e-01,\n",
       "         -3.6049e-01,  1.6955e-01],\n",
       "        [ 5.6733e-01, -9.5879e-01, -5.8084e-01, -9.6463e-01,  1.1085e+00,\n",
       "         -4.6979e-01,  3.3517e-01],\n",
       "        [ 3.1429e-02, -3.4815e-01, -2.9923e-01, -3.7914e-01,  7.6482e-01,\n",
       "         -5.7818e-01,  8.1965e-01],\n",
       "        [ 6.1679e-01, -1.9700e-01, -6.4860e-01, -4.3452e-01,  7.6939e-01,\n",
       "          3.9976e-01,  2.4992e-01],\n",
       "        [ 4.0628e-01,  4.3332e-01, -1.0636e-01,  4.7909e-01,  8.7466e-01,\n",
       "         -1.2398e-01, -1.9411e-01],\n",
       "        [ 1.4556e-01,  6.7242e-01, -2.9581e-01,  2.6856e-01,  2.6501e-01,\n",
       "          1.8033e-01, -8.9543e-02],\n",
       "        [ 1.6517e-01, -6.3473e-01,  9.4258e-02, -3.3114e-01,  6.2039e-01,\n",
       "         -5.0853e-01,  6.0487e-01],\n",
       "        [ 1.9518e-01, -2.1145e-01, -3.2494e-02, -3.1672e-01,  6.6411e-01,\n",
       "         -2.4940e-01,  4.9660e-01],\n",
       "        [ 6.1679e-01, -1.9700e-01, -6.4860e-01, -4.3452e-01,  7.6939e-01,\n",
       "          3.9976e-01,  2.4992e-01],\n",
       "        [ 2.8561e-01, -5.3861e-01, -8.6633e-02, -1.8442e-01,  8.8701e-01,\n",
       "         -4.2795e-01,  5.3933e-01],\n",
       "        [ 6.6555e-01, -1.3448e+00, -7.1636e-01, -9.7729e-01,  8.4892e-01,\n",
       "         -3.9910e-01,  5.9230e-01],\n",
       "        [ 4.1055e-01, -3.1655e-01, -3.8575e-01, -7.0470e-01,  7.2625e-01,\n",
       "         -1.3191e-01,  1.7967e-01],\n",
       "        [ 5.7084e-01, -1.4094e-01, -1.6839e-01, -6.3290e-01,  5.0527e-01,\n",
       "         -1.7253e-02,  4.4012e-02],\n",
       "        [ 4.8202e-01,  3.3923e-01, -3.8087e-01, -4.0361e-01,  3.9894e-01,\n",
       "          4.3597e-01,  4.8533e-01],\n",
       "        [ 6.1679e-01, -1.9700e-01, -6.4860e-01, -4.3452e-01,  7.6939e-01,\n",
       "          3.9976e-01,  2.4992e-01],\n",
       "        [ 2.5285e-01,  9.2020e-02, -1.8365e-01, -1.4911e-01,  3.4007e-01,\n",
       "          1.4804e-01, -1.7895e-01],\n",
       "        [ 6.9694e-01, -1.3395e+00, -5.9656e-01, -1.1767e+00,  9.9264e-01,\n",
       "         -5.3607e-01,  7.9664e-01],\n",
       "        [ 5.3877e-01, -4.2266e-01, -4.2010e-01, -4.0881e-01,  7.4327e-01,\n",
       "          2.5530e-01,  1.1877e-01],\n",
       "        [ 5.4263e-01,  2.5674e-01, -2.4715e-01, -1.5252e-01,  2.4790e-01,\n",
       "          5.0661e-01, -1.3142e-01],\n",
       "        [-1.6583e-02, -1.5052e-01, -1.9749e-01, -3.3901e-01,  7.8241e-01,\n",
       "         -4.6351e-01,  5.8022e-01],\n",
       "        [ 5.7562e-01,  5.0191e-01,  1.1030e-02,  3.2896e-01,  5.1587e-01,\n",
       "          1.5401e-01, -8.1046e-02],\n",
       "        [ 4.9635e-01, -2.9313e-01, -4.6771e-01, -5.8124e-01,  5.0277e-01,\n",
       "          3.1918e-01,  3.1546e-01],\n",
       "        [ 5.7437e-01, -6.7477e-02, -6.9620e-01, -6.0695e-01,  5.2890e-01,\n",
       "          4.6364e-01,  4.4661e-01],\n",
       "        [ 3.1429e-02, -3.4815e-01, -2.9923e-01, -3.7914e-01,  7.6482e-01,\n",
       "         -5.7818e-01,  8.1965e-01],\n",
       "        [ 3.6386e-01,  5.6284e-01, -1.5396e-01,  3.0666e-01,  6.3417e-01,\n",
       "         -6.0098e-02,  2.5796e-03],\n",
       "        [ 8.3199e-01, -1.1960e+00, -3.2782e-01, -8.6872e-01,  9.2221e-01,\n",
       "         -4.0526e-01,  4.2202e-01],\n",
       "        [ 7.4972e-01, -1.1460e+00, -7.7367e-01, -9.6623e-01,  1.1693e+00,\n",
       "         -3.7546e-01,  6.8882e-01],\n",
       "        [ 3.8713e-05, -3.5350e-01, -4.1903e-01, -1.7970e-01,  6.2110e-01,\n",
       "         -4.4121e-01,  6.1531e-01],\n",
       "        [ 4.8518e-01, -1.2785e+00, -7.6155e-01, -1.1990e+00,  1.1109e+00,\n",
       "         -7.5018e-01,  8.8026e-01],\n",
       "        [ 2.4319e-01, -4.0908e-01, -1.3424e-01, -3.5685e-01,  6.4651e-01,\n",
       "         -3.6407e-01,  7.3602e-01],\n",
       "        [ 4.0503e-01, -1.3607e-01, -8.1359e-01, -4.5682e-01,  8.8770e-01,\n",
       "          1.8565e-01,  3.3355e-01],\n",
       "        [ 4.0503e-01, -1.3607e-01, -8.1359e-01, -4.5682e-01,  8.8770e-01,\n",
       "          1.8565e-01,  3.3355e-01],\n",
       "        [ 5.6733e-01, -9.5879e-01, -5.8084e-01, -9.6463e-01,  1.1085e+00,\n",
       "         -4.6979e-01,  3.3517e-01],\n",
       "        [ 4.0503e-01, -1.3607e-01, -8.1359e-01, -4.5682e-01,  8.8770e-01,\n",
       "          1.8565e-01,  3.3355e-01],\n",
       "        [ 3.5702e-01,  6.1557e-02, -7.1185e-01, -4.1668e-01,  9.0529e-01,\n",
       "          3.0032e-01,  9.4119e-02],\n",
       "        [ 3.6261e-01, -6.5468e-03, -8.6119e-01, -6.2925e-01,  6.4720e-01,\n",
       "          2.4953e-01,  5.3024e-01],\n",
       "        [ 4.0503e-01, -1.3607e-01, -8.1359e-01, -4.5682e-01,  8.8770e-01,\n",
       "          1.8565e-01,  3.3355e-01],\n",
       "        [ 7.4972e-01, -1.1460e+00, -7.7367e-01, -9.6623e-01,  1.1693e+00,\n",
       "         -3.7546e-01,  6.8882e-01],\n",
       "        [ 3.6098e-01, -3.6894e-01, -5.9167e-01, -5.8392e-01,  9.7406e-01,\n",
       "         -1.2946e-01,  3.9539e-01],\n",
       "        [ 7.3349e-01, -1.4897e+00, -7.2876e-01, -1.1950e+00,  1.6227e+00,\n",
       "         -6.4298e-01,  3.0409e-01],\n",
       "        [ 3.6261e-01, -6.5468e-03, -8.6119e-01, -6.2925e-01,  6.4720e-01,\n",
       "          2.4953e-01,  5.3024e-01],\n",
       "        [ 6.1679e-01, -1.9700e-01, -6.4860e-01, -4.3452e-01,  7.6939e-01,\n",
       "          3.9976e-01,  2.4992e-01],\n",
       "        [ 2.5422e-01, -5.4395e-01, -2.0644e-01,  1.5026e-02,  7.4329e-01,\n",
       "         -2.9098e-01,  3.3499e-01],\n",
       "        [ 1.6517e-01, -6.3473e-01,  9.4258e-02, -3.3114e-01,  6.2039e-01,\n",
       "         -5.0853e-01,  6.0487e-01],\n",
       "        [-5.5495e-03, -2.8539e-01, -2.6969e-01,  3.2864e-02,  8.7919e-01,\n",
       "         -3.9042e-01,  1.7919e-01],\n",
       "        [ 2.4319e-01, -4.0908e-01, -1.3424e-01, -3.5685e-01,  6.4651e-01,\n",
       "         -3.6407e-01,  7.3602e-01],\n",
       "        [ 4.0628e-01,  4.3332e-01, -1.0636e-01,  4.7909e-01,  8.7466e-01,\n",
       "         -1.2398e-01, -1.9411e-01],\n",
       "        [ 3.6386e-01,  5.6284e-01, -1.5396e-01,  3.0666e-01,  6.3417e-01,\n",
       "         -6.0098e-02,  2.5796e-03],\n",
       "        [ 3.2181e-01,  4.1176e-01, -8.5002e-02,  2.5331e-01,  1.8813e-01,\n",
       "          8.9131e-02,  1.6793e-01],\n",
       "        [ 3.8859e-01, -2.8074e-01, -8.5650e-01, -2.5556e-01,  1.2806e+00,\n",
       "         -7.4907e-02,  2.1436e-01],\n",
       "        [ 4.0503e-01, -1.3607e-01, -8.1359e-01, -4.5682e-01,  8.8770e-01,\n",
       "          1.8565e-01,  3.3355e-01],\n",
       "        [ 5.3877e-01, -4.2266e-01, -4.2010e-01, -4.0881e-01,  7.4327e-01,\n",
       "          2.5530e-01,  1.1877e-01],\n",
       "        [ 1.4556e-01,  6.7242e-01, -2.9581e-01,  2.6856e-01,  2.6501e-01,\n",
       "          1.8033e-01, -8.9543e-02],\n",
       "        [ 4.0503e-01, -1.3607e-01, -8.1359e-01, -4.5682e-01,  8.8770e-01,\n",
       "          1.8565e-01,  3.3355e-01],\n",
       "        [ 3.2701e-01, -3.6173e-01, -5.8509e-01, -4.3111e-01,  8.6157e-01,\n",
       "          4.1191e-02,  2.0239e-01],\n",
       "        [ 4.0503e-01, -1.3607e-01, -8.1359e-01, -4.5682e-01,  8.8770e-01,\n",
       "          1.8565e-01,  3.3355e-01],\n",
       "        [ 5.3877e-01, -4.2266e-01, -4.2010e-01, -4.0881e-01,  7.4327e-01,\n",
       "          2.5530e-01,  1.1877e-01],\n",
       "        [ 4.2463e-02, -4.8302e-01, -3.7143e-01, -7.2683e-03,  8.6159e-01,\n",
       "         -5.0509e-01,  4.1862e-01],\n",
       "        [ 4.6365e-01, -1.4438e+00, -1.0129e+00, -1.3457e+00,  1.0277e+00,\n",
       "         -4.1257e-01,  7.0923e-01],\n",
       "        [ 5.7562e-01,  5.0191e-01,  1.1030e-02,  3.2896e-01,  5.1587e-01,\n",
       "          1.5401e-01, -8.1046e-02],\n",
       "        [ 2.4791e-01,  9.1827e-01, -2.6699e-01,  3.5829e-01,  1.9388e-01,\n",
       "          3.3849e-01, -4.1308e-01],\n",
       "        [ 3.2207e-01,  4.6452e-01, -6.6844e-01,  7.6284e-02,  7.1537e-01,\n",
       "          2.3164e-01, -3.1742e-02],\n",
       "        [ 2.1077e-01,  8.5506e-01,  2.6804e-01,  1.5238e-01,  8.9357e-02,\n",
       "          4.2538e-01, -3.8521e-01],\n",
       "        [ 3.5642e-01,  4.2090e-01,  2.2310e-01,  3.1551e-01,  6.6514e-02,\n",
       "         -2.7227e-02, -2.9733e-01],\n",
       "        [-1.6583e-02, -1.5052e-01, -1.9749e-01, -3.3901e-01,  7.8241e-01,\n",
       "         -4.6351e-01,  5.8022e-01],\n",
       "        [-1.6583e-02, -1.5052e-01, -1.9749e-01, -3.3901e-01,  7.8241e-01,\n",
       "         -4.6351e-01,  5.8022e-01],\n",
       "        [ 1.6152e-01,  2.3615e-01, -3.0236e-01,  1.8152e-01,  4.0911e-01,\n",
       "         -2.5526e-02,  3.0359e-01],\n",
       "        [ 2.9435e-01, -1.4073e-01, -1.2979e-01,  2.3105e-01,  2.7882e-02,\n",
       "         -7.1190e-02,  9.5461e-02],\n",
       "        [ 5.7562e-01,  5.0191e-01,  1.1030e-02,  3.2896e-01,  5.1587e-01,\n",
       "          1.5401e-01, -8.1046e-02],\n",
       "        [ 2.4319e-01, -4.0908e-01, -1.3424e-01, -3.5685e-01,  6.4651e-01,\n",
       "         -3.6407e-01,  7.3602e-01],\n",
       "        [ 4.5063e-01,  3.3388e-01, -5.0068e-01, -2.0417e-01,  2.5522e-01,\n",
       "          5.7294e-01,  2.8100e-01],\n",
       "        [ 5.0546e-01, -7.5811e-01, -4.8307e-01, -7.6324e-01,  9.7169e-01,\n",
       "         -4.1514e-01,  2.5236e-01],\n",
       "        [ 5.6657e-01,  6.0893e-01,  1.1100e-01,  5.5089e-01,  6.5369e-01,\n",
       "         -9.3219e-03, -3.2977e-01],\n",
       "        [ 2.3760e-01, -3.4098e-01,  1.5110e-02, -1.4429e-01,  9.0460e-01,\n",
       "         -3.1328e-01,  2.9990e-01],\n",
       "        [ 6.6555e-01, -1.3448e+00, -7.1636e-01, -9.7729e-01,  8.4892e-01,\n",
       "         -3.9910e-01,  5.9230e-01],\n",
       "        [ 6.2231e-01, -3.7748e-01, -2.2076e-01, -6.8240e-01,  6.0794e-01,\n",
       "          8.2199e-02,  9.6041e-02],\n",
       "        [ 4.7016e-01, -9.8111e-01, -5.7295e-01, -4.0408e-01,  1.2138e+00,\n",
       "         -2.7630e-01,  2.6235e-01],\n",
       "        [ 4.0899e-01, -5.6657e-01, -6.9341e-01, -6.2405e-01,  9.5647e-01,\n",
       "         -2.4413e-01,  6.3482e-01],\n",
       "        [ 4.0628e-01,  4.3332e-01, -1.0636e-01,  4.7909e-01,  8.7466e-01,\n",
       "         -1.2398e-01, -1.9411e-01],\n",
       "        [ 4.8202e-01,  3.3923e-01, -3.8087e-01, -4.0361e-01,  3.9894e-01,\n",
       "          4.3597e-01,  4.8533e-01],\n",
       "        [ 5.7437e-01, -6.7477e-02, -6.9620e-01, -6.0695e-01,  5.2890e-01,\n",
       "          4.6364e-01,  4.4661e-01],\n",
       "        [ 7.3854e-02, -4.7768e-01, -2.5162e-01, -2.0671e-01,  1.0053e+00,\n",
       "         -6.4206e-01,  6.2295e-01],\n",
       "        [ 4.9306e-01,  2.0436e-01, -4.5307e-01, -3.1740e-02,  4.9571e-01,\n",
       "          5.0906e-01,  8.4305e-02],\n",
       "        [ 9.1001e-01, -9.7038e-01, -5.5631e-01, -8.9443e-01,  9.4834e-01,\n",
       "         -2.6080e-01,  5.5317e-01],\n",
       "        [ 4.0503e-01, -1.3607e-01, -8.1359e-01, -4.5682e-01,  8.8770e-01,\n",
       "          1.8565e-01,  3.3355e-01],\n",
       "        [ 2.9435e-01, -1.4073e-01, -1.2979e-01,  2.3105e-01,  2.7882e-02,\n",
       "         -7.1190e-02,  9.5461e-02],\n",
       "        [ 2.4319e-01, -4.0908e-01, -1.3424e-01, -3.5685e-01,  6.4651e-01,\n",
       "         -3.6407e-01,  7.3602e-01]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.982726\n",
      "Epoch  100/1000 Cost: 0.489954\n",
      "Epoch  200/1000 Cost: 0.331328\n",
      "Epoch  300/1000 Cost: 0.255378\n",
      "Epoch  400/1000 Cost: 0.208865\n",
      "Epoch  500/1000 Cost: 0.177086\n",
      "Epoch  600/1000 Cost: 0.153876\n",
      "Epoch  700/1000 Cost: 0.136125\n",
      "Epoch  800/1000 Cost: 0.122085\n",
      "Epoch  900/1000 Cost: 0.110689\n",
      "Epoch 1000/1000 Cost: 0.101251\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
