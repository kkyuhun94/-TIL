{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습정리\n",
    "\n",
    "### 14장. 단순 회귀 분석\n",
    "* correlation : 두 변수의 선형관계를 갖고 있는지 확인 가능\n",
    "* 선형관계의 존재 여부를 확인하는 것 외에도 더욱 자세히 살펴볼 필요가 있음\n",
    "\n",
    "####  14.1 모델\n",
    "* 회귀 모델 : $y_i = \\beta x_i+\\alpha + \\varepsilon _i$\n",
    "    * ex. 친구수와 사용자가 사이트에서 보내는 시간의 관계를 설명해주는 모델 \n",
    "    * $y_i$ : 사용자 i가 매일 사이트에서 보내는 시간\n",
    "    * $x_i$ : 사용자 i의 친구 수 \n",
    "    * $\\varepsilon$ : 모델이 고려하지 못하는 다른 요소로 발생하는 오류\n",
    "\n",
    "* 오류 : $\\varepsilon_i = y_i - (\\beta x_i + \\alpha) $\n",
    "\n",
    "* 최소자승법(least squares) : 에러의 합을 최소화하는 알파와 베타값을 찾음\n",
    "    * 독립 변수 x의 평균이 주어지면 알파는 종속 변수 y의 평균을 예측\n",
    "    * 베타는 입력변수가 std(x)만큼 증가한다면 예측값이 correlation(x,y)*std(y)만큼 증가한다는 것을 의미\n",
    "        * x와 y가 완벽한 양의 상관관계 : x가 1증가할때 y도 1증가\n",
    "        * 완벽한 음의 상관관계 : x가 증가할때 y는 감소\n",
    "        * 상관관계가 없을 때 : 베타 =0 아무런 영향이 없음\n",
    "\n",
    "* 모델이 주어진 데이터에 얼마나 적합한지 알아보기\n",
    "    * 그래프 시각화\n",
    "    * 결정계수(R제곱 값) : 종속 변수의 총 변화량 중 모델이 잡아낼 수 있는 변화량의 비율\n",
    "        * 최소 0, 최대 1(에러0)\n",
    "        \n",
    "####  14.2 경사 하강법 사용하기\n",
    "* 만약 theta = [alpha,beta]로 정하면 경사하강법을 통해 모델을 만들 수 있음\n",
    "\n",
    "####  14.3 최대가능도 추정법\n",
    "* 최소 자승법을 사용한 이유\n",
    "    * 최대가능도 추정법(max-imum likelihood estimation, MLE)\n",
    "        * 표본데이터 : $v_1,...,v_n$\n",
    "        * $p(v_1,...,v_n|\\theta)$\n",
    "          * $\\theta$를 모른다면 표본이 주어졌을 때 $\\theta$가 발생할 가능도(likelihood)로 생각\n",
    "          * $\\to L(\\theta|v_1,...,v_n)$\n",
    "        * $\\theta$ : 가능도를 최대화해 주는 값\n",
    "        * 확률 질량 함수 대신 확률 밀도 함수를 사용하는 연속형 분포에도 이를 동일하게 적용 가능\n",
    "        * 회귀 분석에서의 오류를 평균 0,표준편차 $\\sigma $인 정규분포를 따른다고 가정\n",
    "            * $ L(\\alpha,\\beta| x_i,y_i,\\sigma ) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-(y_i-\\alpha-\\beta x_i)^2/2\\sigma^2)$\n",
    "            * 전체 데이터에 대한 가능도는 각 데이터의 가능도를 모두 곱한 값\n",
    "            * 오류의 제곱 값을 최소화하는 알파와 베타가 계산되는 지점이 가능도가 최대화되는 지점\n",
    "            * 오류 제곱 값 최소화 = 관측 발생 가능도 최대화 \n",
    "\n",
    "####  14.4 더 공부해 보고 싶다면 \n",
    "* 15장 다중 회귀 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from typing import List, Tuple\n",
    "Vector = List[float]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회귀모델\n",
    "def predict(alpha: float, beta: float, x_i: float) -> float :\n",
    "    return beta*x_i + alpha\n",
    "\n",
    "# 오류\n",
    "def error(alpha: float, beta: float, x_i: float, y_i: float) -> float :\n",
    "    \"\"\"실제 결과가 y_i일 때, betaz*x_i+alpha로 계산된 예측값의 오류\"\"\"\n",
    "    return predict(alpha, beta, x_i) - y_i\n",
    "\n",
    "def sum_of_sqerrors(alpha : float, beta : float, x : Vector, y : Vector) -> float :\n",
    "    return sum(error(alpha,beta, x_i,y_i)**2 for x_i,y_i in zip(x,y))\n",
    "\n",
    "#최소 자승법\n",
    "def least_squares_fit(x : Vector, y : Vector) -> Tuple[float, float] :\n",
    "    \"\"\"x와 y가 학습데이터로 주어졌을 때 오류의 제곱 값을 최소화해 주는 알파와 베타를 계산\"\"\"\n",
    "    beta = np.corrcoef(x,y)[0,1] * np.std(y)/np.std(x)\n",
    "    alpha = np.mean(y) - beta*np.mean(x)\n",
    "    return alpha, beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy에서 두 변수의 상관계수\n",
    "np.corrcoef(x,y)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.0, 3.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [i for i in range(-100,110,10)]\n",
    "y = [3*i-5 for i in x]\n",
    "\n",
    "# y=3x-5를 찾아내기 \n",
    "least_squares_fit(x,y)\n",
    "\n",
    "# y = -5 + 3x으로 예측가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_sum_of_squares(y : Vector) -> float :\n",
    "    \"\"\"평균을 기준으로 y_i의 변화량을 제곱한 값의 총합\"\"\"\n",
    "    return sum( v**2 for v in x-np.mean(x))\n",
    "\n",
    "def r_squared(alpha: float, beta: float, x: Vector, y: Vector) -> float :\n",
    "    \"\"\"모델이 잡아낼 수 있는 y의 변화량의 비율은 1 - 모델이 잡아내지 못하는 y의 변화량의 비율로 계산\"\"\"\n",
    "    return 1.0 - (sum_of_sqerrors(alpha, beta, x, y) / total_sum_of_squares(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.748909090909091"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = -3\n",
    "beta = 3.5\n",
    "rsq = r_squared(alpha, beta, x, y)\n",
    "rsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 오차가 없을 때 최댓값 = 1\n",
    "alpha = -5\n",
    "beta = 3\n",
    "rsq = r_squared(alpha, beta, x, y)\n",
    "rsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 387778.474:   0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gradient_step' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2032864efd63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# 최종적으로, 추측 갱신\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mguess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgrad_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mguess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gradient_step' is not defined"
     ]
    }
   ],
   "source": [
    "# 경사 하강법으로 모델 만들기\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "num_epochs = 10000\n",
    "random.seed(0)\n",
    "\n",
    "guess = [random.random(), random.random()] # 임의의 위치에서 출발\n",
    "learning_rate = 0.00001\n",
    "\n",
    "with tqdm.trange(num_epochs) as t :\n",
    "    for _ in t :\n",
    "        alpha,beta = guess\n",
    "        # 알파에 대한 손실함수의 편미분 : error^2의 미분 2*error\n",
    "        grad_a = sum(2*error(alpha, beta, x_i,y_i)\n",
    "                    for x_i, y_i in zip(x, y))\n",
    "        \n",
    "        grad_b = sum(2*error(alpha, beta, x_i,y_i)*x_i\n",
    "                    for x_i,y_i in zip(x, y))\n",
    "        \n",
    "        # tqdm 설명에 넣기 위해 손실 계산\n",
    "        loss = sum_of_sqerrors(alpha,beta, x, y)\n",
    "        t.set_description(f\"loss: {loss:.3f}\")\n",
    "        \n",
    "        # 최종적으로, 추측 갱신\n",
    "        guess = gradient_step(guess, [grad_a, grad_b], -learning_rate)\n",
    "        \n",
    "alpha, beta = guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
