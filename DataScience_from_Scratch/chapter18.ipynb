{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습정리\n",
    "\n",
    "### 18장. 신경망\n",
    "* 인공신경망(artificial neural network, 신경망) : 뇌의 동작에서 영감을 받은 예측 모델\n",
    "    * 뇌 : 뉴런의 집합\n",
    "        * 각 뉴런이 다른 뉴런이 출력한 결과를 입력 받아 특정 연산을 수행\n",
    "        * 계산 결과가 특정 임계치를 넘으면 활성화, 넘지 않으면 비활성화\n",
    "    * 인공신경망 : 인공 뉴런으로 구성\n",
    "        * 입력값을 받아 계산을 수행\n",
    "        * 신경망의 활용분야 : 필기인식, 얼굴 인식 등 딥러닝에서 활발하게 활용\n",
    "        * 대부분의 신경망은 블랙박스(black box)이다. : 어떻게 문제를 풀고 있는지 제대로 이해하기도, 학습시키기도 어려움\n",
    "        * 인공지능(artificial intelligence)을 만들어 기술적 특이점(Singularity)을 앞당기고자 할 때는 신경망도 좋은 선택\n",
    "  \n",
    "####  18.1 퍼셉트론\n",
    "* 퍼셉트론(perceptron) : n개의 이진수(binary)가 하나의 뉴런을 통과해서 가중합이 0보다 크면 활성화되는 가장 간단한 신경망 구조\n",
    "    * x를 초평면(hyperplane)으로 구분된 두 개의 공간으로 분리\n",
    "    * 가중치(weights)를 변경해서 여러 문제를 풀 수 있음\n",
    "        * ex) 논리게이트 만들기 \n",
    "        * AND게이트 : 입력값이 모두 1이면 1을 반환, 하나라도 1이 아니면 0을 반환 \n",
    "        * OR 게이트 : 입력값이 1을 하나라도 갖고 있으면 1을 반환\n",
    "        * NOT 게이트 : 1을 0으로, 0을 1으로 변환\n",
    "        * XOR 게이트 : 1이 1개일 때 1을 반환, 나머지 0 반환 \n",
    "            * 단일 퍼셉트론으로는 풀 수 없음\n",
    "            * AND,OR게이트로 XOR게이트를 구할 수 있음 : or이지만 and는 아님 \n",
    "\n",
    "####  18.2 순방향 신경망\n",
    "* 인공신경망으로 뇌를 묘사하기\n",
    "    * 구성 : 한 방향으로 연결된 개별 층(layer)으로 추상화하는 것이 일반적\n",
    "       * 입력층(input layer) : 입력값을 받아 그대로 다음층으로 값을 전송\n",
    "       * 은닉층(hidden layer)\n",
    "       * 출력층(output layer)\n",
    "    * 퍼셉트론과 마찬가지로 각 뉴런에는 가중치와 편향이 존재 \n",
    "        * 여기서는 표현을 단순화 하기 위해 편향을 가중치 벡터 끝에 덧붙이고 항상 1의 값을 가지는 편향의 입력값을 사용\n",
    "    * 각 뉴런 : 퍼셉트론과 마찬가지로 입력값과 가중치의 곱을 합함\n",
    "    * 그 값에 step_function을 적용하지 않고 조금 더 부드러운 모양의 시그모이드(sigmoid)함수를 적용\n",
    "        * sigmoid 함수 : step_function을 근사한 매끄러운 곡선\n",
    "        * $\\frac{1}{1+e^{-t}}$\n",
    "        * sigmoid를 쓰는 이유 : 미적분을 사용하기 위해서는 매끄러운 함수(smooth funcion)를 사용해야함\n",
    "    * 여러 뉴런으로 각 층을 구성, 여러 층으로 최종 신경망을 표현\n",
    "        * 출력층 : 은닉층에서 받은 값을 '두번 째 입력값'으로 계산\n",
    "        * 은닉층 : 입력의 특성(feature)을 계산, 출력층은 그 특성을 알맞게 조합 \n",
    "        \n",
    "####  18.3 역전파\n",
    "* 각 뉴런이 어떤 역할을 하는지 알 수 없기 떄문에 신경망을 수동으로 만들지는 않음\n",
    "    * 대신 데이터를 이용해서 신경망을 학습시킴\n",
    "* 역전파(backpropagation) : 신경망을 학습시킬 때 일반적으로 사용되는 방법\n",
    "    * 경사 하강법 또는 그 변이가 사용됨\n",
    "* 역전파 과정    \n",
    "    * 입력 벡터와 출력 벡터로 구성된 학습데이터가 주어졌다고 가정\n",
    "        * ex) xor_network : 입력벡터 : [1,0], 출력벡터 : [1]\n",
    "    * 가중치 조정 알고리즘을 학습 데이터에 수렵할 때까지 반복적으로 적용  \n",
    "        1. 입력 벡터에 대해 feed_forward를 수행, 모든 뉴런의 출력값을 계산\n",
    "        2. 결괏값을 알기 때문에, 오류의 제곱의 합인 손실(loss)을 계산\n",
    "        3. 출력층 뉴런의 가중치에 따라 손실의 그래디언트를 계산\n",
    "        4. 은닉층 뉴런의 그래디언트를 계산하기 위해 출력층의 그래디언트와 오류를 뒤로 '전파'한다\n",
    "        5. 경사 하강법을 한번 진행\n",
    "   \n",
    "####  18.4 예시 : Fizz Buzz\n",
    "* Fizz Buzz 문제 : 유명한 프로그래밍 챌린지 \n",
    "    * 1부터 100까지의 숫자 중에서 \n",
    "    * 3으로 나눠진다면 \"fizz\"를 출력\n",
    "    * 5로 나눠지면 \"buzz\"를 출력 \n",
    "    * 15로 나눠지면 \"fizzbuzz\"를 출력\n",
    "* 신경망을 사용하기 위해서는 정수를 벡터 형태로 변환\n",
    "\n",
    "####  18.5 더 공부해 보고 싶다면 \n",
    "* 19장 딥러닝\n",
    "* 텐서플로로 Fizz Buzz풀기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 퍼셉트론 : x를 초평면으로 구분된 두 공간으로 분리 \n",
    "from typing import List\n",
    "import numpy as np\n",
    "Vector = List[float]\n",
    "\n",
    "def step_function(x: float) -> float :\n",
    "    return 1.0 if x >= 0 else 0.0             # 0보다 클때 1 아니면 0 출력\n",
    "\n",
    "def perceptron_output(weights : Vector, bias : float, x: Vector) -> float :\n",
    "    \"\"\"퍼셉트론이 활성화 되면 1, 아니면 0을 반환\"\"\"\n",
    "    calculation = np.dot(weights, x) + bias      # c = wx + b \n",
    "    return step_function(calculation)            # c가 0보다 크면 1 아니면 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# AND 게이트 : 입력값이 모두 1이면 1을 반환, 하나라도 1이 아니면 0을 반환 \n",
    "and_weights = [2.,2]\n",
    "and_bias = -3.\n",
    "print(perceptron_output(and_weights, and_bias, [1,1]))  # 2+2-3 =1\n",
    "print(perceptron_output(and_weights, and_bias, [0,1]))  # 2+0-3 =-1 : 0출력\n",
    "print(perceptron_output(and_weights, and_bias, [1,0]))  # -1 : 0\n",
    "print(perceptron_output(and_weights, and_bias, [0,0]))  # -3 : 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# OR 게이트 : 입력값이 1을 하나라도 갖고 있으면 1을 반환\n",
    "or_weights = [2.,2]\n",
    "or_bias = -1.\n",
    "print(perceptron_output(or_weights, or_bias, [1,1]))  \n",
    "print(perceptron_output(or_weights, or_bias, [0,1])) \n",
    "print(perceptron_output(or_weights, or_bias, [1,0])) \n",
    "print(perceptron_output(or_weights, or_bias, [0,0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# NOT 게이트 : 1을 0으로, 0을 1으로 변환\n",
    "not_weights = [-2.]\n",
    "not_bias = 1.\n",
    "\n",
    "print(perceptron_output(not_weights, not_bias, [0]))\n",
    "print(perceptron_output(not_weights, not_bias, [1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 논리 게이트를 만들기 위해 꼭 뉴런을 사용할 필요는 없음 \n",
    "# XOR 게이트 : 1이 1개일 때 1을 반환, 나머지 0 반환\n",
    "and_gate = min\n",
    "or_gate = max\n",
    "xor_gate = lambda x,y : 0 if x == y else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid\n",
    "import math\n",
    "\n",
    "def sigmoid(t: float) -> float :\n",
    "    return 1/ (1 + math.exp(-t))\n",
    "\n",
    "def neuron_output(weights : Vector, inputs : Vector) -> float :\n",
    "    # weights에는 편향이 포함, inputs는 1을 포함 -> wx+b -> dot[w,b]*[x,1] \n",
    "    return sigmoid(np.dot(weights, inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 구현\n",
    "def feed_forward(neural_network : List[List[Vector]], input_vector : Vector) -> List[Vector] :\n",
    "    \"\"\"신경망에 입력 벡터를 전달, 모든 층의 결괏값을 반환\"\"\"\n",
    "    outputs : List[Vector] = []\n",
    "    \n",
    "    for layer in neural_network :\n",
    "        input_with_bias = input_vector + [1]\n",
    "        output = [neuron_output(neuron, input_with_bias) for neuron in layer]\n",
    "        outputs.append(output)\n",
    "        \n",
    "        # 이번 층의 결괏값은 다음 층의 입력값이 됨\n",
    "        input_vector = output\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,1]+[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.38314668300676e-14\n",
      "0.9999999999999059\n",
      "0.9999999999999059\n",
      "9.383146683006828e-14\n"
     ]
    }
   ],
   "source": [
    "# XOR게이트 구현\n",
    "# neuron_output이 0또는 1에 아주 가까운 값을 가질 수 있도록 가중치의 크기를 키워줌\n",
    "                                            \n",
    "xor_network = [# 은닉층\n",
    "                [[20., 20, -30],  # and 뉴런\n",
    "                [20., 20, -10]],  # or 뉴런\n",
    "              # 출력층  \n",
    "              [[-60., 60, -30]]] # 1번째 입력값이 아닌 2번째 입력값을 받는 뉴런\n",
    "\n",
    "# 순방향 신경망은 모든 층에 대한 결과를 계산하기 때문에\n",
    "# [-1] (마지막 항)은 '최종 결과'를, [0]은 해당 벡터에서 '값'을 반환\n",
    "print(feed_forward(xor_network, input_vector = [0,0])[-1][0]) # 0\n",
    "print(feed_forward(xor_network, input_vector = [1,0])[-1][0]) # 1  \n",
    "print(feed_forward(xor_network, input_vector = [0,1])[-1][0]) # 1\n",
    "print(feed_forward(xor_network, input_vector = [1,1])[-1][0]) # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient 계산 함수\n",
    "\n",
    "def sqerror_gradients(network : List[List[Vector]], \n",
    "                      input_vector : Vector, \n",
    "                      target_vector : Vector) -> List[List[Vector]] :\n",
    "    \"\"\"신경망, 입력 벡터, 출력 벡터가 주어졌다고 가정,\n",
    "    예측값을 출력, 가중치에 대한 loss의 제곱으로 그래디언트를 계산 \"\"\"\n",
    "    \n",
    "    # 순방향 전파\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "    \n",
    "    # 함수를 적용하기 전 출력층 뉴런의 가중치에 따른 그래디언트\n",
    "    output_deltas = [output*(1-output)*(output-target)\n",
    "                    for output,target in zip(outputs,target_vector)]\n",
    "    \n",
    "    # 출력층 뉴런의 가중치에 따른 그래디언트\n",
    "    output_grads = [[output_deltas[i]*hidden_output\n",
    "                    for hidden_output in hidden_outputs + [1]]\n",
    "                   for i, output_neuron in enumerate(network[-1])]\n",
    "    \n",
    "    # 함수를 적용하기 전 은닉층 뉴런의 가중치에 따른 그래디언트 \n",
    "    hidden_deltas = [hidden_output*(1-hidden_output)*\n",
    "                     np.dot(output_deltas,[n[i] for n in network[-1]])\n",
    "                    for i, hidden_output in enumerate(hidden_outputs)]\n",
    "    \n",
    "    # 은닉층 뉴런의 가중치에 대한 그래디언트\n",
    "    hidden_grads = [[hidden_deltas[i]*input for input in input_vector +[1]]\n",
    "                   for i, hidden_neuron in enumerate(network[0])]\n",
    "\n",
    "    return [hidden_grads, output_grads]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 학습시키기 XOR네트워크사용\n",
    "import random\n",
    "random.seed(0)\n",
    "# 학습 데이터\n",
    "xs = [[0.,0],[0.,1],[1.,0],[1.,1]]\n",
    "ys = [[0.],[1.],[1.],[0.]]\n",
    "\n",
    "# 임의의 가중치로 네트워크를 초기화\n",
    "network = [# 은닉층 : 2개 입력 -> 2개 출력\n",
    "            [[random.random() for _ in range(2+1)],[random.random() for _ in range(2+1)]],\n",
    "             # 출력층 : 2개 입력 -> 1개 출력\n",
    "             [[random.random() for _ in range(2+1)]]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient_step\n",
    "def gradient_step(v: Vector, gradient: Vector, step_size : float) -> Vector :\n",
    "    \"\"\"v에서 step_size만큼 이동하기\"\"\"\n",
    "    step = np.dot(step_size, gradient)\n",
    "    return np.sum([v,step],axis = 0) # add(v,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neural net for xor: 100%|██████████| 20000/20000 [00:05<00:00, 3590.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# 경사 하강법으로 신경망을 학습\n",
    "# 파라미터 벡터 별로 그래디언트가 존재하기 때문에 각가에 대해 gradient_step을 호출\n",
    "from typing import Callable\n",
    "import tqdm\n",
    "learning_rate = 1.0\n",
    "# 네트워크 학습\n",
    "for epoch in tqdm.trange(20000, desc=\"neural net for xor\") :\n",
    "    for x,y in zip(xs,ys) :\n",
    "        gradients = sqerror_gradients(network, x, y)\n",
    "        # 각 층의 각 뉴런에 대해 gradient step을 취한다\n",
    "        network = [[gradient_step(neuron, grad, -learning_rate)\n",
    "                   for neuron,grad in zip(layer, layer_grad)]\n",
    "                  for layer, layer_grad in zip(network,gradients)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009033699537611711\n",
      "0.9923280275107558\n",
      "0.9923292625479107\n",
      "0.007855695468228051\n"
     ]
    }
   ],
   "source": [
    "# XOR\n",
    "print(feed_forward(network, input_vector = [0,0])[-1][0]) # 0\n",
    "print(feed_forward(network, input_vector = [1,0])[-1][0]) # 1  \n",
    "print(feed_forward(network, input_vector = [0,1])[-1][0]) # 1\n",
    "print(feed_forward(network, input_vector = [1,1])[-1][0]) # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([ 6.95350561,  6.95278579, -3.1484762 ]),\n",
       "  array([ 5.11589944,  5.11540788, -7.83960343])],\n",
       " [array([ 10.96170583, -11.63060535,  -5.14422906])]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#최종 학습된 신경망\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fizz Buzz문제 \n",
    "# 출력 값을 벡터 형태로 변환\n",
    "# 4가지 출력값이 있기 떄문에 0,1로 이루어진 4차원 벡터 생성 \n",
    "def fizz_buzz_encode(x : int) -> Vector :\n",
    "    if x % 15 == 0 :\n",
    "        return [0,0,0,1]       # fizzbuzz\n",
    "    elif x % 5 == 0 :\n",
    "        return [0,0,1,0]       # buzz\n",
    "    elif x % 3 == 0 :\n",
    "        return [0,1,0,0]       # fizz\n",
    "    else:\n",
    "        return [1,0,0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0]\n",
      "[0, 1, 0, 0]\n",
      "[0, 0, 1, 0]\n",
      "[0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(fizz_buzz_encode(2))\n",
    "print(fizz_buzz_encode(6))\n",
    "print(fizz_buzz_encode(10))\n",
    "print(fizz_buzz_encode(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력값 변환 : 각 입력값을 이진수로 표현\n",
    "def binary_encode(x : int) -> Vector :\n",
    "    binary : List[float] = []\n",
    "    \n",
    "    for i in range(10) :\n",
    "        binary.append(x % 2)\n",
    "        x = x // 2\n",
    "    \n",
    "    return binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 1, 0, 0, 1, 1, 0, 0, 0]\n",
      "[1, 1, 1, 0, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(binary_encode(0))\n",
    "print(binary_encode(1))\n",
    "print(binary_encode(10))\n",
    "print(binary_encode(101))\n",
    "print(binary_encode(999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 부터 100까지의 숫자에 대해 결과를 산출하는 것이 목표\n",
    "# 101부터 1023(10개의 이진수로 표현할 수 있는 가장 큰 수)로 학습\n",
    "xs = [binary_encode(n) for n in range(101,1024)]\n",
    "ys = [fizz_buzz_encode(n) for n in range(101,1024)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 가중치로 초기화 된 신경망 생성\n",
    "# 10개의 입력뉴런(10차원 벡터)과 4개의 출력 뉴런(4차원 벡터)\n",
    "# 은닉층은 25개 뉴런으로 구성, 차원을 쉽게 바꿀 수 있도록 변수로 생성\n",
    "\n",
    "NUM_HIDDEN = 25\n",
    "\n",
    "network = [ \n",
    "    # 은닉층 : 10개 입력 -> NUM_HIDDEN 출력\n",
    "    [[random.random() for _ in range(10 + 1)] for _ in range(NUM_HIDDEN)],\n",
    "    \n",
    "    # 출력층 : NUM_HIDDEN 입력 -> 4개 출력\n",
    "    [[random.random() for _ in range(NUM_HIDDEN + 1)] for _ in range(4)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fizz buzz (loss :  58.79): 100%|██████████| 500/500 [05:29<00:00,  1.52it/s] \n"
     ]
    }
   ],
   "source": [
    "# 학습과정에서 오차 제곱의 합을 계산 \n",
    "# squared distance(a,b) : scipy.distance.euclidean(a,b)\n",
    "from scipy.spatial import distance\n",
    "\n",
    "learning_rate = 1.0\n",
    "\n",
    "with tqdm.trange(500) as t :\n",
    "    for epoch in t:\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for x,y in zip(xs,ys) :\n",
    "            predicted = feed_forward(network, x)[-1]\n",
    "            epoch_loss += distance.euclidean(predicted, y)\n",
    "            gradients = sqerror_gradients(network, x, y)\n",
    "            \n",
    "            # 각 층의 각 뉴런에 대해 매 그래디언트 만큼 이동\n",
    "            network = [[gradient_step(neuron, grad, -learning_rate)\n",
    "                       for neuron, grad in zip(layer, layer_grad)]\n",
    "                      for layer, layer_grad in zip(network, gradients)]\n",
    "        \n",
    "        t.set_description(f\"fizz buzz (loss : {epoch_loss : .2f})\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트 안의 가장 큰 값의 인덱스를 반환해서 하나의 예측값으로 출력해야함\n",
    "def argmax(xs : list) -> float :\n",
    "    \"\"\"최댓값의 인덱스를 반환\"\"\"\n",
    "    return max(range(len(xs)), key = lambda i : xs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(argmax([0,-1]))\n",
    "print(argmax([-1,0]))\n",
    "print(argmax([-1,10,5,20,-3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1\n",
      "2 2 2\n",
      "3 fizz fizz\n",
      "4 4 4\n",
      "5 buzz buzz\n",
      "6 fizz fizz\n",
      "7 7 7\n",
      "8 8 8\n",
      "9 fizz fizz\n",
      "10 buzz buzz\n",
      "11 11 11\n",
      "12 fizz fizz\n",
      "13 13 13\n",
      "14 14 14\n",
      "15 fizpredicted fizpredicted\n",
      "16 16 16\n",
      "17 17 17\n",
      "18 fizz fizz\n",
      "19 19 19\n",
      "20 buzz buzz\n",
      "21 fizz fizz\n",
      "22 22 22\n",
      "23 23 23\n",
      "24 fizz fizz\n",
      "25 buzz buzz\n",
      "26 26 26\n",
      "27 fizz fizz\n",
      "28 28 28\n",
      "29 29 29\n",
      "30 fizpredicted fizpredicted\n",
      "31 31 31\n",
      "32 32 32\n",
      "33 fizz fizz\n",
      "34 34 34\n",
      "35 buzz buzz\n",
      "36 fizz fizz\n",
      "37 37 37\n",
      "38 38 38\n",
      "39 fizz fizz\n",
      "40 buzz buzz\n",
      "41 41 41\n",
      "42 fizz fizz\n",
      "43 43 43\n",
      "44 44 44\n",
      "45 fizpredicted fizpredicted\n",
      "46 46 46\n",
      "47 47 47\n",
      "48 fizz fizz\n",
      "49 49 49\n",
      "50 buzz buzz\n",
      "51 fizz fizz\n",
      "52 52 52\n",
      "53 53 53\n",
      "54 fizz fizz\n",
      "55 buzz buzz\n",
      "56 56 56\n",
      "57 fizz fizz\n",
      "58 58 58\n",
      "59 59 59\n",
      "60 fizpredicted fizpredicted\n",
      "61 61 61\n",
      "62 62 62\n",
      "63 fizz fizz\n",
      "64 64 64\n",
      "65 buzz buzz\n",
      "66 fizz fizz\n",
      "67 67 67\n",
      "68 68 68\n",
      "69 fizz fizz\n",
      "70 buzz buzz\n",
      "71 71 71\n",
      "72 fizz fizz\n",
      "73 73 73\n",
      "74 74 74\n",
      "75 fizpredicted fizpredicted\n",
      "76 76 76\n",
      "77 buzz 77\n",
      "78 fizz fizz\n",
      "79 79 79\n",
      "80 buzz buzz\n",
      "81 fizz fizz\n",
      "82 82 82\n",
      "83 83 83\n",
      "84 fizz fizz\n",
      "85 85 buzz\n",
      "86 86 86\n",
      "87 fizz fizz\n",
      "88 88 88\n",
      "89 89 89\n",
      "90 fizpredicted fizpredicted\n",
      "91 91 91\n",
      "92 buzz 92\n",
      "93 fizz fizz\n",
      "94 94 94\n",
      "95 buzz buzz\n",
      "96 fizz fizz\n",
      "97 97 97\n",
      "98 98 98\n",
      "99 fizz fizz\n",
      "100 buzz buzz\n",
      "97 / 100\n"
     ]
    }
   ],
   "source": [
    "# 예측\n",
    "num_correct = 0\n",
    "for n in range(1,101) :\n",
    "    x = binary_encode(n)\n",
    "    predicted = argmax(feed_forward(network, x)[-1])\n",
    "    actual = argmax(fizz_buzz_encode(n))\n",
    "    labels = [str(n), \"fizz\",\"buzz\",\"fizpredicted\"]\n",
    "    print(n,labels[predicted], labels[actual])\n",
    "    \n",
    "    if predicted == actual :\n",
    "        num_correct += 1\n",
    "    \n",
    "print(num_correct, \"/\", 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
