{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습정리\n",
    "\n",
    "### 19장. 딥러닝\n",
    "* 딥러닝(deep learning) : 한 개이상의 은닉층을 지닌 '깊은' 신경망을 의미\n",
    "    * 일반적으로 다양한 신경망 구조를 모두 아우는 용어\n",
    "####  19.1 텐서\n",
    "* 복잡한 신경망 구조를 다루다 보면 더 고차원의 배열이 필요\n",
    "* 텐서(tensor) : n차원의 배열 (대부분의 신경망 라이브러리의 명칭 따라서 텐서라 함, 수학적으로 따지면 텐서로 부르면 안된다는 것 인지)\n",
    "    * 이 책에서는 간단하게 텐서를 리스트로 구현\n",
    "        * 벡터, 행렬, 고차원의 텐서는 리스트의 일종으로 볼 수 있지만, 리스트는 n차원의 배열이 아님\n",
    "    \n",
    "####  19.2 층 추상화\n",
    "* 딥러닝에서는 다양한 신경망 구조를 사용가능\n",
    "    * ex. 각 뉴런이 이전 입력값을 기억하도록 만들거나 시그모이드 대신 다른 활성화 함수를 사용\n",
    "    * 두 층보다 더 깊은 신경망생성\n",
    "    * 신경망의 각 층을 나타내는 Layer 추상화 : 입력값에 특정 함수를 적용하거나 역전파를 할 수 있어야 함\n",
    "    * 18장에서 구현한 신경망 : 선형 층위에 시그모이드 층으로 구성된 신경망\n",
    "####  19.3 선형 층\n",
    "* 18장에서 구현한 신경망을 기반으로 뉴런의 dot(weights, inputs)를 나타내는 선형 층(linear layer)을 구현 가능\n",
    "    * 선형 층의 초기값 : 임의로 생성\n",
    "        * 초깃값의 설정은 매우 중요. 파라미터의 초깃값에 따라 신경망의 학습속도,학습 여부 자체가 결정\n",
    "        * 만약 초깃값이 너무 크면 활성화 함수의 그래디언트는 0에 가깝게 됨\n",
    "            * 그래디언트가 0에 가까운 파라미터는 경사하강법으로 학습 불가\n",
    "        * 파라미터를 임의로 생성해 주는 방법\n",
    "            1. 균등 분포 : random.random(), 0과 1사이 임의의 값으로 설정\n",
    "            2. 표준 정규 분포 : 임의의 초깃값 생성\n",
    "            3. Xavier 초기화 : 평균이 0이고 편차가 2/(입력 값의 개수 + 출력 값의 개수)인 정규 분포에서 임의의 초깃값 생성\n",
    "            \n",
    "####  19.4 순차적인 층으로 구성된 신경망\n",
    "* 신경망을 순차적인 층으로 구성되어 있다고 생각\n",
    "    * 여러 층을 하나의 층으로 표현 가능 : 하나의 신경망 자체를 Layer메서드로 하나의 층으로 표현\n",
    "    \n",
    "####  19.5 손실 함수와 최적화\n",
    "* 손실 함수(loss function)와 그래디언트 함수 \n",
    "\n",
    "####  19.6 예시 : XOR문제 다시 풀어 보기\n",
    "####  19.7 다른 활성화 함수\n",
    "####  19.8 예시 : Fizz Buzz 다시 풀어보기\n",
    "####  19.9 Softmax와 Cross-Entropy\n",
    "####  19.10 드롭아웃\n",
    "####  19.11 예시 : MNIST\n",
    "####  19.12 모델 저장 및 불러오기\n",
    "#### 19.13 더 공부해 보고 싶다면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_cdf(x : float, mu: float = 0, sigma: float = 1) -> float :\n",
    "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma )) / 2\n",
    "\n",
    "def inverse_normal_cdf(p : float,\n",
    "                      mu : float = 1,\n",
    "                      sigma : float = 1,\n",
    "                      tolerance : float = 0.00001) -> float :\n",
    "    \"\"\"이진 검색을 사용해서 역함수를 근사\"\"\"\n",
    "    \n",
    "    # 표준정규분포가 아니라면 표준정규분포로 변환\n",
    "    if mu != 0 or sigma != 1:\n",
    "        return mu + sigma*inverse_normal_cdf(p, tolerance=tolerance)\n",
    "    \n",
    "    low_z = -10.0        # normal_cdf(-10)은 0에 근접\n",
    "    hi_z = 10.0          # normal_cdf(10)은 0에 근접\n",
    "    while hi_z - low_z > tolerance :\n",
    "        mid_z = (low_z + hi_z) / 2  # 중간 값\n",
    "        mid_p = normal_cdf(mid_z)   # 중간 값의 누적분포 값 계산\n",
    "        if mid_p < p :\n",
    "            low_z = mid_z           # 중간 값이 너무 작다면 더 큰 값들을 검색\n",
    "        else :\n",
    "            hi_z = mid_z            # 중간 값이 너무 크다면 더 작은 값들을 검색\n",
    "    \n",
    "    return mid_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서를 리스트로 정의\n",
    "Tensor = list\n",
    "\n",
    "# 텐서의 크기를 반환해 주는 도우미 함수 \n",
    "from typing import List\n",
    "\n",
    "def shape(tensor : Tensor) -> List[int] :\n",
    "    sizes : List[int] = []\n",
    "    while isinstance(tensor, list) :\n",
    "        sizes.append(len(tensor))\n",
    "        tensor = tensor[0]\n",
    "    return sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "print(shape([1,2,3]))\n",
    "print(np.shape([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2]\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "print(shape([[1,2],[3,4],[5,6]]))\n",
    "print(np.shape([[1,2],[3,4],[5,6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_1d(tensor: Tensor) -> bool :\n",
    "    \"\"\"만약 tensor[0]이 리스트라면 고차원 텐서를 의미\n",
    "    그러지 않으면 1차원 벡터를 의미\"\"\"\n",
    "    return not isinstance(tensor[0], list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_1d([1,2,3]) # 1차원 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_sum(tensor: Tensor) -> float :\n",
    "    \"\"\"텐서 안의 모든 값의 합을 반환\"\"\"\n",
    "    if is_1d(tensor) :\n",
    "        return sum(tensor) # 벡터의 경우, 파이썬의 기본 함수인 sum을 사용\n",
    "    else :\n",
    "        return sum(tensor_sum(tensor_i) for tensor_i in tensor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(tensor_sum([1,2,3]))\n",
    "print(np.sum([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(tensor_sum([[1,2],[3,4]]))\n",
    "print(np.sum([[1,2],[3,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "def tensor_apply(f: Callable[[float], float], tensor : Tensor) -> Tensor :\n",
    "    \"\"\"텐서 안의 모든 값에 f를 적용\"\"\"\n",
    "    if is_1d(tensor) :\n",
    "        return [f(x) for x in tensor]\n",
    "    else :\n",
    "        return [tensor_apply(f, tensor_i) for tensor_i in tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n",
      "[[2, 4], [6, 8]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor_apply(lambda x : x+1, [1,2,3]))\n",
    "print(tensor_apply(lambda x : 2*x, [[1,2],[3,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크기와 동일한 0텐서를 만듬\n",
    "def zeros_like(tensor: Tensor) -> Tensor :\n",
    "    return tensor_apply(lambda _ : 0.0, tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "[[0.0, 0.0], [0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "print(zeros_like([1,2,3]))\n",
    "print(zeros_like([[1,2],[3,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 텐서의 대칭되는 값에 일괄적으로 함수를 적용할 수 있게 해주는 함수\n",
    "def tensor_combine(f: Callable[[float, float], float],\n",
    "                  t1: Tensor,\n",
    "                  t2: Tensor) -> Tensor :\n",
    "    \"\"\"두 텐서의 대칭되는 t1과 t2에 일괄적으로 함수를 적용\"\"\"\n",
    "    if is_1d(t1) :\n",
    "        return [f(x,y) for x,y in zip(t1, t2)]\n",
    "    else :\n",
    "        return [tensor_combine(f, t1_i, t2_i) for t1_i, t2_i in zip(t1,t2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 7, 9]\n",
      "[4, 10, 18]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "print(tensor_combine(operator.add, [1,2,3], [4,5,6]))\n",
    "print(tensor_combine(operator.mul, [1,2,3], [4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 층 추상화\n",
    "from typing import Iterable,Tuple\n",
    "\n",
    "class Layer :\n",
    "    \n",
    "    \"\"\"딥러닝 신경망은 Layer들로 구성되어 있음\n",
    "    각 Layer별로 순방향으로 입력값에 어떤 계산을 하고 역방향으로 그래디언트를 전파해야함\"\"\"\n",
    "    \n",
    "    def forward(self, input) :\n",
    "        \"\"\"\n",
    "        타입이 명시되어 있지 않은 것을 유의\n",
    "        입력층과 출력값의 타입을 제한하지 않음\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, gradient) :\n",
    "        \"\"\"\n",
    "        역방향에서도 그래디언트의 타입을 제한하지 않을 것\n",
    "        메서드를 호출할 때 유의\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def params(self) -> Iterable[Tensor] :\n",
    "        \"\"\"\n",
    "        해당 층의 파라미터를 반환\n",
    "        기본적으로 아무것도 반환하지 않음\n",
    "        만약 특정 층에서 반환할 파라미터가 없다면 구현할 필요 없음\n",
    "        \"\"\"\n",
    "        return ()\n",
    "    \n",
    "    def grads(self) -> Iterable[Tensor] :\n",
    "        \"\"\"\n",
    "        params()처럼 그래디언트를 반환\n",
    "        \"\"\"\n",
    "        return ()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시그모이드 층 구현\n",
    "def sigmoid(t: float) -> float :\n",
    "    return 1/ (1 + math.exp(-t))\n",
    "\n",
    "class Sigmoid(Layer) :\n",
    "    def forward(self, input: Tensor) -> Tensor :\n",
    "        \"\"\"\n",
    "        입력된 Tensor의 모든 값에 sigmoid를 계산\n",
    "        backpropagation을 위해 결괏값을 저장\n",
    "        \"\"\"\n",
    "        self.sigmoids = tensor_apply(sigmoid, input)\n",
    "        return self.sigmoids\n",
    "    \n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(lambda sig, \n",
    "                              grad: sig*(1-sig)*grad, # 미적분과 연쇄법칙 : output*(1-output)*(output-target) \n",
    "                              self.sigmoids, gradient) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형층에서의 파라미터 생성\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "\n",
    "def random_uniform(*dims: int) -> Tensor :\n",
    "    if len(dims) == 1 :\n",
    "        return [random.random() for _ in range(dims[0])]\n",
    "    else :\n",
    "        return [random_uniform(*dims[1:]) for _ in range(dims[0])]\n",
    "    \n",
    "def random_normal(*dims: int,\n",
    "                 mean : float = 0.0,\n",
    "                 variance : float = 1.0) -> Tensor :\n",
    "    if len(dims) == 1 :\n",
    "        return [mean + variance * inverse_normal_cdf(random.random()) for _ in range(dims[0])]\n",
    "    \n",
    "    else:\n",
    "        return [random_normal(*dims[1:], mean = mean, variance = variance) for _ in range(dims[0])]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(random_uniform(2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded in comparison",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-eef4df6c2003>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-deb6c3f8012f>\u001b[0m in \u001b[0;36mrandom_normal\u001b[0;34m(mean, variance, *dims)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-deb6c3f8012f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-deb6c3f8012f>\u001b[0m in \u001b[0;36mrandom_normal\u001b[0;34m(mean, variance, *dims)\u001b[0m\n\u001b[1;32m     13\u001b[0m                  variance : float = 1.0) -> Tensor :\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvariance\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minverse_normal_cdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-deb6c3f8012f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m                  variance : float = 1.0) -> Tensor :\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvariance\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minverse_normal_cdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-16ed178e0258>\u001b[0m in \u001b[0;36minverse_normal_cdf\u001b[0;34m(p, mu, sigma, tolerance)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# 표준정규분포가 아니라면 표준정규분포로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmu\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minverse_normal_cdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlow_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10.0\u001b[0m        \u001b[0;31m# normal_cdf(-10)은 0에 근접\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001b[0;32m<ipython-input-4-16ed178e0258>\u001b[0m in \u001b[0;36minverse_normal_cdf\u001b[0;34m(p, mu, sigma, tolerance)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# 표준정규분포가 아니라면 표준정규분포로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmu\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minverse_normal_cdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlow_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10.0\u001b[0m        \u001b[0;31m# normal_cdf(-10)은 0에 근접\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded in comparison"
     ]
    }
   ],
   "source": [
    "shape(random_normal(5,6,mean=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19.5 손실 함수와 최적화\n",
    "\n",
    "class Loss :\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        \"\"\"예측 값이 얼마나 정확한가? (손실값이 크면 클수록 좋지 않음)\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor :\n",
    "        \"\"\"예측값이 변하면 손실은 얼마나 변하는가? \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "# 평균제곱 오차 손실함수 \n",
    "class SSE(loss) :\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float :\n",
    "        # 각 예측값의 제곱오차를 계산한 수 텐서로 표현\n",
    "        squared_errors = tensor_combine(\n",
    "            lambda predicted, actual: (predicted - actual)**2, predicted, actual)\n",
    "        \n",
    "        # 모든 제곱오차를 더함\n",
    "        return tensor_sum(squared_errors)\n",
    "    \n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor :\n",
    "        return tensor_combine(\n",
    "            lambda predicted, actual: 2*(predicted - actual),\n",
    "            predicted,\n",
    "            actual)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
