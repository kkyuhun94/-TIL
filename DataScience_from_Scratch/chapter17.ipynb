{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습정리\n",
    "\n",
    "### 17장. 의사결정나무\n",
    "* 정성적인 특징\n",
    "####  17.1 의사결정 나무\n",
    "* 의사 결정나무 : 의사결정 경로(descision path)와 결과(outcome)를 나타내는 데 나무 구조를 사용\n",
    "    1. 분류나무(classification tree) : 결괏값이 이진(binary)인 경우로 고려 \n",
    "    2. 회귀나무(regression tree)\n",
    "* 의사 결정 나무의 장점\n",
    "    1. 이해하기 쉬움\n",
    "    2. 예측 프로세스가 명백\n",
    "    3. 숫자형 데이터와 범주형 데이터를 동시에 다룰 수 있음\n",
    "    4. 특정 변수의 값이 누락되어도 사용 가능\n",
    "* 의사 결정 나무는 새로운 데이터에 대한 일반화 성능이 좋지 않게 오버피팅되기 쉬움\n",
    "\n",
    "####  17.2 엔트로피\n",
    "* 의사결정 나무\n",
    "    * 어떤 질문을 할지 \n",
    "    * 어떤 순서로 질문 할지\n",
    "* 엔트로피(entropy) : 얼마만큼의 정보를 담고 있는가\n",
    "    * 데이터의 불확실성(uncertainty)를 나타낼 때도 같은 표현\n",
    "        * 모든 데이터 포인트가 하나의 클래스에 속하는 경우 : 불확실성이 없고, 엔트로피가 낮다\n",
    "        * 모든 데이터 포인트가 모든 클래스에 고르게 분포 : 불확실성과 엔트로피가 높음\n",
    "* 엔트로피의 수학적 표현 \n",
    "    * 한 데이터 포인트가 클래스 $c_i$에 속할 확률 $p_i$라 하고 0log0=0일때 엔트로피 :\n",
    "        * $H(S) = -p_1log_2p_1-...-p_nlog_2p_n$\n",
    "            * 각 항($-p_ilog_2p_n$)의 값은 항상 0보다 크거나 같음\n",
    "            * $p_i$의 값이 0또는 1에 가까울때 0에 가까워 짐\n",
    "            * 모든 $p_i$가 0 또는 1에 가까움 : 대부분의 데이터 포인트가 하나의 클래스에 속하면 : 엔트로피는 아주 작음\n",
    "            * 대부분의 $p_i$가 0에 가깝지 않으면 : 대부분의 데이터 포인트가 여러 클래스에 고르게 분포 : 엔트로피는 큰값을 가짐\n",
    "\n",
    "* 엔트로피를 구할 때는 어떤 레이블에 어떤 확률값이 주어졌는지 까지 알 필요 없이 확률값만 알면 구할 수 있음\n",
    "\n",
    "####  17.3 파티션의 엔트로피\n",
    "* 의사결정나무의 각 단계는 데이터를 여러개의 파티션(partition)으로 분할\n",
    "    * 하나의 데이터 셋을 여러개의 파티션으로 나누더라도 데이터셋 전체에 관한 엔트로피를 계산할 수 있는 방법이 필요\n",
    "        * 파티션 하나하나가 낮은 엔트로피를 가지는 경우에는 전반적인 엔트로피도 낮음\n",
    "        * 파티션 하나하나가 높은 엔트로피를 가지는 경우에는 전반적인 엔트로피도 높음\n",
    "        \n",
    "* 수학적으로 전체 엔트로피의 계산\n",
    "    * 데이터 S를 $q_1,..,q_m$의 비율을 가지는 파티션 $S_1,...,S_m$으로 나누는 경우의 엔트로피 : 가중합\n",
    "        * $H = q_1H(S_1)+...+q_mH(S_m)$\n",
    "\n",
    "* 파티션의 엔트로피의 문제점 \n",
    "    * 다양한 값을 가지는 변수를 사용해서 파티션을 나누는 경우 오버피팅이 되어 엔트로피가 낮아짐\n",
    "        * 의사결정 나무를 사용할 때는 다양한 값을 가지는 변수를 최대한 피해야 함\n",
    "        * 변수의 여러 값을 적은 수의 버킷(bucket)으로 나눠서 선택 가능한 값의 종류를 줄여야 함\n",
    "        \n",
    "####  17.4 의사결정나무 만들기\n",
    "* 의사결정나무의 구성\n",
    "    1. 결정 노드(decision node) : 답변에 따라 다른 경로로 안내 \n",
    "    2. 잎 노드(leaf node) : 예측 값이 무엇인지 알려줌 \n",
    "* 알고리즘\n",
    "    * 레이블이 있는 데이터와 파티션을 나눌 수 있는 변수의 목록이 주어짐\n",
    "    * greedy(탐욕적) 알고리즘 : 매 순간순간 가장 최적이라고 생각되는 선택을 함\n",
    "        * 모든 데이터 포인트의 클래스 레이블이 동일 : 해당 레이블로 잎노드를 만들고 종료\n",
    "        * 변수 목록이 비어 있다면(더 이상 질문이 없다면) 가장 빈도수가 높은 클래스 레이블로 예측하는 잎 노드를 만들고 종료\n",
    "        * 그게 아니면 각 변수로 데이터의 파티션을 나눔\n",
    "        * 파티션을 나눴을 때 엔트로피가 가장 낮은 변수를 선택\n",
    "        * 선택된 변수로 결정 노드를 생성\n",
    "        * 남아 있는 변수들로 각 파티션에 관해 위 과정을 반복\n",
    "    * ID3 알고리즘 : 상대적으로 이해하기 쉽고 구현하기 쉬움\n",
    "        1. 엔트로피를 계산 : 엔트로피가 최솟값을 갖게하는 파티션을 찾음\n",
    "        2. 찾은 파티션으로 하위나무(subtree)생성\n",
    "        3. 파티션 내에서 또 다시 엔트로피를 계산 : 0을 만드는 속성을 찾음\n",
    "        \n",
    "####  17.5 종합하기\n",
    "* 알고리즘이 동작하는 과정\n",
    "    1. 나무를 어떻게 표현할지 결정\n",
    "        * 잎 : 한 개의 값을 예측\n",
    "        * 하위나무(subtree) : 특정 변수의 값에 따라 나뉨\n",
    "        * 분기 : 처음 보는 변수의 값을 위해 사용할 기본값을 포함\n",
    "    2. 새로운 데이터중에 변수값이 기존에 관찰되지 않은 것일 때\n",
    "        * 최대 빈도 레이블이 달린 default_value의 변수값을 이용 \n",
    "        \n",
    "####  17.6 랜덤 포레스트\n",
    "* 의사결정나무를 쉽게 학습 데이터에 맞출 수 있는 것 만큼 오버피팅이 되는 경향이 있다.\n",
    "* 오버피팅을 방지할 수 있는 대표적인 방법 : 랜덤포레스트(Random forests)\n",
    "    * 랜덤 포레스트 : 여러 개의 의사결정나무를 만들어 다수결로 결과물을 종합\n",
    "        * 분류나무라면 나무의 결과로 과반 투표를 하면 되고, 회귀나무라면 예측값의 평균을 냄\n",
    "        * 랜덤성을 얻기 위한 방법\n",
    "            1. 통합 부트스트랩(bootstrap aggregating), 배깅(bagging)\n",
    "                * inputs 전체를 이용해서 나무를 학습하는 것이 아니라 bootstrap_sample(inputs)의 결과물을 각 나무의 input으로 넣어 학습\n",
    "                * 서로 다른 데이터로 구축된 나무들이 랜덤성이 생기게 됨\n",
    "                * 성능 측정을 잘 설계하면 데이터 전체를 학습에 사용해도 됨  \n",
    "                * 샘플링 하지 않은 데이터를 테스트 데이터로 이용 가능\n",
    "            2. 앙상블 학습 (ensemble learning)\n",
    "                * 파티션을 나누는 변수 best_attribute에 랜덤성을 부여\n",
    "                * 모든 변수 중 최적의 변수를 선택하는 것이 아니라 일부를 선택해서 그 일부 중에서 최적의 변수를 선택\n",
    "                * 성능이 떨어지거나 편향이 높고 분산이 낮은 여러 모델을 동시에 활용해서 전체적으로 성능이 좋은 모델을 구축\n",
    "            \n",
    "####  17.7 더 공부해 보고 싶다면\n",
    "* scikit-learn\n",
    "    * 의사결정나무 모델\n",
    "    * RandomForestClassifier 등 다양한 앙상블 모듈\n",
    "* XGBoost : 캐글과 같은 기계학습 대회에서 종종 우승하는 그래디언트 부스팅(gradient boosting)나무를 학습\n",
    "* 위키피디아 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엔트로피 계산\n",
    "from typing import List\n",
    "import math\n",
    "\n",
    "def entropy(class_probabilities : List[float]) -> float :\n",
    "    \"\"\"클래스에 속할 확률을 입력하면 엔트로피를 계산\"\"\"\n",
    "    return sum(-p*math.log(p,2) for p in class_probabilities if p > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "0.8112781244591328\n"
     ]
    }
   ],
   "source": [
    "print(entropy([1.0]))\n",
    "print(entropy([0.5,0.5]))\n",
    "print(entropy([0.25,0.75]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 엔트로피 계산\n",
    "from typing import Any\n",
    "from collections import Counter\n",
    "\n",
    "def class_probabilities(labels : List[Any]) -> List[float] :\n",
    "    total_count = len(labels)\n",
    "    return [count / total_count for count in Counter(labels).values()]\n",
    "\n",
    "def data_entropy(labels: List[Any]) -> float :\n",
    "    return entropy(class_probabilities(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "0.8112781244591328\n"
     ]
    }
   ],
   "source": [
    "print(data_entropy(['a']))\n",
    "print(data_entropy(['True',\"False\"]))\n",
    "print(data_entropy([3,4,4,4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_entropy(subsets: List[List[Any]]) -> float :\n",
    "    \"\"\"주어진 데이터의 파티션 안의 엔트로피를 반환\"\"\"\n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "    \n",
    "    return sum(data_entropy(subset)*len(subset) / total_count for subset in subsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 데이터\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "class Candidate(NamedTuple):\n",
    "    level : str\n",
    "    lang : str\n",
    "    tweets : bool\n",
    "    phd : bool\n",
    "    did_well : Optional[bool] = None # 레이블이 없는 데이터도 허용\n",
    "                 #   level     lang    tweets   phd    did_well\n",
    "inputs = [ Candidate('Senior','Java','False','False','False'),        \n",
    "          Candidate('Senior','Java','False','True','False'),\n",
    "          Candidate('Mid','Python','False','False','True'), \n",
    "          Candidate('Junior','Python','False','False','True'),\n",
    "          Candidate('Junior','R','True','False','True'),\n",
    "          Candidate('Junior','R','True','True','False'),\n",
    "          Candidate('Mid','R','True','True','True'),\n",
    "          Candidate('Senior','Python','False','False','False'),\n",
    "          Candidate('Senior','R','True','False','True'),\n",
    "          Candidate('Junior','Python','True','False','True'),\n",
    "          Candidate('Senior','Python','True','True','True'),\n",
    "          Candidate('Mid','Python','False','True','True'),\n",
    "          Candidate('Mid','Java','True','False','True'),\n",
    "          Candidate('Junior','Python','False','True','False')\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 총 네가지 변수의 가지를 나눌 수 있다.\n",
    "# 1단계 : 가장 낮은 엔트로피를 반환하는 파티션 찾기 \n",
    "\n",
    "from typing import Dict, TypeVar \n",
    "from collections import defaultdict\n",
    "\n",
    "T = TypeVar('T') # 입력 데이터 inputs를 위한 일반 타입\n",
    "\n",
    "def partition_by(inputs : List[T], attribute : str) -> Dict[Any, List[T]] :\n",
    "    \"\"\"attribute에 따라 inputs의 파티션을 나눔\"\"\"\n",
    "    partitions : Dict[Any, List[T]] = defaultdict(list)\n",
    "    for input in inputs :\n",
    "        key = getattr(input, attribute)  # 특정 attribute의 값을 불러오고\n",
    "        partitions[key].append(input)    # 이 input에 맞는 파티션 추가\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_entropy_by(inputs : List[Any], attribute : str, label_attribute : str) -> float :\n",
    "    \"\"\"주어진 파티션에 대응되는 엔트로피를 계산\"\"\"\n",
    "    # partitions는 inputs로 구성되어 있다.\n",
    "    partitions = partition_by(inputs, attribute)\n",
    "    \n",
    "    # 하지만 partition_entropy에는 클래스 레이블만 있으면 됨\n",
    "    labels = [[getattr(input, label_attribute) for input in partition] for partition in partitions.values()]\n",
    "    \n",
    "    return partition_entropy(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0.6935361388961919\n",
      "lang 0.8601317128547441\n",
      "tweets 0.7884504573082896\n",
      "phd 0.8921589282623617\n"
     ]
    }
   ],
   "source": [
    "# 엔트로피를 최소화 하는 파티션 : level(직급)으로 subtree 생성\n",
    "for key in ['level', 'lang', 'tweets', 'phd'] :\n",
    "    print(key, partition_entropy_by(inputs, key, 'did_well'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# level = Mid의 모두가 True클래스에 속함\n",
    "Mid_inputs = [input for input in inputs if input.level == 'Mid']\n",
    "partition_entropy_by(Mid_inputs, 'level', 'did_well')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# level = Senior은 한 클래스에 모두 포함되지 않음 : 또 다른 subtree 생성\n",
    "senior_inputs = [input for input in inputs if input.level == 'Senior']\n",
    "\n",
    "# Senior의 각 엔트리를 구해서 0또는 최솟값을 찾음 \n",
    "partition_entropy_by(senior_inputs, 'lang', 'did_well')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# level = Senior중 tweets를 하는 사람들은 모두 True\n",
    "partition_entropy_by(senior_inputs, 'tweets', 'did_well') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9509775004326938"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_entropy_by(senior_inputs, 'phd', 'did_well')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# level = Junior의 엔트로피 : 0이 아님\n",
    "Junior_inputs = [input for input in inputs if input.level == 'Junior']\n",
    "partition_entropy_by(Junior_inputs, 'level', 'did_well')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9509775004326938\n",
      "0.9509775004326938\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(partition_entropy_by(Junior_inputs, 'lang', 'did_well'))\n",
    "print(partition_entropy_by(Junior_inputs, 'tweets', 'did_well'))\n",
    "print(partition_entropy_by(Junior_inputs, 'phd', 'did_well')) # 0을 만듬\n",
    "# level = Junior 중 phd는 모두 False를 반환함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트리 구조 잎, 분기 \n",
    "from typing import NamedTuple, Union, Any\n",
    "\n",
    "class Leaf(NamedTuple) : # 잎\n",
    "    value : Any\n",
    "\n",
    "class Split(NamedTuple) : # 분기 \n",
    "    attribute : str          \n",
    "    subtrees : dict\n",
    "    default_value : Any = None\n",
    "\n",
    "DecisionTree = Union[Leaf, Split]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채용 트리 \n",
    "hiring_tree = Split('level', {      # 직급 먼저 고려\n",
    "    'Junior' : Split('phd',         # 주니어 일경우 phd고려\n",
    "        { False : Leaf(True),       # phd : False -> True라고 예측\n",
    "         True : Leaf(False)         # phd : True -> Fasle라고 예측\n",
    "    }),\n",
    "    'Mid' : Leaf(True),             # Mid 일경우 모두 True라고 예측\n",
    "    'Senior' : Split('tweets',      # 시니어 일경우 tweets고려\n",
    "        { False : Leaf(False),       # tweets : False -> False라고 예측\n",
    "         True : Leaf(True)         # tweets : True -> False라고 예측\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 후보자가 기존의 관찰되지 않은 변수값을 가졌을 때 default value사용\n",
    "def classify(tree : DecisionTree, input : Any) -> Any :\n",
    "    \"\"\"의사결정나무로 새로운 입력값을 분류\"\"\"\n",
    "    # 잎 노드이면 값을 반환\n",
    "    if isinstance(tree, Leaf) :\n",
    "        return tree.value\n",
    "    \n",
    "    # 그게 아니라면 변수로 나무의 파티션을 나눔\n",
    "    # 키로 변수값, 값으로 하위나무를 나타내는 dict를 사용\n",
    "    subtree_key = getattr(input, tree.attribute)\n",
    "    \n",
    "    if subtree_key not in tree.subtrees : # 키에 해당하는 하위나무가 없다면\n",
    "        return tree.default_value # 기본값을 반환\n",
    "    \n",
    "    subtree = tree.subtrees[subtree_key] # 적합한 하위나무를 고르고\n",
    "    return classify(subtree, input) # 입력된 데이터를 분류 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 데이터로부터 실제 나무를 구축 : True와 False로만 잎노드 구성 -> 학습데이터에 관해서 만큼은 나무의 예측오류가 0\n",
    "def build_tree_id3(inputs : List[Any],\n",
    "                  split_attributes : List[str],\n",
    "                  target_attribute : str) -> DecisionTree :\n",
    "    # 레이블의 수를 센다\n",
    "    label_counts = Counter(getattr(input, target_attribute) for input in inputs)\n",
    "    most_common_label = label_counts.most_common(1)[0][0]\n",
    "    \n",
    "    # 레이블의 종류가 하나 뿐이라면 그것으로 예측하라\n",
    "    if len(label_counts) == 1 :\n",
    "        return Leaf(most_common_label)\n",
    "    \n",
    "    # 분기할 수 있는 변수가 더 이상 없다면 최대 빈도 레이블을 반환\n",
    "    if not split_attributes:\n",
    "        return Leaf(most_common_label)\n",
    "    \n",
    "    # 그게 아니면 특정 변수로 분기\n",
    "    def split_entropy(attribute: str) -> float :\n",
    "        \"\"\"기준이 될 변수를 선택하는 함수\"\"\"\n",
    "        return partition_entropy_by(inputs, attribute, target_attribute)\n",
    "    \n",
    "    best_attribute = min(split_attributes, key = split_entropy) # 엔트리를 최소화하는 속성을 찾음\n",
    "    \n",
    "    partitions = partition_by(inputs, best_attribute)           # 해당속성으로 분기 \n",
    "    new_attributes = [a for a in split_attributes if a != best_attribute]      \n",
    "    \n",
    "    # 재귀적으로 하위 나무 구축\n",
    "    subtrees = {attribute_value : build_tree_id3(subset, new_attributes, target_attribute)\n",
    "               for attribute_value, subset in partitions.items()}\n",
    "    \n",
    "    return Split(best_attribute, subtrees, default_value = most_common_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "tree = build_tree_id3(inputs, ['level', 'lang', 'tweets', 'phd'], 'did_well')\n",
    "\n",
    "# True로 예측해야함 \n",
    "classify(tree, Candidate(\"Junior\", \"Java\", True, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# False로 예측해야함 \n",
    "not classify(tree, Candidate(\"Junior\", \"Java\", True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 관찰 된적 없는값도 분류 가능. True로 예측해야함\n",
    "classify(tree, Candidate(\"Intern\", \"Java\", True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_candidates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-8d1478f25d08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 만약 파티션 기준으로 사용할 변수가 얼마 남지 않았다면 전부 사용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_candidates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_split_candidates\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msampled_split_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_candidates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 아니라면 랜덤하게 변수를 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'split_candidates' is not defined"
     ]
    }
   ],
   "source": [
    "# 만약 파티션 기준으로 사용할 변수가 얼마 남지 않았다면 전부 사용\n",
    "if len(split_candidates) <= self.num_split_candidates :\n",
    "    sampled_split_candidates = split_candidates\n",
    "\n",
    "# 아니라면 랜덤하게 변수를 선택\n",
    "else:\n",
    "    sampled_split_candidates = random.sample(split_candidates,\n",
    "                                                self.num_split_candidates)\n",
    "        \n",
    "# 선택된 변수 중에서 가장 적절한 변수를 선택\n",
    "best_attribute = min(sampled_split_candidates, key = split_entropy)\n",
    "    \n",
    "partitions = partition_by(inputs, best_attribute)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
