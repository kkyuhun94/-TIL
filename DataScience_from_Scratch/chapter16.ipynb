{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습정리\n",
    "\n",
    "### 16장. 로지스틱 회귀 분석\n",
    "* 종속 변수가 범주형 데이터\n",
    "* 일종의 분류 기법\n",
    "\n",
    "####  16.1 문제\n",
    "* ex. 유료 계정으로 전환할지 예측\n",
    "    * 선형 회귀 분석 : 유료 계정 등록 여부 = $\\beta_0 + \\beta_1 경력 + \\beta_2소득 + \\epsilon$\n",
    "        * 발생하는 몇가지 문제점 \n",
    "            1. 결과값이 아주 큰 양수값 또는 음수값이므로 0과1로 등록여부가 결정되는 예측값을 해석하기 힘들다\n",
    "            2. beta에 관한 추정이 편향됨 : 실제 y의 최댓값은 1, 아주 큰 예측값은 큰 음의 오류를 발생시킴\n",
    "\n",
    "####  16.2 로지스틱 함수\n",
    "* 로지스틱 회귀 분석 : 로지스틱 함수(logistic function)를 사용\n",
    "    * 로지스틱 함수는 입력값이 양의 방향으로 커질수록 출력값이 1에 가까워짐\n",
    "    * $y_i = f(x_i\\beta)+\\epsilon$\n",
    "    * 선형 회귀 분석에서의 가능도(likelihood) : 오차 제곱 합을 최소화 하는 베타가 가능도도 최대화 함\n",
    "    * 로지스틱 회귀 분석에서의 가능도 : 오차 제곱합과 별개\n",
    "        * 경사 하강법(gradient descent)으로 직접 최대화\n",
    "        * $\\beta$가 주어졌을 때 각 $y_i$ \n",
    "            * $f(x_i\\beta)의 확률로 1$\n",
    "            * $1-f(x_i\\beta)의 확률로 0$\n",
    "* $y_i$의 확률 밀도 함수(probability density function)\n",
    "    * $p(y_i|x_i,\\beta) = f(x_i\\beta)^{y_i}(1-f(x_i\\beta))^{1-y_i}$\n",
    "        * $y_i$가 0이면 : $1-f(x_i\\beta)$\n",
    "        * $y_i$가 1이면 : $f(x_i\\beta)$\n",
    "    * 로그 가능도(log likelihood)를 최대화\n",
    "        * $logL(\\beta|x_i,y_i) = y_ilogf(x_i\\beta)+(1-y_i)log(1-f(x_i\\beta))$\n",
    "        * 로그 함수 : 단조 증가함수, 로그 가능도를 최대화 하는 beta는 가능도 또한 최대화 (역(최소화)도 성립)\n",
    "            * 경사 하강법은 함수를 최소화 할 때 사용하므로 로그 가능도의 부호를 바꾼 네거티브(negative)로 그 가능도를 사용\n",
    "        * 데이터 포인트 끼리 서로 독립일 때 데이터 전체의 가능도 = 개별 데이터 포인트의 가능도의 단순 곱\n",
    "            * 데이터 전체에 대한 로그 가능도 = 개별 데이터 포인트의 가능도의 단순 합 \n",
    "            * 미분해서 그래디언트를 구할 수 있음\n",
    "    \n",
    "####  16.3 모델 적용하기\n",
    "* 데이터를 학습 데이터와 테스트 데이터로 분할, 경사 하강법(batch gradient descent)을 적용\n",
    "* 결과를 통해서 다른 조건이 동일할때 한가지 특성의 영향은 파악할 수 있다 \n",
    "\n",
    "####  16.4 적합성(Goodness of fit)\n",
    "* 평가용으로 따로 빼 놓았던 데이터를 사용\n",
    "    * 예측 값이 0.5를 초과할 때마다 사용자가 유료 계정으로 등록한다고 예측\n",
    "* 정밀도(precision) : TP / (TP+FP) - 유료계정이라고 예측했는데 유료계정 이용자일 확률 \n",
    "* 재현율(recall) : TP / (TP+FN) - 유료 계정 이용자를 유료 계정이용자라고 예측할 확률\n",
    "\n",
    "####  16.5 서포트 벡터 머신\n",
    "* dot(beta_hat, x_i) = 0인 부분이 두 클래스의 경계면\n",
    "* 초평면(hyperplane) : 전체 파라미터 공간을 '유료계정'과 '유료 계정이 아닌' 두개의 부분 공간으로 나누는 경계면\n",
    "    * 가능도를 최대화하는 결과의 부산물로 얻을 수 있음\n",
    "* 애초에 초평면을 찾는 것을 목적으로 하는 것도 존재 : support vector machine(SVM)     \n",
    "    * 각 클래스 안의 초평면과 가장 가까운 점의 거리를 최대화 하는 방식\n",
    "    * 데이터를 분류할 수 있는 초평면이 아예 존재하지 않을 수도 있음\n",
    "* 커널 트릭(kernel trick) : 데이터가 선형적으로 구분될 수 있는 조금 더 차원이 높은 공간으로 데이터를 변환하여 보냄\n",
    "    * 데이터 포인트를 더 높은 차원의 공간으로 직접 매핑하기 보다 '커널 함수'로 더 높은 차원에서의 내적을 계산하고 그것으로 초평면을 찾음\n",
    "    \n",
    "####  16.6 더 공부해 보고 싶다면\n",
    "* scikit-learn : 로지스틱 회귀 분석, 서포트 벡터 머신을 위한 모델 제공\n",
    "* scikit-learn - LIBSVM : 서포트 벡터 머신 모듈이 사용하는 구현체 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로지스틱 함수\n",
    "def logistic(x : float) -> float :\n",
    "    return 1.0 / (1+math.exp(-x))\n",
    "\n",
    "# 로지스틱 함수 미분 \n",
    "def logistic_prime(x: float) -> float :\n",
    "    y = logistic(x)\n",
    "    return y *(1-y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네거티브 로그 가능도(경사 하강법은 최소화하는 함수이므로 가능도의 부호를 바꿔서 찾음)\n",
    "import math\n",
    "from typing import List\n",
    "Vector = List[float]\n",
    "import numpy as np\n",
    "\n",
    "def _negative_log_likelihood(x : Vector, y: float, beta : Vector) -> float :\n",
    "    \"\"\"데이터 포인트의 네거티브 로그 가능도\"\"\"\n",
    "    if y == 1:\n",
    "        return -math.log(logistic(np.dot(x,beta))) # f\n",
    "    else :\n",
    "        return -math.log(1 - logistic(dot(x,beta))) # 1-f\n",
    "    \n",
    "    \n",
    "# 로그 가능도의 단순 합\n",
    "def negative_log_likelihood(xs : List[Vector],\n",
    "                           ys : List[float],\n",
    "                           beta : Vector) -> float :\n",
    "    return sum(_negative_log_likelihood(x, y, beta) for x, y in zip(xs, ys))\n",
    "\n",
    "# 미적분 \n",
    "# vector_sum : np.sum(x , axis = 0)\n",
    "\n",
    "def _negative_log_partial_j(x: Vector, y: float, beta: Vector, j: int) -> float :\n",
    "    \"\"\"데이터 포인트 j번째 편미분, 여기서 i는 데이터 포인트의 인덱스를 의미\"\"\"\n",
    "    return -(y - logistic(dot(x, beta))) * x[j]\n",
    "\n",
    "def _negative_log_gradient(x: Vector, y: float, beta: Vector) -> Vector :\n",
    "    \"\"\"데이터 포인트의 그래디언트 \"\"\"\n",
    "    return [_negative_log_partial_j(x, y, beta, j) for j in range(len(beta))]\n",
    "\n",
    "def negative_log_gradient(xs : List[Vector], ys : List[float], beta : Vector) -> Vector :\n",
    "    return vector_sum([_negative_log_gradient(x, y, beta) for x,y in zip(xs, ys)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recaled_xs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2c61db4e880d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecaled_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'recaled_xs' is not defined"
     ]
    }
   ],
   "source": [
    "# 모델 적용\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "\n",
    "random.seed(0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(recaled_xs, ys, 0.33) # machine_learning \n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 임의의 시작점\n",
    "beta = [random.random() for _ in range(3)]\n",
    "\n",
    "with tqdm.trange(5000) as t :\n",
    "    for epoch in t :\n",
    "        gradient = negative_log_gradient(x_train,y_train, beta)\n",
    "        beta = gradient_step(beta, gradient, -learning_rate)\n",
    "        loss = negative_log_likelihood(x_train, y_train, beta)\n",
    "        t.set_description(f\"loss: {loss:.3f} beta: {beta}\")\n",
    "\n",
    "# beta = [-2.0, 4.7, -4.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale된 데이터를 다시 변환\n",
    "from scratch.working_with_data import scale\n",
    "\n",
    "means, stdevs = scale(xs)\n",
    "beta_unscaled = [(beta[0] - beta[1]*means[1] / stedvs[1] - beta[2]*means[2] / stedvs[2]),\n",
    "                beta[1] / stedvs[1],\n",
    "                beta[2] / stedvs[2]]\n",
    "\n",
    "# [8.9, 1.6, -0.000288]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적합성\n",
    "true_positives = false_positives = true_negatives = false_negatives = 0\n",
    "\n",
    "for x_i, y_i in zip(x_test, y_test) :\n",
    "    prediction = logistic(dot(beta, x_i))\n",
    "    \n",
    "    if y_i == 1 and prediction >= 0.5 :\n",
    "        true_positives += 1\n",
    "    elif y_i == 1:\n",
    "        false_negatives += 1\n",
    "    elif prediction >= 0.5 :\n",
    "        false_positives += 1\n",
    "    else :\n",
    "        true_negatives += 1\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives) # 정밀도\n",
    "recall = true_positives / (true_positives + false_negatives) # 재현율\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값 시각화\n",
    "predictions = [logistic(np.dot(beta, x_i)) for x_i in x_test]\n",
    "plt.scatter(predictions, y_test, marker = '+')\n",
    "plt.xlabel(\"predicted probability\")\n",
    "plt.ylabel(\"actual outcome\")\n",
    "plt.title(\"Logistic Regression Predicted vs. Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
