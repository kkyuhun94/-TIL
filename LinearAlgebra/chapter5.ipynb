{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Introduction to Linear Algebra\n",
    "> ### 📌선형대수학개론\n",
    "> 강좌를 들은 후 복습, 요약하면서 TIL에 기록\n",
    "    \n",
    "* [1. Linear Equations in Linear Algebra](https://github.com/kkyuhun94/TIL/blob/master/LinearAlgebra/1.LinearEquations_in_LinearAlgebra.md)\n",
    "* [2. Matrix Algebra]()\n",
    "* [3. Determinants]()\n",
    "* [4. Eigenvalues and Eigenvector]()\n",
    "* [5. Orthogonality and Least Squares]()\n",
    "* [6. Symmetric Matrices and Quadratic Forms]()\n",
    "* [7. Extra Algorithms]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    " \n",
    "### 5. Orthognality and Eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">5.1 Inner Product, Length, and Orthogonality\n",
    "\n",
    "* Inner product\n",
    "    * u,v를 inner product : 각각의 entry를 곱해서 더함\n",
    "    * $u\\centerdot v = u^Tv$\n",
    "        * $u\\centerdot v=v\\centerdot u$\n",
    "    \n",
    "* Theorem 1\n",
    "    * $a. u,v,w in R^n, scalar c$\n",
    "    * $b. (u+v) \\centerdot w = u \\centerdot w +v \\centerdot w $\n",
    "    * $c. (cu) \\centerdot v= c(u \\centerdot v) $\n",
    "    * $d. u\\centerdot u \\geq 0,$ \n",
    "        * $u\\centerdot u=0 \\leftrightarrow u=0$\n",
    "\n",
    "* Length of Vector \n",
    "    * $v$의 length = norm = $ \\lVert v \\rVert $\n",
    "    * unit vector : $ \\lVert v \\rVert $가 1\n",
    "    * normalizing $v$: $v$의 길이가 1이 되도록 $ \\lVert v \\rVert $를 나눠줌, 방향은 같음\n",
    "    * Distaznce in $R^n$ : dist($u,v$) = $ \\lVert u-v \\rVert $\n",
    "    \n",
    "* Orthogonality : orthogonal \n",
    "    * Orthogonal Vectorss in $R^2,R^3$ : 직각관계\n",
    "        * $dist(u,-v)=dist(u,v)$\n",
    "        * $u,v$가 orthogonal : $u\\centerdot v=0$\n",
    "        * 0벡터는 모든 벡터에 orthogonal\n",
    "\n",
    "* Theorem 2. Pythagorean Theorem\n",
    "    * $u,v$가 orthogonal $\\leftrightarrow   {\\lVert u+v \\rVert}^2= {\\lVert u \\rVert}^2+ {\\lVert v \\rVert}^2 $\n",
    "\n",
    "* Orthogonal complement \n",
    "    * 평면 $W$가 $R^3$에 있고, 직선 $L$이 $W$에 직각 : $z\\centerdot w$ \n",
    "    * $z$가 $R^n$의subspace $W$의 모든 벡터에 수직일때, $z$는 $W$에 orthogonal \n",
    "    * set of all vector $z$가 orthogonal to $W$일때,\n",
    "        * $z$는 $W$의 orthogonal complement, $z= W^{\\perp}$\n",
    "    * subspace 복습\n",
    "        * 1. 0을 포함\n",
    "        * 2. vector sum에 닫혀있다\n",
    "        * 3. scalar multiplication에 닫혀있다.\n",
    "\n",
    "* Theorem 3\n",
    "    * $(Row A)^\\perp=Nul A$ and $(Col A)^\\perp=Nul A^T$\n",
    "    * $Row A : A$의 row들을 벡터화 하고 span한 space\n",
    "    * $Col A : A$의 column space\n",
    "    * $Nul A : A$의 Null space ($Ax=0$을 반족하는 모든 $x$)\n",
    "        \n",
    ">5.2 Orthogonal Sets\n",
    "\n",
    "* Orthogonal set\n",
    "    * {$u_1,...,u_p$in $R^n$에서 나올수 있는 모든 pair가 orthogonal set.\n",
    "    * 항상 $u_i\\centerdot u_j=0$를 성립\n",
    "\n",
    "* Theorem 4\n",
    "    * $S$ = {$u_1,...,u_p$}가 orthogonal set, nonzero vectors일 때\n",
    "    * $S$ 는 linearly independent, unique trivial solution\n",
    "    \n",
    "* Orthogonal basis\n",
    "    * subspace $W$ of $R^n$의 orthogonal basis는 W의 basis이며 orthogonal set이다.\n",
    " \n",
    "* Theorem 5 : $R^n$의 subspace $W$의 orthogonal basis {$u_1,...,u_p$}, \n",
    "    * $y$ in $W$일 때 weights in the linear combination\n",
    "        * $y = c_1u_1+...+c_pu_p$\n",
    "    * weight : $c_j = \\frac{y \\centerdot {u_j}}{u_j\\centerdot u_j}$\n",
    "\n",
    "* Orthogonal projection\n",
    "    * $B={u_1,..,u_n}$이고 $R^n$의 orthogonal basis\n",
    "        * $y = c_1u_1+...+c_nu_n$\n",
    "    * orthogonal projection of $y$ onto $u_1$\n",
    "        * $\\hat y=proj_Ly=\\frac{y\\centerdot u_1}{u_1\\centerdot u_1}u_1$\n",
    "        * $y =\\hat y + z$ ,$z$는 $u_1$에 orthogonal한 $y$의 component\n",
    "        \n",
    "* Orthonormal set\n",
    "    * orthonormal : orthogonal set of unit vectors\n",
    "    * 모두 nonzero, 길이 1인 orthogonal set\n",
    "\n",
    "* Theorem 6\n",
    "    * $m$x$n$ matrix $U$가 orthonormal columns을 갖는다\n",
    "    * $\\leftrightarrow U^TU=I$\n",
    "\n",
    "* Theorem 7 \n",
    "    * $m$x$n$ matrix $U$가 orthonormal columns을 갖는다\n",
    "    * $x,y$in $R^n$ \n",
    "    * $\\to a,b,c$\n",
    "        * a. $\\lVert Ux\\rVert =\\lVert x\\rVert$ : length가 같다 \n",
    "        * b. $(Ux)\\centerdot (Uy)=x\\centerdot y$\n",
    "        * c. $(Ux)\\centerdot (Uy)=0 \\leftrightarrow x\\centerdot y=0$\n",
    "\n",
    ">5.3 Orthogoanl Projections\n",
    "\n",
    "* Orthogonal projection\n",
    "    * $y$를 직선 $L$위에 Orthogonal projection : $\\hat y = proj_Ly=\\frac{y\\centerdot u}{u\\centerdot u}u$\n",
    "\n",
    "* Theorem 8. The orthogonal Decomposition Theorem\n",
    "    * $W$가 $R^n$의 subspace이고, $y$는 $R^n$에 존재할때 : $y = \\hat y +z$\n",
    "        * $\\hat y$ in $W, z$ in $W^\\perp$\n",
    "        * $\\hat y$ : orthogonal pojection of $y$ onto $W$ = $proj_Wy$ \n",
    "            * $y$가 $W$=Span{$u_1,..,u_p$}에 존재할때, $proj_Wy=y=\\hat y,z=0$\n",
    "\n",
    "* Theorem 9. The Best Approximation Theorem \n",
    "    * $W$가 $R^n$의 subspace이고, $y$는 $R^n$에 존재하는 어떤벡터이고 $\\hat y$ 가 $W$에 $y$를 projection\n",
    "        * $\\hat y$가 y와 W사이의 거리가 가장 가까운 곳이다.\n",
    "        * $\\lVert y-\\hat y \\rVert$<$\\lVert y-v \\rVert$\n",
    "\n",
    "* Theorem 10\n",
    "    * {$u_1,..,u_p$}가 $R^n$의 subspace $W$의 orthonormal basis이면\n",
    "        * $U$가 $[u_1,u_2,...u_p]$일때: $proj_Wy=UU^Ty$\n",
    "        * \n",
    "\n",
    ">5.4 The Gram-Schmidt Process\n",
    "\n",
    "* Basic Idea for the Gram-Schmidt Process\n",
    "    * {$u_1,u_2$}가 $W=$Span{$u_1,u_2$}의 basis이면 linearly independent set:\n",
    "        * $v_1=u_1,v_2=u_2-\\hat u_2$\n",
    "        * $v_1$ in $W$, $v_2$ in $W$, orthogonal, linearly independent set.\n",
    "        * $W$ = Span{$v_1,v_2$} = Span{$u_1,u_2$} : 동일한 subspace\n",
    "    * {$u_1,u_2,u_3$}일 경우 : $W$ = Span{$v_1,v_2,v_3$}= Span{$u_1,u_2,u_3$}\n",
    "        * $v_1=u_1,v_2=u_2-\\hat u_2,v_3=u_3-\\hat u_3$\n",
    "\n",
    "* The Gram-Schmidt process\n",
    "    * subspace W의 basis{$x_1,...,x_p$}가 존재할때,\n",
    "        * {$v_1,...,v_p$}는 orthogonal basis for $W$\n",
    "        * Span{$v_1,...,v_k$}=Span{$x_1,...,x_k$} for 1$\\leq k \\leq p$\n",
    "\n",
    "* Orthonomal Bases\n",
    "    * orthogonal basis for W를 Normalize : $v_k$를 $\\lVert v_k \\rVert$로 나눔\n",
    "\n",
    "* QR factorization of Matrices\n",
    "    * 1. m x n matrix $A= [x_1,x_2,...,x_n]$: linearly independant columns\n",
    "        * a basis for $Col A$: {$x_1,...x_n$}\n",
    "    * 2. a orthonormal basis for $Col A$: {$u_1,...u_n$}\n",
    "        * $Q =[u_1,...u_n]$\n",
    "        * $x_k$: linear combination of $u_1,...u_k$\n",
    "    * 3. $x_k=Qr_k$\n",
    "        * $A =[x-1 x_2,...,x_n]=Q[r_1,r_2,...,r_n]=QR$ \n",
    "    * 4. R은 upper triangular matrix, 모든 diagonal은 nonzero : n개의 pivot을 가짐(nonzero)\n",
    "        * $Rc=0$, $QRc=Q0=0$, $Ac=0$\n",
    "        * R is invertible, only trivial solution\n",
    "\n",
    "* Theorem 12. The QR Factorization\n",
    "    * m x n matrix A, linearly independent columns : A=QR\n",
    "        * Q : orthonormal basis for Col A\n",
    "        * R : uppertriangular invertible matrix\n",
    "\n",
    "* QR Factorization Steps \n",
    "    * 1. Gram-Schmidt를 사용, orthonormal basis for Col A를 구함 : Q\n",
    "    * 2. $Q^TA=Q^T(QR)=IR=R$을 만족하는 R\n",
    "        * $x_k = r_{1k}u_1+r_{2k}u_2+..+r_{kk}u_k+0u_{k+1}+...+0u_n$ \n",
    "        * $x_{k+1}=r_{1k+1}u_1+r_{2{k+1}}u_2+....+r_{{k+1}{k+1}}u_{k+1}+0u_{k+2}+...+0u_n$ \n",
    "    * 3. $R$의 pivot($r_{kk}$)이 음수일때,$r_{kk}$부터 $r_{kn}$까지 $u_k$를 $(-u_k)$로 바꿔서 곱한다.\n",
    "    \n",
    ">5.5 Least-Squares Problems\n",
    "\n",
    "* Least-squares problems and solution\n",
    "    * $Ax=b$ 형태는 실제로 잘 일어나지 않음 : $A\\hat x$ 가 $b$에 근사\n",
    "    * $\\lVert b-Ax \\rVert$를 최소화 하는 $x=\\hat x$\n",
    "    * $b$와 $A\\hat x$사이 거리를 최소화 : orthogonal projection\n",
    "    * $A\\hat x=\\hat b=proj_{Col A}b$는 consistent(항상 해가 존재)\n",
    "    * $Col A,b-A\\hat x$는 서로 orthogonal\n",
    "    * normal equations for $Ax = b$ : $A^TAx=A^Tb$\n",
    "\n",
    "* Theorem 13\n",
    "    * least-squares solutions of $Ax=b$ = normal equations $A^TAx=A^Tb$의 solutions\n",
    "        * $A^T(b-A\\hat x)=0$\n",
    "        * $b-A\\hat x$가 $ColA$에 orthogonal\n",
    "        * $b = A\\hat x+(b-A\\hat x)$ : unique orthogonal decomposition\n",
    "        * $A\\hat x$은 반드시 $b$의 $ColA$에 onto하는 orthogonal projection이 되어야 한다.\n",
    "\n",
    "* Theorem 14        \n",
    "    * $A^TA$ 가 invertible $\\leftrightarrow A$의 column들이 linearly independent\n",
    "        * 이때, $Ax=b$는 unique least-squares solution $\\hat x$를 갖는다.\n",
    "        * $\\hat x =(A^TA)^{-1}A^Tb$\n",
    "\n",
    "* Theorem 15\n",
    "    * m x n matrix A가 컬럼끼리 linearly independent할 때, A=QR이면\n",
    "        * Ax=b가 unique least-squares solution을 가지고,\n",
    "        * $\\hat x=R^{-1}Q^Tb$\n",
    "\n",
    "* Least-squares fitting of Other Curves : 곡선의 경우 근사 \n",
    "    * $y=\\beta_0+\\beta_1x+\\beta_2x^2$\n",
    "    * predicted $y$-value = Observed $y$-value\n",
    "* Weighted Least-Squares(Advanced Topic)\n",
    "    * 각각의 error($y-\\hat y$)에 가중치를 둠.\n",
    "    * $\\lVert Wy-W\\hat y\\rVert$\n",
    "        * $WX\\beta=Wy$\n",
    "        * $(WX)$\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
