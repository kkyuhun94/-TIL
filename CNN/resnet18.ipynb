{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "# from collections import OrderedDict\n",
    "%matplotlib inline\n",
    "\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계층적인 폴더 구조를 갖고 있는 데이터셋을 불러올때 사용 : 폴더 이름 = 클래스 명\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device =='cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another\n",
    "dataset = ImageFolder('data/Images')\n",
    "\n",
    "test_pct = 0.2\n",
    "test_size = int(len(dataset)*test_pct)\n",
    "dataset_size = len(dataset) - test_size\n",
    "\n",
    "val_pct = 0.1\n",
    "val_size = int(dataset_size*val_pct)\n",
    "train_size = dataset_size - val_size\n",
    "\n",
    "train, val, test = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# custom Dataset\n",
    "class DogData(Dataset) :\n",
    "    def __init__(self, ds, transform = None) :\n",
    "        self.ds = ds\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self) :\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, idx) :\n",
    "        img, label = self.ds[idx]\n",
    "        if self.transform :\n",
    "            img = self.transform(img)\n",
    "            return img, label\n",
    "\n",
    "        \n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225]) \n",
    "                                      ])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "                                     transforms.Resize(255), \n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                                     transforms.Resize(255), \n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "train = DogData(train, train_transforms)\n",
    "val = DogData(val, val_transforms)\n",
    "test = DogData(test, test_transforms)\n",
    "\n",
    "\n",
    "num_workers = 4\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, \n",
    "                                              num_workers=num_workers, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, \n",
    "                                            num_workers=num_workers, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size,\n",
    "                                             num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ResNet18을 위해 최대한 간단히 수정한 BasicBlock 클래스 정의 : 두개의 convolution으로 구성\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        # 3x3 필터를 사용 (너비와 높이를 줄일 때는 stride 값 조절)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes) # 배치 정규화(batch normalization)\n",
    "\n",
    "        # 3x3 필터를 사용 (패딩을 1만큼 주기 때문에 너비와 높이가 동일)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes) # 배치 정규화(batch normalization)\n",
    "\n",
    "        self.shortcut = nn.Sequential() # identity인 경우\n",
    "        if stride != 1: # stride가 1이 아니라면, Identity mapping(입력값과 출력값이 같음)이 아닌 경우\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x) # (핵심) skip connection\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ResNet 클래스 정의\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=120):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # 64개의 3x3 필터(filter)를 사용\n",
    "        # 논문과 필터 개수도 동일하게 사용\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(25088, num_classes) #fully connected layer\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1) # stride가 맨처음 convolution 연산에만 2로 적용, 그 이후에 1\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes # 다음 레이어를 위해 채널 수 변경\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ResNet18 함수 정의\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet18().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0107e+00, -1.2956e-01, -1.5022e-01, -8.6283e-01, -2.1740e-01,\n",
      "          5.9322e-01,  3.0726e-01, -2.8572e-01, -6.3624e-05, -2.7621e-02,\n",
      "          5.3679e-01, -1.2125e-01, -1.1412e+00,  5.8157e-01,  2.9282e-01,\n",
      "         -3.4895e-02,  2.1470e-01,  3.6355e-01,  3.3334e-01, -5.5904e-01,\n",
      "         -3.4468e-01,  2.7047e-01,  3.3255e-02,  4.4940e-02,  1.1195e-01,\n",
      "          2.7890e-01, -7.4188e-02, -5.8290e-01,  1.3141e+00, -2.3595e-01,\n",
      "         -6.6026e-01,  7.8959e-01, -1.0109e+00,  2.8891e-01,  3.0610e-01,\n",
      "         -4.6119e-01, -2.4808e-01,  8.0487e-02, -6.0250e-02,  3.6706e-01,\n",
      "          5.3499e-01, -2.5119e-01,  4.2025e-01, -1.6216e-01,  4.2144e-01,\n",
      "          1.4326e+00, -3.9242e-01, -2.9798e-01, -2.4065e-01,  3.2243e-02,\n",
      "         -3.9907e-01,  2.2065e-01, -4.0792e-01,  3.5146e-01, -1.2525e-01,\n",
      "          2.7907e-01,  9.1486e-02,  2.4694e-01,  1.4650e-01, -4.8784e-01,\n",
      "          1.3902e+00, -2.2075e-01,  9.7394e-01,  4.9485e-01,  5.8165e-01,\n",
      "         -4.2763e-02,  4.0674e-01,  8.4865e-01,  6.2894e-01, -5.0728e-01,\n",
      "         -1.6603e-01, -1.8655e-02,  9.4057e-01, -1.8174e-01, -5.8081e-01,\n",
      "          7.4216e-01, -3.1307e-01, -2.2743e-01,  5.3405e-01, -5.5710e-01,\n",
      "         -8.4686e-01,  1.0305e-01, -3.7300e-01, -1.9717e-02,  6.2118e-01,\n",
      "         -7.1049e-01, -1.3078e+00,  1.6556e-01, -2.6132e-01, -7.4056e-01,\n",
      "         -4.8711e-01,  3.1872e-01, -1.0756e+00, -2.5168e-01,  5.9027e-02,\n",
      "          9.7893e-01, -3.3639e-02, -4.5968e-01,  1.0259e-01,  1.7630e-01,\n",
      "          2.1035e-01,  2.7864e-01, -5.1673e-02,  1.9012e-01, -1.4504e-01,\n",
      "         -3.6478e-01,  4.0150e-01, -4.8583e-01, -4.3405e-01,  1.6448e-01,\n",
      "          3.1715e-01,  6.1137e-01,  7.0314e-02, -3.0064e-01,  7.0858e-01,\n",
      "         -1.6242e-01, -2.3890e-01, -7.8414e-02,  1.1219e-01, -4.2190e-01]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# test 실행 \n",
    "a=torch.Tensor(1,3,224,224).to(device)\n",
    "out = resnet(a)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate를 단계적으로 줄여주는 방법\n",
    "# 150 -> lr/10\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate\n",
    "    if epoch >= 100:\n",
    "        lr /= 10\n",
    "    if epoch >= 150:\n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468c2ffe623b4b6baa03b146617d8608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/200 | step: 100/927 | trn loss: 7.3571 | val loss: 4.8030 | acc: 1.03\n",
      "epoch: 1/200 | step: 200/927 | trn loss: 4.7952 | val loss: 4.7904 | acc: 0.79\n",
      "epoch: 1/200 | step: 300/927 | trn loss: 4.7858 | val loss: 4.7880 | acc: 0.79\n",
      "epoch: 1/200 | step: 400/927 | trn loss: 4.7829 | val loss: 4.7853 | acc: 0.97\n",
      "epoch: 1/200 | step: 500/927 | trn loss: 4.7895 | val loss: 4.7836 | acc: 0.91\n",
      "epoch: 1/200 | step: 600/927 | trn loss: 4.7823 | val loss: 4.7843 | acc: 0.67\n",
      "epoch: 1/200 | step: 700/927 | trn loss: 4.7800 | val loss: 4.7829 | acc: 0.85\n",
      "epoch: 1/200 | step: 800/927 | trn loss: 4.7796 | val loss: 4.7746 | acc: 0.91\n",
      "epoch: 1/200 | step: 900/927 | trn loss: 4.7741 | val loss: 4.7698 | acc: 1.22\n",
      "epoch: 2/200 | step: 100/927 | trn loss: 4.7604 | val loss: 4.7589 | acc: 1.22\n",
      "epoch: 2/200 | step: 200/927 | trn loss: 4.7647 | val loss: 4.7459 | acc: 0.97\n",
      "epoch: 2/200 | step: 300/927 | trn loss: 4.7545 | val loss: 4.7364 | acc: 1.46\n",
      "epoch: 2/200 | step: 400/927 | trn loss: 4.7448 | val loss: 4.7239 | acc: 1.64\n",
      "epoch: 2/200 | step: 500/927 | trn loss: 4.7391 | val loss: 4.7207 | acc: 1.64\n",
      "epoch: 2/200 | step: 600/927 | trn loss: 4.7341 | val loss: 4.7094 | acc: 1.82\n",
      "epoch: 2/200 | step: 700/927 | trn loss: 4.7512 | val loss: 4.7229 | acc: 1.58\n",
      "epoch: 2/200 | step: 800/927 | trn loss: 4.7264 | val loss: 4.7211 | acc: 1.70\n",
      "epoch: 2/200 | step: 900/927 | trn loss: 4.7250 | val loss: 4.6889 | acc: 1.76\n",
      "epoch: 3/200 | step: 100/927 | trn loss: 4.7140 | val loss: 4.6956 | acc: 1.82\n",
      "epoch: 3/200 | step: 200/927 | trn loss: 4.7049 | val loss: 4.6748 | acc: 2.43\n",
      "epoch: 3/200 | step: 300/927 | trn loss: 4.7142 | val loss: 4.6805 | acc: 1.64\n",
      "epoch: 3/200 | step: 400/927 | trn loss: 4.7106 | val loss: 4.6716 | acc: 1.82\n",
      "epoch: 3/200 | step: 500/927 | trn loss: 4.7124 | val loss: 4.6885 | acc: 1.94\n",
      "epoch: 3/200 | step: 600/927 | trn loss: 4.6923 | val loss: 4.6539 | acc: 1.94\n",
      "epoch: 3/200 | step: 700/927 | trn loss: 4.7124 | val loss: 4.6515 | acc: 2.00\n",
      "epoch: 3/200 | step: 800/927 | trn loss: 4.7000 | val loss: 4.6723 | acc: 1.88\n",
      "epoch: 3/200 | step: 900/927 | trn loss: 4.7005 | val loss: 4.6586 | acc: 2.49\n",
      "epoch: 4/200 | step: 100/927 | trn loss: 4.6758 | val loss: 4.6420 | acc: 2.49\n",
      "epoch: 4/200 | step: 200/927 | trn loss: 4.6884 | val loss: 4.6117 | acc: 2.49\n",
      "epoch: 4/200 | step: 300/927 | trn loss: 4.6584 | val loss: 4.6053 | acc: 2.61\n",
      "epoch: 4/200 | step: 400/927 | trn loss: 4.6610 | val loss: 4.5931 | acc: 2.86\n",
      "epoch: 4/200 | step: 500/927 | trn loss: 4.6582 | val loss: 4.5920 | acc: 2.73\n",
      "epoch: 4/200 | step: 600/927 | trn loss: 4.6547 | val loss: 4.5799 | acc: 2.43\n",
      "epoch: 4/200 | step: 700/927 | trn loss: 4.6386 | val loss: 4.5650 | acc: 2.61\n",
      "epoch: 4/200 | step: 800/927 | trn loss: 4.6523 | val loss: 4.5529 | acc: 2.86\n",
      "epoch: 4/200 | step: 900/927 | trn loss: 4.6097 | val loss: 4.5326 | acc: 3.04\n",
      "epoch: 5/200 | step: 100/927 | trn loss: 4.6320 | val loss: 4.5827 | acc: 2.55\n",
      "epoch: 5/200 | step: 200/927 | trn loss: 4.6332 | val loss: 4.5571 | acc: 3.71\n",
      "epoch: 5/200 | step: 300/927 | trn loss: 4.6070 | val loss: 4.5625 | acc: 2.86\n",
      "epoch: 5/200 | step: 400/927 | trn loss: 4.6082 | val loss: 4.5422 | acc: 3.04\n",
      "epoch: 5/200 | step: 500/927 | trn loss: 4.6176 | val loss: 4.5408 | acc: 2.67\n",
      "epoch: 5/200 | step: 600/927 | trn loss: 4.6172 | val loss: 4.5701 | acc: 3.22\n",
      "epoch: 5/200 | step: 700/927 | trn loss: 4.6040 | val loss: 4.5473 | acc: 2.98\n",
      "epoch: 5/200 | step: 800/927 | trn loss: 4.6021 | val loss: 4.5275 | acc: 3.40\n",
      "epoch: 5/200 | step: 900/927 | trn loss: 4.6258 | val loss: 4.5316 | acc: 3.71\n",
      "epoch: 6/200 | step: 100/927 | trn loss: 4.6114 | val loss: 4.5114 | acc: 3.71\n",
      "epoch: 6/200 | step: 200/927 | trn loss: 4.5749 | val loss: 4.4971 | acc: 3.77\n",
      "epoch: 6/200 | step: 300/927 | trn loss: 4.6152 | val loss: 4.5181 | acc: 4.01\n",
      "epoch: 6/200 | step: 400/927 | trn loss: 4.5504 | val loss: 4.5038 | acc: 4.07\n",
      "epoch: 6/200 | step: 500/927 | trn loss: 4.6101 | val loss: 4.5122 | acc: 2.67\n",
      "epoch: 6/200 | step: 600/927 | trn loss: 4.5808 | val loss: 4.5117 | acc: 3.71\n",
      "epoch: 6/200 | step: 700/927 | trn loss: 4.6210 | val loss: 4.4966 | acc: 4.07\n",
      "epoch: 6/200 | step: 800/927 | trn loss: 4.5758 | val loss: 4.5043 | acc: 3.16\n",
      "epoch: 6/200 | step: 900/927 | trn loss: 4.5877 | val loss: 4.4968 | acc: 3.52\n",
      "epoch: 7/200 | step: 100/927 | trn loss: 4.5468 | val loss: 4.4854 | acc: 4.19\n",
      "epoch: 7/200 | step: 200/927 | trn loss: 4.5857 | val loss: 4.4856 | acc: 3.65\n",
      "epoch: 7/200 | step: 300/927 | trn loss: 4.5801 | val loss: 4.4806 | acc: 4.07\n",
      "epoch: 7/200 | step: 400/927 | trn loss: 4.5535 | val loss: 4.4775 | acc: 3.77\n",
      "epoch: 7/200 | step: 500/927 | trn loss: 4.5781 | val loss: 4.5547 | acc: 2.92\n",
      "epoch: 7/200 | step: 600/927 | trn loss: 4.5442 | val loss: 4.4803 | acc: 2.86\n",
      "epoch: 7/200 | step: 700/927 | trn loss: 4.5718 | val loss: 4.4599 | acc: 3.52\n",
      "epoch: 7/200 | step: 800/927 | trn loss: 4.6171 | val loss: 4.4956 | acc: 3.52\n",
      "epoch: 7/200 | step: 900/927 | trn loss: 4.5894 | val loss: 4.4802 | acc: 3.83\n",
      "epoch: 8/200 | step: 100/927 | trn loss: 4.5430 | val loss: 4.4531 | acc: 4.25\n",
      "epoch: 8/200 | step: 200/927 | trn loss: 4.5507 | val loss: 4.4722 | acc: 3.77\n",
      "epoch: 8/200 | step: 300/927 | trn loss: 4.5723 | val loss: 4.4727 | acc: 4.62\n",
      "epoch: 8/200 | step: 400/927 | trn loss: 4.5395 | val loss: 4.4577 | acc: 4.80\n",
      "epoch: 8/200 | step: 500/927 | trn loss: 4.5760 | val loss: 4.4625 | acc: 3.89\n",
      "epoch: 8/200 | step: 600/927 | trn loss: 4.5707 | val loss: 4.4731 | acc: 4.19\n",
      "epoch: 8/200 | step: 700/927 | trn loss: 4.5606 | val loss: 4.4415 | acc: 5.29\n",
      "epoch: 8/200 | step: 800/927 | trn loss: 4.5278 | val loss: 4.4408 | acc: 4.13\n",
      "epoch: 8/200 | step: 900/927 | trn loss: 4.5391 | val loss: 4.4461 | acc: 4.25\n",
      "epoch: 9/200 | step: 100/927 | trn loss: 4.5483 | val loss: 4.4528 | acc: 3.77\n",
      "epoch: 9/200 | step: 200/927 | trn loss: 4.5623 | val loss: 4.4759 | acc: 3.77\n",
      "epoch: 9/200 | step: 300/927 | trn loss: 4.5451 | val loss: 4.4552 | acc: 4.07\n",
      "epoch: 9/200 | step: 400/927 | trn loss: 4.5508 | val loss: 4.4597 | acc: 4.01\n",
      "epoch: 9/200 | step: 500/927 | trn loss: 4.5506 | val loss: 4.4503 | acc: 4.43\n",
      "epoch: 9/200 | step: 600/927 | trn loss: 4.5533 | val loss: 4.4379 | acc: 5.22\n",
      "epoch: 9/200 | step: 700/927 | trn loss: 4.5163 | val loss: 4.4534 | acc: 4.25\n",
      "epoch: 9/200 | step: 800/927 | trn loss: 4.5281 | val loss: 4.4260 | acc: 4.92\n",
      "epoch: 9/200 | step: 900/927 | trn loss: 4.5597 | val loss: 4.4308 | acc: 4.43\n",
      "epoch: 10/200 | step: 100/927 | trn loss: 4.5132 | val loss: 4.4360 | acc: 5.04\n",
      "epoch: 10/200 | step: 200/927 | trn loss: 4.5377 | val loss: 4.4170 | acc: 4.68\n",
      "epoch: 10/200 | step: 300/927 | trn loss: 4.5242 | val loss: 4.4281 | acc: 4.74\n",
      "epoch: 10/200 | step: 400/927 | trn loss: 4.5372 | val loss: 4.4248 | acc: 4.86\n",
      "epoch: 10/200 | step: 500/927 | trn loss: 4.5151 | val loss: 4.4243 | acc: 4.80\n",
      "epoch: 10/200 | step: 600/927 | trn loss: 4.5135 | val loss: 4.4251 | acc: 4.92\n",
      "epoch: 10/200 | step: 700/927 | trn loss: 4.5582 | val loss: 4.4148 | acc: 4.62\n",
      "epoch: 10/200 | step: 800/927 | trn loss: 4.5114 | val loss: 4.3932 | acc: 5.83\n",
      "epoch: 10/200 | step: 900/927 | trn loss: 4.5544 | val loss: 4.4244 | acc: 4.80\n",
      "epoch: 11/200 | step: 100/927 | trn loss: 4.4679 | val loss: 4.4571 | acc: 4.86\n",
      "epoch: 11/200 | step: 200/927 | trn loss: 4.5306 | val loss: 4.4098 | acc: 5.16\n",
      "epoch: 11/200 | step: 300/927 | trn loss: 4.5206 | val loss: 4.4062 | acc: 4.74\n",
      "epoch: 11/200 | step: 400/927 | trn loss: 4.4894 | val loss: 4.4001 | acc: 4.62\n",
      "epoch: 11/200 | step: 500/927 | trn loss: 4.4969 | val loss: 4.4422 | acc: 4.74\n",
      "epoch: 11/200 | step: 600/927 | trn loss: 4.5000 | val loss: 4.4264 | acc: 3.95\n",
      "epoch: 11/200 | step: 700/927 | trn loss: 4.5013 | val loss: 4.3978 | acc: 5.16\n",
      "epoch: 11/200 | step: 800/927 | trn loss: 4.5175 | val loss: 4.3814 | acc: 4.80\n",
      "epoch: 11/200 | step: 900/927 | trn loss: 4.4817 | val loss: 4.3868 | acc: 5.53\n",
      "epoch: 12/200 | step: 100/927 | trn loss: 4.5145 | val loss: 4.4220 | acc: 5.29\n",
      "epoch: 12/200 | step: 200/927 | trn loss: 4.4936 | val loss: 4.3941 | acc: 5.22\n",
      "epoch: 12/200 | step: 300/927 | trn loss: 4.5037 | val loss: 4.4217 | acc: 5.41\n",
      "epoch: 12/200 | step: 400/927 | trn loss: 4.5009 | val loss: 4.3818 | acc: 5.77\n",
      "epoch: 12/200 | step: 500/927 | trn loss: 4.4958 | val loss: 4.3829 | acc: 5.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12/200 | step: 600/927 | trn loss: 4.4873 | val loss: 4.3671 | acc: 5.77\n",
      "epoch: 12/200 | step: 700/927 | trn loss: 4.4995 | val loss: 4.3663 | acc: 5.47\n",
      "epoch: 12/200 | step: 800/927 | trn loss: 4.4904 | val loss: 4.3931 | acc: 4.92\n",
      "epoch: 12/200 | step: 900/927 | trn loss: 4.4931 | val loss: 4.3643 | acc: 5.35\n",
      "epoch: 13/200 | step: 100/927 | trn loss: 4.4885 | val loss: 4.3942 | acc: 4.62\n",
      "epoch: 13/200 | step: 200/927 | trn loss: 4.4647 | val loss: 4.3885 | acc: 5.16\n",
      "epoch: 13/200 | step: 300/927 | trn loss: 4.4711 | val loss: 4.3949 | acc: 5.16\n",
      "epoch: 13/200 | step: 400/927 | trn loss: 4.5213 | val loss: 4.4346 | acc: 5.59\n",
      "epoch: 13/200 | step: 500/927 | trn loss: 4.5068 | val loss: 4.3805 | acc: 5.22\n",
      "epoch: 13/200 | step: 600/927 | trn loss: 4.4416 | val loss: 4.3812 | acc: 4.86\n",
      "epoch: 13/200 | step: 700/927 | trn loss: 4.4809 | val loss: 4.4164 | acc: 5.16\n",
      "epoch: 13/200 | step: 800/927 | trn loss: 4.4871 | val loss: 4.3775 | acc: 5.22\n",
      "epoch: 13/200 | step: 900/927 | trn loss: 4.4684 | val loss: 4.4023 | acc: 4.98\n",
      "epoch: 14/200 | step: 100/927 | trn loss: 4.4842 | val loss: 4.3699 | acc: 5.04\n",
      "epoch: 14/200 | step: 200/927 | trn loss: 4.4592 | val loss: 4.3734 | acc: 5.65\n",
      "epoch: 14/200 | step: 300/927 | trn loss: 4.4339 | val loss: 4.3831 | acc: 5.65\n",
      "epoch: 14/200 | step: 400/927 | trn loss: 4.4695 | val loss: 4.3706 | acc: 5.77\n",
      "epoch: 14/200 | step: 500/927 | trn loss: 4.4143 | val loss: 4.3547 | acc: 5.71\n",
      "epoch: 14/200 | step: 600/927 | trn loss: 4.4410 | val loss: 4.3371 | acc: 6.14\n",
      "epoch: 14/200 | step: 700/927 | trn loss: 4.4324 | val loss: 4.3723 | acc: 5.10\n",
      "epoch: 14/200 | step: 800/927 | trn loss: 4.4727 | val loss: 4.3457 | acc: 6.01\n",
      "epoch: 14/200 | step: 900/927 | trn loss: 4.4270 | val loss: 4.3440 | acc: 5.71\n",
      "epoch: 15/200 | step: 100/927 | trn loss: 4.4710 | val loss: 4.3475 | acc: 6.01\n",
      "epoch: 15/200 | step: 200/927 | trn loss: 4.4186 | val loss: 4.3492 | acc: 5.29\n",
      "epoch: 15/200 | step: 300/927 | trn loss: 4.4683 | val loss: 4.3518 | acc: 5.16\n",
      "epoch: 15/200 | step: 400/927 | trn loss: 4.4282 | val loss: 4.3315 | acc: 6.08\n",
      "epoch: 15/200 | step: 500/927 | trn loss: 4.4674 | val loss: 4.3584 | acc: 5.10\n",
      "epoch: 15/200 | step: 600/927 | trn loss: 4.4266 | val loss: 4.3025 | acc: 6.08\n",
      "epoch: 15/200 | step: 700/927 | trn loss: 4.4016 | val loss: 4.3190 | acc: 6.99\n",
      "epoch: 15/200 | step: 800/927 | trn loss: 4.4359 | val loss: 4.3277 | acc: 5.59\n",
      "epoch: 15/200 | step: 900/927 | trn loss: 4.4466 | val loss: 4.3310 | acc: 5.89\n",
      "epoch: 16/200 | step: 100/927 | trn loss: 4.4344 | val loss: 4.3434 | acc: 5.47\n",
      "epoch: 16/200 | step: 200/927 | trn loss: 4.4294 | val loss: 4.3527 | acc: 5.77\n",
      "epoch: 16/200 | step: 300/927 | trn loss: 4.3808 | val loss: 4.2998 | acc: 6.26\n",
      "epoch: 16/200 | step: 400/927 | trn loss: 4.3800 | val loss: 4.3014 | acc: 5.35\n",
      "epoch: 16/200 | step: 500/927 | trn loss: 4.3744 | val loss: 4.3012 | acc: 5.89\n",
      "epoch: 16/200 | step: 600/927 | trn loss: 4.4057 | val loss: 4.3089 | acc: 6.20\n",
      "epoch: 16/200 | step: 700/927 | trn loss: 4.3919 | val loss: 4.2920 | acc: 5.95\n",
      "epoch: 16/200 | step: 800/927 | trn loss: 4.3552 | val loss: 4.2910 | acc: 6.08\n",
      "epoch: 16/200 | step: 900/927 | trn loss: 4.3846 | val loss: 4.2624 | acc: 5.89\n",
      "epoch: 17/200 | step: 100/927 | trn loss: 4.3588 | val loss: 4.2819 | acc: 6.38\n",
      "epoch: 17/200 | step: 200/927 | trn loss: 4.3529 | val loss: 4.2375 | acc: 6.68\n",
      "epoch: 17/200 | step: 300/927 | trn loss: 4.3542 | val loss: 4.2616 | acc: 6.32\n",
      "epoch: 17/200 | step: 400/927 | trn loss: 4.3637 | val loss: 4.3119 | acc: 6.20\n",
      "epoch: 17/200 | step: 500/927 | trn loss: 4.3519 | val loss: 4.2482 | acc: 5.16\n",
      "epoch: 17/200 | step: 600/927 | trn loss: 4.3795 | val loss: 4.2723 | acc: 5.77\n",
      "epoch: 17/200 | step: 700/927 | trn loss: 4.3352 | val loss: 4.2361 | acc: 6.68\n",
      "epoch: 17/200 | step: 800/927 | trn loss: 4.3472 | val loss: 4.2420 | acc: 6.74\n",
      "epoch: 17/200 | step: 900/927 | trn loss: 4.3500 | val loss: 4.2391 | acc: 6.80\n",
      "epoch: 18/200 | step: 100/927 | trn loss: 4.2958 | val loss: 4.2234 | acc: 6.93\n",
      "epoch: 18/200 | step: 200/927 | trn loss: 4.3240 | val loss: 4.2745 | acc: 5.65\n",
      "epoch: 18/200 | step: 300/927 | trn loss: 4.3356 | val loss: 4.2311 | acc: 5.35\n",
      "epoch: 18/200 | step: 400/927 | trn loss: 4.3644 | val loss: 4.2436 | acc: 7.47\n",
      "epoch: 18/200 | step: 500/927 | trn loss: 4.3347 | val loss: 4.2222 | acc: 6.62\n",
      "epoch: 18/200 | step: 600/927 | trn loss: 4.2967 | val loss: 4.2282 | acc: 7.29\n",
      "epoch: 18/200 | step: 700/927 | trn loss: 4.3477 | val loss: 4.2069 | acc: 6.08\n",
      "epoch: 18/200 | step: 800/927 | trn loss: 4.2806 | val loss: 4.2258 | acc: 7.11\n",
      "epoch: 18/200 | step: 900/927 | trn loss: 4.2853 | val loss: 4.1882 | acc: 7.29\n",
      "epoch: 19/200 | step: 100/927 | trn loss: 4.2948 | val loss: 4.1791 | acc: 7.05\n",
      "epoch: 19/200 | step: 200/927 | trn loss: 4.2417 | val loss: 4.2690 | acc: 6.14\n",
      "epoch: 19/200 | step: 300/927 | trn loss: 4.3152 | val loss: 4.1915 | acc: 7.29\n",
      "epoch: 19/200 | step: 400/927 | trn loss: 4.2962 | val loss: 4.1635 | acc: 7.72\n",
      "epoch: 19/200 | step: 500/927 | trn loss: 4.2583 | val loss: 4.1841 | acc: 6.26\n",
      "epoch: 19/200 | step: 600/927 | trn loss: 4.3099 | val loss: 4.1879 | acc: 6.62\n",
      "epoch: 19/200 | step: 700/927 | trn loss: 4.3109 | val loss: 4.1784 | acc: 7.35\n",
      "epoch: 19/200 | step: 800/927 | trn loss: 4.3106 | val loss: 4.1763 | acc: 6.62\n",
      "epoch: 19/200 | step: 900/927 | trn loss: 4.3244 | val loss: 4.1588 | acc: 7.96\n",
      "epoch: 20/200 | step: 100/927 | trn loss: 4.3220 | val loss: 4.1515 | acc: 7.53\n",
      "epoch: 20/200 | step: 200/927 | trn loss: 4.2725 | val loss: 4.1545 | acc: 7.59\n",
      "epoch: 20/200 | step: 300/927 | trn loss: 4.2593 | val loss: 4.1775 | acc: 6.93\n",
      "epoch: 20/200 | step: 400/927 | trn loss: 4.2310 | val loss: 4.1768 | acc: 7.84\n",
      "epoch: 20/200 | step: 500/927 | trn loss: 4.2449 | val loss: 4.1411 | acc: 7.23\n",
      "epoch: 20/200 | step: 600/927 | trn loss: 4.2974 | val loss: 4.1442 | acc: 7.17\n",
      "epoch: 20/200 | step: 700/927 | trn loss: 4.2609 | val loss: 4.1190 | acc: 7.11\n",
      "epoch: 20/200 | step: 800/927 | trn loss: 4.2767 | val loss: 4.1351 | acc: 7.53\n",
      "epoch: 20/200 | step: 900/927 | trn loss: 4.2672 | val loss: 4.1308 | acc: 8.08\n",
      "epoch: 21/200 | step: 100/927 | trn loss: 4.2350 | val loss: 4.1442 | acc: 7.41\n",
      "epoch: 21/200 | step: 200/927 | trn loss: 4.2327 | val loss: 4.1312 | acc: 8.20\n",
      "epoch: 21/200 | step: 300/927 | trn loss: 4.1978 | val loss: 4.1200 | acc: 8.32\n",
      "epoch: 21/200 | step: 400/927 | trn loss: 4.2261 | val loss: 4.0889 | acc: 9.11\n",
      "epoch: 21/200 | step: 500/927 | trn loss: 4.2426 | val loss: 4.1036 | acc: 9.17\n",
      "epoch: 21/200 | step: 600/927 | trn loss: 4.2002 | val loss: 4.1154 | acc: 8.63\n",
      "epoch: 21/200 | step: 700/927 | trn loss: 4.2232 | val loss: 4.0771 | acc: 8.63\n",
      "epoch: 21/200 | step: 800/927 | trn loss: 4.2013 | val loss: 4.1228 | acc: 8.38\n",
      "epoch: 21/200 | step: 900/927 | trn loss: 4.2470 | val loss: 4.1330 | acc: 7.11\n",
      "epoch: 22/200 | step: 100/927 | trn loss: 4.2046 | val loss: 4.0707 | acc: 9.11\n",
      "epoch: 22/200 | step: 200/927 | trn loss: 4.2254 | val loss: 4.1088 | acc: 7.96\n",
      "epoch: 22/200 | step: 300/927 | trn loss: 4.1271 | val loss: 4.0295 | acc: 9.05\n",
      "epoch: 22/200 | step: 400/927 | trn loss: 4.1924 | val loss: 4.1021 | acc: 8.69\n",
      "epoch: 22/200 | step: 500/927 | trn loss: 4.2371 | val loss: 4.0685 | acc: 8.75\n",
      "epoch: 22/200 | step: 600/927 | trn loss: 4.1567 | val loss: 4.0770 | acc: 8.51\n",
      "epoch: 22/200 | step: 700/927 | trn loss: 4.2705 | val loss: 4.0872 | acc: 8.63\n",
      "epoch: 22/200 | step: 800/927 | trn loss: 4.1804 | val loss: 4.0762 | acc: 8.63\n",
      "epoch: 22/200 | step: 900/927 | trn loss: 4.1574 | val loss: 4.0720 | acc: 8.63\n",
      "epoch: 23/200 | step: 100/927 | trn loss: 4.1380 | val loss: 4.0383 | acc: 8.81\n",
      "epoch: 23/200 | step: 200/927 | trn loss: 4.1751 | val loss: 4.0230 | acc: 8.99\n",
      "epoch: 23/200 | step: 300/927 | trn loss: 4.1761 | val loss: 4.0590 | acc: 8.02\n",
      "epoch: 23/200 | step: 400/927 | trn loss: 4.1669 | val loss: 4.0890 | acc: 8.44\n",
      "epoch: 23/200 | step: 500/927 | trn loss: 4.2015 | val loss: 4.1088 | acc: 8.81\n",
      "epoch: 23/200 | step: 600/927 | trn loss: 4.1499 | val loss: 4.0738 | acc: 7.90\n",
      "epoch: 23/200 | step: 700/927 | trn loss: 4.1079 | val loss: 4.0497 | acc: 8.81\n",
      "epoch: 23/200 | step: 800/927 | trn loss: 4.1311 | val loss: 3.9693 | acc: 9.96\n",
      "epoch: 23/200 | step: 900/927 | trn loss: 4.1510 | val loss: 4.0598 | acc: 9.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24/200 | step: 100/927 | trn loss: 4.1101 | val loss: 3.9819 | acc: 9.90\n",
      "epoch: 24/200 | step: 200/927 | trn loss: 4.1110 | val loss: 4.0284 | acc: 9.42\n",
      "epoch: 24/200 | step: 300/927 | trn loss: 4.1406 | val loss: 3.9512 | acc: 9.48\n",
      "epoch: 24/200 | step: 400/927 | trn loss: 4.0838 | val loss: 4.0215 | acc: 9.96\n",
      "epoch: 24/200 | step: 500/927 | trn loss: 4.1650 | val loss: 4.0220 | acc: 9.90\n",
      "epoch: 24/200 | step: 600/927 | trn loss: 4.1613 | val loss: 3.9561 | acc: 10.87\n",
      "epoch: 24/200 | step: 700/927 | trn loss: 4.1035 | val loss: 3.9726 | acc: 10.63\n",
      "epoch: 24/200 | step: 800/927 | trn loss: 4.1204 | val loss: 3.9774 | acc: 10.15\n",
      "epoch: 24/200 | step: 900/927 | trn loss: 4.0887 | val loss: 4.0006 | acc: 9.60\n",
      "epoch: 25/200 | step: 100/927 | trn loss: 4.0226 | val loss: 3.9723 | acc: 10.45\n",
      "epoch: 25/200 | step: 200/927 | trn loss: 4.0691 | val loss: 3.9714 | acc: 10.45\n",
      "epoch: 25/200 | step: 300/927 | trn loss: 4.0839 | val loss: 3.9321 | acc: 10.27\n",
      "epoch: 25/200 | step: 400/927 | trn loss: 4.0994 | val loss: 3.9462 | acc: 10.51\n",
      "epoch: 25/200 | step: 500/927 | trn loss: 4.0753 | val loss: 3.9836 | acc: 9.42\n",
      "epoch: 25/200 | step: 600/927 | trn loss: 4.0841 | val loss: 3.9454 | acc: 10.15\n",
      "epoch: 25/200 | step: 700/927 | trn loss: 4.0753 | val loss: 3.9287 | acc: 11.18\n",
      "epoch: 25/200 | step: 800/927 | trn loss: 4.1173 | val loss: 3.9419 | acc: 11.66\n",
      "epoch: 25/200 | step: 900/927 | trn loss: 4.0838 | val loss: 3.9550 | acc: 10.15\n",
      "epoch: 26/200 | step: 100/927 | trn loss: 4.0586 | val loss: 3.9414 | acc: 10.39\n",
      "epoch: 26/200 | step: 200/927 | trn loss: 4.0333 | val loss: 3.8745 | acc: 11.97\n",
      "epoch: 26/200 | step: 300/927 | trn loss: 4.0185 | val loss: 3.9589 | acc: 10.33\n",
      "epoch: 26/200 | step: 400/927 | trn loss: 4.0707 | val loss: 3.9422 | acc: 10.15\n",
      "epoch: 26/200 | step: 500/927 | trn loss: 4.0097 | val loss: 3.8828 | acc: 11.30\n",
      "epoch: 26/200 | step: 600/927 | trn loss: 4.0265 | val loss: 3.9169 | acc: 10.94\n",
      "epoch: 26/200 | step: 700/927 | trn loss: 4.0005 | val loss: 3.8587 | acc: 11.12\n",
      "epoch: 26/200 | step: 800/927 | trn loss: 4.0531 | val loss: 3.8821 | acc: 11.24\n",
      "epoch: 26/200 | step: 900/927 | trn loss: 4.0504 | val loss: 3.9258 | acc: 11.12\n",
      "epoch: 27/200 | step: 100/927 | trn loss: 3.9854 | val loss: 3.9172 | acc: 11.30\n",
      "epoch: 27/200 | step: 200/927 | trn loss: 3.9868 | val loss: 3.9056 | acc: 10.87\n",
      "epoch: 27/200 | step: 300/927 | trn loss: 3.9325 | val loss: 3.8355 | acc: 12.52\n",
      "epoch: 27/200 | step: 400/927 | trn loss: 4.0066 | val loss: 3.8564 | acc: 11.73\n",
      "epoch: 27/200 | step: 500/927 | trn loss: 4.0179 | val loss: 3.8585 | acc: 12.03\n",
      "epoch: 27/200 | step: 600/927 | trn loss: 3.9416 | val loss: 3.8975 | acc: 11.42\n",
      "epoch: 27/200 | step: 700/927 | trn loss: 4.0324 | val loss: 3.8616 | acc: 11.85\n",
      "epoch: 27/200 | step: 800/927 | trn loss: 4.0113 | val loss: 3.8810 | acc: 11.36\n",
      "epoch: 27/200 | step: 900/927 | trn loss: 4.0360 | val loss: 3.8539 | acc: 11.97\n",
      "epoch: 28/200 | step: 100/927 | trn loss: 3.9415 | val loss: 3.8594 | acc: 11.12\n",
      "epoch: 28/200 | step: 200/927 | trn loss: 3.9744 | val loss: 3.9120 | acc: 11.54\n",
      "epoch: 28/200 | step: 300/927 | trn loss: 3.9573 | val loss: 3.8287 | acc: 12.27\n",
      "epoch: 28/200 | step: 400/927 | trn loss: 3.9503 | val loss: 3.8250 | acc: 12.03\n",
      "epoch: 28/200 | step: 500/927 | trn loss: 3.9939 | val loss: 3.7797 | acc: 12.76\n",
      "epoch: 28/200 | step: 600/927 | trn loss: 3.9793 | val loss: 3.8075 | acc: 12.21\n",
      "epoch: 28/200 | step: 700/927 | trn loss: 3.9199 | val loss: 3.8116 | acc: 12.64\n",
      "epoch: 28/200 | step: 800/927 | trn loss: 3.9764 | val loss: 3.8195 | acc: 11.79\n",
      "epoch: 28/200 | step: 900/927 | trn loss: 3.9457 | val loss: 3.7810 | acc: 13.24\n",
      "epoch: 29/200 | step: 200/927 | trn loss: 3.9030 | val loss: 3.7855 | acc: 11.60\n",
      "epoch: 29/200 | step: 300/927 | trn loss: 3.9085 | val loss: 3.7514 | acc: 13.12\n",
      "epoch: 29/200 | step: 400/927 | trn loss: 3.9175 | val loss: 3.7447 | acc: 13.55\n",
      "epoch: 29/200 | step: 500/927 | trn loss: 3.8524 | val loss: 3.7826 | acc: 12.58\n",
      "epoch: 29/200 | step: 600/927 | trn loss: 3.9194 | val loss: 3.7553 | acc: 13.12\n",
      "epoch: 29/200 | step: 700/927 | trn loss: 3.9374 | val loss: 3.7610 | acc: 11.97\n",
      "epoch: 29/200 | step: 800/927 | trn loss: 3.8913 | val loss: 3.7486 | acc: 13.43\n",
      "epoch: 29/200 | step: 900/927 | trn loss: 3.9274 | val loss: 3.7694 | acc: 13.30\n",
      "epoch: 30/200 | step: 100/927 | trn loss: 3.8922 | val loss: 3.7378 | acc: 14.16\n",
      "epoch: 30/200 | step: 200/927 | trn loss: 3.8222 | val loss: 3.7492 | acc: 13.49\n",
      "epoch: 30/200 | step: 300/927 | trn loss: 3.8437 | val loss: 3.7778 | acc: 12.58\n",
      "epoch: 30/200 | step: 400/927 | trn loss: 3.8598 | val loss: 3.7366 | acc: 13.79\n",
      "epoch: 30/200 | step: 500/927 | trn loss: 3.8732 | val loss: 3.7952 | acc: 12.82\n",
      "epoch: 30/200 | step: 600/927 | trn loss: 3.8606 | val loss: 3.7574 | acc: 13.18\n",
      "epoch: 30/200 | step: 700/927 | trn loss: 3.8838 | val loss: 3.7413 | acc: 13.85\n",
      "epoch: 30/200 | step: 800/927 | trn loss: 3.8680 | val loss: 3.7456 | acc: 13.00\n",
      "epoch: 30/200 | step: 900/927 | trn loss: 3.8612 | val loss: 3.7288 | acc: 13.91\n",
      "epoch: 31/200 | step: 100/927 | trn loss: 3.8320 | val loss: 3.7181 | acc: 14.76\n",
      "epoch: 31/200 | step: 200/927 | trn loss: 3.8349 | val loss: 3.7501 | acc: 12.82\n",
      "epoch: 31/200 | step: 300/927 | trn loss: 3.7416 | val loss: 3.6979 | acc: 14.34\n",
      "epoch: 31/200 | step: 400/927 | trn loss: 3.8435 | val loss: 3.6669 | acc: 14.64\n",
      "epoch: 31/200 | step: 500/927 | trn loss: 3.8376 | val loss: 3.6744 | acc: 14.40\n",
      "epoch: 31/200 | step: 600/927 | trn loss: 3.8214 | val loss: 3.6868 | acc: 13.97\n",
      "epoch: 31/200 | step: 700/927 | trn loss: 3.8159 | val loss: 3.6887 | acc: 14.88\n",
      "epoch: 31/200 | step: 800/927 | trn loss: 3.8219 | val loss: 3.6749 | acc: 14.70\n",
      "epoch: 31/200 | step: 900/927 | trn loss: 3.8119 | val loss: 3.6477 | acc: 15.19\n",
      "epoch: 32/200 | step: 100/927 | trn loss: 3.7818 | val loss: 3.6634 | acc: 14.82\n",
      "epoch: 32/200 | step: 200/927 | trn loss: 3.7598 | val loss: 3.6802 | acc: 14.82\n",
      "epoch: 32/200 | step: 300/927 | trn loss: 3.8478 | val loss: 3.6774 | acc: 15.13\n",
      "epoch: 32/200 | step: 400/927 | trn loss: 3.7779 | val loss: 3.6824 | acc: 14.88\n",
      "epoch: 32/200 | step: 500/927 | trn loss: 3.8421 | val loss: 3.6281 | acc: 16.16\n",
      "epoch: 32/200 | step: 600/927 | trn loss: 3.7539 | val loss: 3.6361 | acc: 15.37\n",
      "epoch: 32/200 | step: 700/927 | trn loss: 3.7730 | val loss: 3.6498 | acc: 14.22\n",
      "epoch: 32/200 | step: 800/927 | trn loss: 3.7999 | val loss: 3.6426 | acc: 13.97\n",
      "epoch: 32/200 | step: 900/927 | trn loss: 3.7705 | val loss: 3.5940 | acc: 15.01\n",
      "epoch: 33/200 | step: 100/927 | trn loss: 4.1878 | val loss: 3.9989 | acc: 11.48\n",
      "epoch: 33/200 | step: 200/927 | trn loss: 3.9572 | val loss: 3.8817 | acc: 11.79\n",
      "epoch: 33/200 | step: 300/927 | trn loss: 3.9216 | val loss: 3.7898 | acc: 12.82\n",
      "epoch: 33/200 | step: 400/927 | trn loss: 3.9106 | val loss: 3.7448 | acc: 13.55\n",
      "epoch: 33/200 | step: 500/927 | trn loss: 3.8695 | val loss: 3.7758 | acc: 13.00\n",
      "epoch: 33/200 | step: 600/927 | trn loss: 3.8107 | val loss: 3.6685 | acc: 15.49\n",
      "epoch: 33/200 | step: 700/927 | trn loss: 3.8833 | val loss: 3.6548 | acc: 14.64\n",
      "epoch: 33/200 | step: 800/927 | trn loss: 3.8750 | val loss: 3.7083 | acc: 13.67\n",
      "epoch: 33/200 | step: 900/927 | trn loss: 3.7864 | val loss: 3.6988 | acc: 14.82\n",
      "epoch: 34/200 | step: 100/927 | trn loss: 3.7900 | val loss: 3.6482 | acc: 15.25\n",
      "epoch: 34/200 | step: 200/927 | trn loss: 3.7224 | val loss: 3.6836 | acc: 14.28\n",
      "epoch: 34/200 | step: 300/927 | trn loss: 3.7936 | val loss: 3.6625 | acc: 15.80\n",
      "epoch: 34/200 | step: 400/927 | trn loss: 3.7674 | val loss: 3.6292 | acc: 14.88\n",
      "epoch: 34/200 | step: 500/927 | trn loss: 3.7235 | val loss: 3.5766 | acc: 16.10\n",
      "epoch: 34/200 | step: 600/927 | trn loss: 3.7175 | val loss: 3.6708 | acc: 14.28\n",
      "epoch: 34/200 | step: 700/927 | trn loss: 3.8367 | val loss: 3.6089 | acc: 15.25\n",
      "epoch: 34/200 | step: 800/927 | trn loss: 3.7903 | val loss: 3.5586 | acc: 15.98\n",
      "epoch: 34/200 | step: 900/927 | trn loss: 3.7913 | val loss: 3.5889 | acc: 16.22\n",
      "epoch: 35/200 | step: 100/927 | trn loss: 3.7484 | val loss: 3.5988 | acc: 15.19\n",
      "epoch: 35/200 | step: 200/927 | trn loss: 3.6511 | val loss: 3.5988 | acc: 16.04\n",
      "epoch: 35/200 | step: 300/927 | trn loss: 3.6700 | val loss: 3.5736 | acc: 16.77\n",
      "epoch: 35/200 | step: 400/927 | trn loss: 3.6997 | val loss: 3.5712 | acc: 16.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35/200 | step: 500/927 | trn loss: 3.7363 | val loss: 3.5330 | acc: 16.28\n",
      "epoch: 35/200 | step: 600/927 | trn loss: 3.7045 | val loss: 3.5750 | acc: 17.38\n",
      "epoch: 35/200 | step: 700/927 | trn loss: 3.7394 | val loss: 3.5034 | acc: 17.86\n",
      "epoch: 35/200 | step: 800/927 | trn loss: 3.7360 | val loss: 3.5597 | acc: 16.40\n",
      "epoch: 35/200 | step: 900/927 | trn loss: 3.7438 | val loss: 3.5033 | acc: 17.74\n",
      "epoch: 36/200 | step: 100/927 | trn loss: 3.5988 | val loss: 3.5412 | acc: 17.50\n",
      "epoch: 36/200 | step: 200/927 | trn loss: 3.6694 | val loss: 3.5674 | acc: 17.13\n",
      "epoch: 36/200 | step: 300/927 | trn loss: 3.6515 | val loss: 3.5286 | acc: 16.28\n",
      "epoch: 36/200 | step: 400/927 | trn loss: 3.6297 | val loss: 3.5064 | acc: 18.17\n",
      "epoch: 36/200 | step: 500/927 | trn loss: 3.6392 | val loss: 3.5697 | acc: 16.10\n",
      "epoch: 36/200 | step: 600/927 | trn loss: 3.6842 | val loss: 3.4921 | acc: 17.92\n",
      "epoch: 36/200 | step: 700/927 | trn loss: 3.6885 | val loss: 3.5445 | acc: 16.59\n",
      "epoch: 36/200 | step: 800/927 | trn loss: 3.5986 | val loss: 3.5341 | acc: 17.01\n",
      "epoch: 36/200 | step: 900/927 | trn loss: 3.6486 | val loss: 3.4637 | acc: 17.98\n",
      "epoch: 37/200 | step: 100/927 | trn loss: 3.5605 | val loss: 3.4661 | acc: 17.86\n",
      "epoch: 37/200 | step: 200/927 | trn loss: 3.6000 | val loss: 3.5633 | acc: 16.46\n",
      "epoch: 37/200 | step: 300/927 | trn loss: 3.5565 | val loss: 3.4580 | acc: 17.80\n",
      "epoch: 37/200 | step: 400/927 | trn loss: 3.5886 | val loss: 3.4487 | acc: 17.62\n",
      "epoch: 37/200 | step: 500/927 | trn loss: 3.5666 | val loss: 3.4669 | acc: 17.56\n",
      "epoch: 37/200 | step: 600/927 | trn loss: 3.6579 | val loss: 3.4599 | acc: 17.56\n",
      "epoch: 37/200 | step: 700/927 | trn loss: 3.6138 | val loss: 3.3934 | acc: 21.26\n",
      "epoch: 37/200 | step: 800/927 | trn loss: 3.5468 | val loss: 3.4545 | acc: 19.20\n",
      "epoch: 37/200 | step: 900/927 | trn loss: 3.7084 | val loss: 3.4329 | acc: 19.32\n",
      "epoch: 38/200 | step: 100/927 | trn loss: 3.5148 | val loss: 3.4567 | acc: 19.32\n",
      "epoch: 38/200 | step: 200/927 | trn loss: 3.5122 | val loss: 3.4552 | acc: 18.23\n",
      "epoch: 38/200 | step: 300/927 | trn loss: 3.4522 | val loss: 3.4950 | acc: 17.92\n",
      "epoch: 38/200 | step: 400/927 | trn loss: 3.5739 | val loss: 3.4426 | acc: 19.44\n",
      "epoch: 38/200 | step: 500/927 | trn loss: 3.5389 | val loss: 3.3980 | acc: 19.68\n",
      "epoch: 38/200 | step: 600/927 | trn loss: 3.5857 | val loss: 3.4073 | acc: 19.14\n",
      "epoch: 38/200 | step: 700/927 | trn loss: 3.5462 | val loss: 3.3993 | acc: 19.81\n",
      "epoch: 38/200 | step: 800/927 | trn loss: 3.5514 | val loss: 3.3497 | acc: 19.38\n",
      "epoch: 38/200 | step: 900/927 | trn loss: 3.5925 | val loss: 3.3847 | acc: 19.26\n",
      "epoch: 39/200 | step: 100/927 | trn loss: 3.4911 | val loss: 3.4129 | acc: 20.23\n",
      "epoch: 39/200 | step: 200/927 | trn loss: 3.4852 | val loss: 3.3686 | acc: 21.32\n",
      "epoch: 39/200 | step: 300/927 | trn loss: 3.5187 | val loss: 3.3859 | acc: 19.93\n",
      "epoch: 39/200 | step: 400/927 | trn loss: 3.5673 | val loss: 3.3656 | acc: 21.39\n",
      "epoch: 39/200 | step: 500/927 | trn loss: 3.5288 | val loss: 3.3869 | acc: 20.11\n",
      "epoch: 39/200 | step: 600/927 | trn loss: 3.4767 | val loss: 3.2942 | acc: 21.02\n",
      "epoch: 39/200 | step: 700/927 | trn loss: 3.4097 | val loss: 3.3478 | acc: 20.05\n",
      "epoch: 39/200 | step: 800/927 | trn loss: 3.5316 | val loss: 3.3274 | acc: 20.90\n",
      "epoch: 39/200 | step: 900/927 | trn loss: 3.5067 | val loss: 3.2935 | acc: 20.78\n",
      "epoch: 40/200 | step: 100/927 | trn loss: 3.4581 | val loss: 3.3621 | acc: 19.62\n",
      "epoch: 40/200 | step: 200/927 | trn loss: 3.4407 | val loss: 3.3071 | acc: 19.93\n",
      "epoch: 40/200 | step: 300/927 | trn loss: 3.4484 | val loss: 3.3449 | acc: 20.84\n",
      "epoch: 40/200 | step: 400/927 | trn loss: 3.4644 | val loss: 3.2928 | acc: 21.02\n",
      "epoch: 40/200 | step: 500/927 | trn loss: 3.4258 | val loss: 3.3249 | acc: 20.84\n",
      "epoch: 40/200 | step: 600/927 | trn loss: 3.4405 | val loss: 3.3220 | acc: 19.93\n",
      "epoch: 40/200 | step: 700/927 | trn loss: 3.4309 | val loss: 3.2602 | acc: 22.17\n",
      "epoch: 40/200 | step: 800/927 | trn loss: 3.4161 | val loss: 3.2953 | acc: 20.84\n",
      "epoch: 40/200 | step: 900/927 | trn loss: 3.4840 | val loss: 3.2637 | acc: 21.69\n",
      "epoch: 41/200 | step: 100/927 | trn loss: 3.4021 | val loss: 3.2757 | acc: 20.47\n",
      "epoch: 41/200 | step: 200/927 | trn loss: 3.3572 | val loss: 3.2908 | acc: 21.57\n",
      "epoch: 41/200 | step: 300/927 | trn loss: 3.4859 | val loss: 3.2629 | acc: 21.69\n",
      "epoch: 41/200 | step: 400/927 | trn loss: 3.4303 | val loss: 3.3031 | acc: 20.78\n",
      "epoch: 41/200 | step: 500/927 | trn loss: 3.4175 | val loss: 3.2836 | acc: 20.60\n",
      "epoch: 41/200 | step: 600/927 | trn loss: 3.3241 | val loss: 3.2270 | acc: 22.78\n",
      "epoch: 41/200 | step: 700/927 | trn loss: 3.3786 | val loss: 3.2636 | acc: 22.42\n",
      "epoch: 41/200 | step: 800/927 | trn loss: 3.3840 | val loss: 3.2807 | acc: 22.05\n",
      "epoch: 41/200 | step: 900/927 | trn loss: 3.4163 | val loss: 3.2804 | acc: 21.02\n",
      "epoch: 42/200 | step: 100/927 | trn loss: 3.3027 | val loss: 3.2162 | acc: 23.09\n",
      "epoch: 42/200 | step: 200/927 | trn loss: 3.2941 | val loss: 3.2606 | acc: 23.45\n",
      "epoch: 42/200 | step: 300/927 | trn loss: 3.2992 | val loss: 3.2785 | acc: 22.24\n",
      "epoch: 42/200 | step: 400/927 | trn loss: 3.3473 | val loss: 3.2182 | acc: 22.54\n",
      "epoch: 42/200 | step: 500/927 | trn loss: 3.2911 | val loss: 3.1968 | acc: 22.42\n",
      "epoch: 42/200 | step: 600/927 | trn loss: 3.2971 | val loss: 3.2082 | acc: 22.17\n",
      "epoch: 42/200 | step: 700/927 | trn loss: 3.4058 | val loss: 3.2406 | acc: 20.84\n",
      "epoch: 42/200 | step: 800/927 | trn loss: 3.3812 | val loss: 3.2507 | acc: 21.14\n",
      "epoch: 42/200 | step: 900/927 | trn loss: 3.3399 | val loss: 3.2262 | acc: 22.24\n",
      "epoch: 43/200 | step: 100/927 | trn loss: 3.2719 | val loss: 3.2298 | acc: 21.57\n",
      "epoch: 43/200 | step: 200/927 | trn loss: 3.2931 | val loss: 3.2063 | acc: 21.20\n",
      "epoch: 43/200 | step: 300/927 | trn loss: 3.2995 | val loss: 3.2423 | acc: 22.54\n",
      "epoch: 43/200 | step: 400/927 | trn loss: 3.3060 | val loss: 3.1497 | acc: 24.30\n",
      "epoch: 43/200 | step: 500/927 | trn loss: 3.2912 | val loss: 3.2678 | acc: 21.81\n",
      "epoch: 43/200 | step: 600/927 | trn loss: 3.3507 | val loss: 3.1471 | acc: 23.88\n",
      "epoch: 43/200 | step: 700/927 | trn loss: 3.2761 | val loss: 3.1274 | acc: 24.06\n",
      "epoch: 43/200 | step: 800/927 | trn loss: 3.3133 | val loss: 3.1475 | acc: 23.57\n",
      "epoch: 43/200 | step: 900/927 | trn loss: 3.2608 | val loss: 3.1447 | acc: 23.94\n",
      "epoch: 44/200 | step: 100/927 | trn loss: 3.2294 | val loss: 3.1099 | acc: 23.94\n",
      "epoch: 44/200 | step: 200/927 | trn loss: 3.1948 | val loss: 3.1769 | acc: 22.66\n",
      "epoch: 44/200 | step: 300/927 | trn loss: 3.2188 | val loss: 3.1710 | acc: 23.51\n",
      "epoch: 44/200 | step: 400/927 | trn loss: 3.2241 | val loss: 3.1647 | acc: 22.36\n",
      "epoch: 44/200 | step: 500/927 | trn loss: 3.2817 | val loss: 3.1857 | acc: 22.84\n",
      "epoch: 44/200 | step: 600/927 | trn loss: 3.1950 | val loss: 3.1356 | acc: 24.24\n",
      "epoch: 44/200 | step: 700/927 | trn loss: 3.2597 | val loss: 3.1247 | acc: 24.06\n",
      "epoch: 44/200 | step: 800/927 | trn loss: 3.2653 | val loss: 3.1015 | acc: 24.79\n",
      "epoch: 44/200 | step: 900/927 | trn loss: 3.2841 | val loss: 3.0655 | acc: 25.52\n",
      "epoch: 45/200 | step: 100/927 | trn loss: 3.1272 | val loss: 3.1615 | acc: 23.57\n",
      "epoch: 45/200 | step: 200/927 | trn loss: 3.1991 | val loss: 3.1071 | acc: 24.67\n",
      "epoch: 45/200 | step: 300/927 | trn loss: 3.1528 | val loss: 3.1836 | acc: 22.05\n",
      "epoch: 45/200 | step: 400/927 | trn loss: 3.1422 | val loss: 3.0935 | acc: 24.79\n",
      "epoch: 45/200 | step: 500/927 | trn loss: 3.2290 | val loss: 3.0972 | acc: 25.39\n",
      "epoch: 45/200 | step: 600/927 | trn loss: 3.2477 | val loss: 3.0731 | acc: 25.03\n",
      "epoch: 45/200 | step: 700/927 | trn loss: 3.1499 | val loss: 3.0142 | acc: 26.31\n",
      "epoch: 45/200 | step: 800/927 | trn loss: 3.2668 | val loss: 3.0642 | acc: 25.52\n",
      "epoch: 45/200 | step: 900/927 | trn loss: 3.1825 | val loss: 2.9816 | acc: 27.16\n",
      "epoch: 46/200 | step: 100/927 | trn loss: 3.0527 | val loss: 3.0843 | acc: 26.55\n",
      "epoch: 46/200 | step: 200/927 | trn loss: 3.0395 | val loss: 3.1170 | acc: 24.12\n",
      "epoch: 46/200 | step: 300/927 | trn loss: 3.1778 | val loss: 3.1175 | acc: 24.54\n",
      "epoch: 46/200 | step: 400/927 | trn loss: 3.1111 | val loss: 3.0247 | acc: 25.21\n",
      "epoch: 46/200 | step: 500/927 | trn loss: 3.1106 | val loss: 3.0414 | acc: 24.54\n",
      "epoch: 46/200 | step: 600/927 | trn loss: 3.1910 | val loss: 3.0401 | acc: 25.52\n",
      "epoch: 46/200 | step: 700/927 | trn loss: 3.1293 | val loss: 2.9842 | acc: 26.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46/200 | step: 800/927 | trn loss: 3.1273 | val loss: 3.0654 | acc: 24.48\n",
      "epoch: 46/200 | step: 900/927 | trn loss: 3.2811 | val loss: 3.0852 | acc: 25.94\n",
      "epoch: 47/200 | step: 100/927 | trn loss: 3.1486 | val loss: 3.0221 | acc: 27.04\n",
      "epoch: 47/200 | step: 200/927 | trn loss: 3.1063 | val loss: 2.9962 | acc: 27.10\n",
      "epoch: 47/200 | step: 300/927 | trn loss: 3.0648 | val loss: 3.0634 | acc: 26.37\n",
      "epoch: 47/200 | step: 400/927 | trn loss: 3.0925 | val loss: 3.0329 | acc: 25.82\n",
      "epoch: 47/200 | step: 500/927 | trn loss: 3.0094 | val loss: 2.9163 | acc: 28.49\n",
      "epoch: 47/200 | step: 600/927 | trn loss: 3.0965 | val loss: 3.0014 | acc: 27.22\n",
      "epoch: 47/200 | step: 700/927 | trn loss: 3.0524 | val loss: 2.9651 | acc: 27.46\n",
      "epoch: 47/200 | step: 800/927 | trn loss: 3.1373 | val loss: 2.9669 | acc: 27.70\n",
      "epoch: 47/200 | step: 900/927 | trn loss: 3.1203 | val loss: 3.0027 | acc: 26.85\n",
      "epoch: 48/200 | step: 100/927 | trn loss: 2.9596 | val loss: 2.9450 | acc: 26.00\n",
      "epoch: 48/200 | step: 200/927 | trn loss: 3.0631 | val loss: 2.9668 | acc: 26.67\n",
      "epoch: 48/200 | step: 300/927 | trn loss: 3.0021 | val loss: 2.9474 | acc: 27.58\n",
      "epoch: 48/200 | step: 400/927 | trn loss: 2.9805 | val loss: 2.9396 | acc: 25.76\n",
      "epoch: 48/200 | step: 500/927 | trn loss: 3.0247 | val loss: 2.9238 | acc: 28.07\n",
      "epoch: 48/200 | step: 600/927 | trn loss: 2.9913 | val loss: 2.9371 | acc: 27.89\n",
      "epoch: 48/200 | step: 700/927 | trn loss: 2.9828 | val loss: 2.9832 | acc: 26.49\n",
      "epoch: 48/200 | step: 800/927 | trn loss: 3.0427 | val loss: 2.9517 | acc: 26.55\n",
      "epoch: 48/200 | step: 900/927 | trn loss: 3.0311 | val loss: 2.8972 | acc: 28.68\n",
      "epoch: 49/200 | step: 100/927 | trn loss: 2.8764 | val loss: 2.9084 | acc: 26.79\n",
      "epoch: 49/200 | step: 200/927 | trn loss: 2.9504 | val loss: 2.9565 | acc: 27.10\n",
      "epoch: 49/200 | step: 300/927 | trn loss: 2.9688 | val loss: 2.9175 | acc: 27.76\n",
      "epoch: 49/200 | step: 400/927 | trn loss: 2.9394 | val loss: 2.9123 | acc: 28.55\n",
      "epoch: 49/200 | step: 500/927 | trn loss: 2.9940 | val loss: 2.8743 | acc: 29.28\n",
      "epoch: 49/200 | step: 600/927 | trn loss: 2.9850 | val loss: 2.8777 | acc: 28.92\n",
      "epoch: 49/200 | step: 700/927 | trn loss: 2.9548 | val loss: 2.8375 | acc: 29.65\n",
      "epoch: 49/200 | step: 800/927 | trn loss: 2.9443 | val loss: 2.8110 | acc: 29.47\n",
      "epoch: 49/200 | step: 900/927 | trn loss: 2.9007 | val loss: 2.8457 | acc: 28.07\n",
      "epoch: 50/200 | step: 100/927 | trn loss: 2.9662 | val loss: 2.8432 | acc: 28.07\n",
      "epoch: 50/200 | step: 200/927 | trn loss: 2.9022 | val loss: 2.8477 | acc: 29.40\n",
      "epoch: 50/200 | step: 300/927 | trn loss: 2.9416 | val loss: 2.8276 | acc: 28.68\n",
      "epoch: 50/200 | step: 400/927 | trn loss: 2.8997 | val loss: 2.8566 | acc: 29.04\n",
      "epoch: 50/200 | step: 500/927 | trn loss: 2.9041 | val loss: 2.8020 | acc: 29.89\n",
      "epoch: 50/200 | step: 700/927 | trn loss: 2.8948 | val loss: 2.7595 | acc: 31.53\n",
      "epoch: 50/200 | step: 800/927 | trn loss: 2.9107 | val loss: 2.7659 | acc: 28.98\n",
      "epoch: 50/200 | step: 900/927 | trn loss: 2.9301 | val loss: 2.8408 | acc: 29.95\n",
      "epoch: 51/200 | step: 100/927 | trn loss: 2.8182 | val loss: 2.8227 | acc: 28.61\n",
      "epoch: 51/200 | step: 200/927 | trn loss: 2.8230 | val loss: 2.8558 | acc: 28.61\n",
      "epoch: 51/200 | step: 300/927 | trn loss: 2.8768 | val loss: 2.7941 | acc: 29.65\n",
      "epoch: 51/200 | step: 400/927 | trn loss: 2.8086 | val loss: 2.7449 | acc: 31.04\n",
      "epoch: 51/200 | step: 500/927 | trn loss: 2.8519 | val loss: 2.8624 | acc: 29.47\n",
      "epoch: 51/200 | step: 600/927 | trn loss: 2.8252 | val loss: 2.7639 | acc: 30.01\n",
      "epoch: 51/200 | step: 700/927 | trn loss: 2.8781 | val loss: 2.7509 | acc: 30.32\n",
      "epoch: 51/200 | step: 800/927 | trn loss: 2.8302 | val loss: 2.7453 | acc: 32.32\n",
      "epoch: 51/200 | step: 900/927 | trn loss: 2.8943 | val loss: 2.7586 | acc: 30.68\n",
      "epoch: 52/200 | step: 100/927 | trn loss: 2.7628 | val loss: 2.8505 | acc: 29.95\n",
      "epoch: 52/200 | step: 200/927 | trn loss: 2.6848 | val loss: 2.7570 | acc: 30.56\n",
      "epoch: 52/200 | step: 300/927 | trn loss: 2.8367 | val loss: 2.7723 | acc: 29.95\n",
      "epoch: 52/200 | step: 400/927 | trn loss: 2.7546 | val loss: 2.7847 | acc: 29.77\n",
      "epoch: 52/200 | step: 500/927 | trn loss: 2.7606 | val loss: 2.7117 | acc: 32.02\n",
      "epoch: 52/200 | step: 600/927 | trn loss: 2.7988 | val loss: 2.6824 | acc: 32.20\n",
      "epoch: 52/200 | step: 700/927 | trn loss: 2.8251 | val loss: 2.6816 | acc: 31.23\n",
      "epoch: 52/200 | step: 800/927 | trn loss: 2.7825 | val loss: 2.7090 | acc: 30.98\n",
      "epoch: 52/200 | step: 900/927 | trn loss: 2.7720 | val loss: 2.7919 | acc: 30.74\n",
      "epoch: 53/200 | step: 100/927 | trn loss: 2.7432 | val loss: 2.6970 | acc: 30.86\n",
      "epoch: 53/200 | step: 200/927 | trn loss: 2.7453 | val loss: 2.7509 | acc: 30.74\n",
      "epoch: 53/200 | step: 300/927 | trn loss: 2.7728 | val loss: 2.6511 | acc: 32.26\n",
      "epoch: 53/200 | step: 400/927 | trn loss: 2.7629 | val loss: 2.6637 | acc: 31.83\n",
      "epoch: 53/200 | step: 500/927 | trn loss: 2.7500 | val loss: 2.6317 | acc: 32.56\n",
      "epoch: 53/200 | step: 600/927 | trn loss: 2.6950 | val loss: 2.6546 | acc: 34.08\n",
      "epoch: 53/200 | step: 700/927 | trn loss: 2.7940 | val loss: 2.6479 | acc: 33.17\n",
      "epoch: 53/200 | step: 800/927 | trn loss: 2.7270 | val loss: 2.6777 | acc: 31.96\n",
      "epoch: 53/200 | step: 900/927 | trn loss: 2.7394 | val loss: 2.6938 | acc: 31.71\n",
      "epoch: 54/200 | step: 100/927 | trn loss: 2.6370 | val loss: 2.6642 | acc: 31.35\n",
      "epoch: 54/200 | step: 200/927 | trn loss: 2.7142 | val loss: 2.7049 | acc: 32.20\n",
      "epoch: 54/200 | step: 300/927 | trn loss: 2.7428 | val loss: 2.6224 | acc: 33.84\n",
      "epoch: 54/200 | step: 400/927 | trn loss: 2.6270 | val loss: 2.6170 | acc: 32.38\n",
      "epoch: 54/200 | step: 500/927 | trn loss: 2.6495 | val loss: 2.7311 | acc: 30.44\n",
      "epoch: 54/200 | step: 600/927 | trn loss: 2.6406 | val loss: 2.6693 | acc: 32.14\n",
      "epoch: 54/200 | step: 700/927 | trn loss: 2.6869 | val loss: 2.5831 | acc: 32.81\n",
      "epoch: 54/200 | step: 800/927 | trn loss: 2.6458 | val loss: 2.6631 | acc: 33.17\n",
      "epoch: 54/200 | step: 900/927 | trn loss: 2.7048 | val loss: 2.6596 | acc: 32.75\n",
      "epoch: 55/200 | step: 100/927 | trn loss: 2.6033 | val loss: 2.6036 | acc: 32.56\n",
      "epoch: 55/200 | step: 200/927 | trn loss: 2.6191 | val loss: 2.6901 | acc: 32.08\n",
      "epoch: 55/200 | step: 300/927 | trn loss: 2.6227 | val loss: 2.6372 | acc: 33.90\n",
      "epoch: 55/200 | step: 400/927 | trn loss: 2.6336 | val loss: 2.5609 | acc: 34.08\n",
      "epoch: 55/200 | step: 500/927 | trn loss: 2.6513 | val loss: 2.5745 | acc: 34.69\n",
      "epoch: 55/200 | step: 600/927 | trn loss: 2.6626 | val loss: 2.6265 | acc: 32.26\n",
      "epoch: 55/200 | step: 700/927 | trn loss: 2.6828 | val loss: 2.5675 | acc: 35.36\n",
      "epoch: 55/200 | step: 800/927 | trn loss: 2.6187 | val loss: 2.6088 | acc: 34.33\n",
      "epoch: 55/200 | step: 900/927 | trn loss: 2.6882 | val loss: 2.5592 | acc: 33.35\n",
      "epoch: 56/200 | step: 100/927 | trn loss: 2.6088 | val loss: 2.5960 | acc: 33.11\n",
      "epoch: 56/200 | step: 200/927 | trn loss: 2.5484 | val loss: 2.5912 | acc: 33.35\n",
      "epoch: 56/200 | step: 300/927 | trn loss: 2.6121 | val loss: 2.5913 | acc: 34.63\n",
      "epoch: 56/200 | step: 400/927 | trn loss: 2.5599 | val loss: 2.5320 | acc: 36.33\n",
      "epoch: 56/200 | step: 500/927 | trn loss: 2.5555 | val loss: 2.6113 | acc: 33.60\n",
      "epoch: 56/200 | step: 600/927 | trn loss: 2.5935 | val loss: 2.6189 | acc: 34.63\n",
      "epoch: 56/200 | step: 700/927 | trn loss: 2.6046 | val loss: 2.4972 | acc: 36.82\n",
      "epoch: 56/200 | step: 800/927 | trn loss: 2.5868 | val loss: 2.4965 | acc: 35.12\n",
      "epoch: 56/200 | step: 900/927 | trn loss: 2.6244 | val loss: 2.4616 | acc: 36.51\n",
      "epoch: 57/200 | step: 100/927 | trn loss: 2.4429 | val loss: 2.5531 | acc: 36.03\n",
      "epoch: 57/200 | step: 200/927 | trn loss: 2.4906 | val loss: 2.5365 | acc: 34.63\n",
      "epoch: 57/200 | step: 300/927 | trn loss: 2.5487 | val loss: 2.5658 | acc: 34.75\n",
      "epoch: 57/200 | step: 400/927 | trn loss: 2.5345 | val loss: 2.5540 | acc: 34.87\n",
      "epoch: 57/200 | step: 500/927 | trn loss: 2.5370 | val loss: 2.5695 | acc: 34.26\n",
      "epoch: 57/200 | step: 600/927 | trn loss: 2.5786 | val loss: 2.4908 | acc: 36.76\n",
      "epoch: 57/200 | step: 700/927 | trn loss: 2.5053 | val loss: 2.4442 | acc: 37.97\n",
      "epoch: 57/200 | step: 800/927 | trn loss: 2.5637 | val loss: 2.4813 | acc: 36.03\n",
      "epoch: 57/200 | step: 900/927 | trn loss: 2.5026 | val loss: 2.4845 | acc: 36.09\n",
      "epoch: 58/200 | step: 100/927 | trn loss: 2.4383 | val loss: 2.5186 | acc: 36.39\n",
      "epoch: 58/200 | step: 200/927 | trn loss: 2.4099 | val loss: 2.4249 | acc: 38.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 58/200 | step: 300/927 | trn loss: 2.4872 | val loss: 2.4555 | acc: 36.63\n",
      "epoch: 58/200 | step: 400/927 | trn loss: 2.4693 | val loss: 2.4491 | acc: 36.39\n",
      "epoch: 58/200 | step: 500/927 | trn loss: 2.4899 | val loss: 2.4702 | acc: 37.12\n",
      "epoch: 58/200 | step: 600/927 | trn loss: 2.4418 | val loss: 2.4086 | acc: 37.67\n",
      "epoch: 58/200 | step: 700/927 | trn loss: 2.5160 | val loss: 2.4268 | acc: 36.63\n",
      "epoch: 58/200 | step: 800/927 | trn loss: 2.4957 | val loss: 2.4537 | acc: 36.39\n",
      "epoch: 58/200 | step: 900/927 | trn loss: 2.5625 | val loss: 2.3995 | acc: 37.36\n",
      "epoch: 59/200 | step: 100/927 | trn loss: 2.4389 | val loss: 2.4538 | acc: 37.36\n",
      "epoch: 59/200 | step: 200/927 | trn loss: 2.3775 | val loss: 2.3432 | acc: 39.61\n",
      "epoch: 59/200 | step: 300/927 | trn loss: 2.4353 | val loss: 2.3936 | acc: 38.46\n",
      "epoch: 59/200 | step: 400/927 | trn loss: 2.4689 | val loss: 2.3584 | acc: 38.15\n",
      "epoch: 59/200 | step: 500/927 | trn loss: 2.4876 | val loss: 2.4082 | acc: 36.94\n",
      "epoch: 59/200 | step: 600/927 | trn loss: 2.4425 | val loss: 2.4212 | acc: 37.06\n",
      "epoch: 59/200 | step: 700/927 | trn loss: 2.4440 | val loss: 2.3790 | acc: 39.06\n",
      "epoch: 59/200 | step: 800/927 | trn loss: 2.4517 | val loss: 2.3508 | acc: 39.00\n",
      "epoch: 59/200 | step: 900/927 | trn loss: 2.4925 | val loss: 2.4117 | acc: 38.09\n",
      "epoch: 60/200 | step: 100/927 | trn loss: 2.3150 | val loss: 2.3447 | acc: 39.43\n",
      "epoch: 60/200 | step: 200/927 | trn loss: 2.3516 | val loss: 2.4087 | acc: 38.64\n",
      "epoch: 60/200 | step: 300/927 | trn loss: 2.4378 | val loss: 2.3154 | acc: 40.95\n",
      "epoch: 60/200 | step: 400/927 | trn loss: 2.3456 | val loss: 2.3505 | acc: 39.25\n",
      "epoch: 60/200 | step: 500/927 | trn loss: 2.3948 | val loss: 2.3497 | acc: 38.64\n",
      "epoch: 60/200 | step: 600/927 | trn loss: 2.4241 | val loss: 2.4121 | acc: 39.25\n",
      "epoch: 60/200 | step: 700/927 | trn loss: 2.4444 | val loss: 2.3422 | acc: 40.46\n",
      "epoch: 60/200 | step: 800/927 | trn loss: 2.4404 | val loss: 2.3387 | acc: 38.27\n",
      "epoch: 60/200 | step: 900/927 | trn loss: 2.4514 | val loss: 2.3528 | acc: 38.76\n",
      "epoch: 61/200 | step: 100/927 | trn loss: 2.2682 | val loss: 2.3901 | acc: 38.70\n",
      "epoch: 61/200 | step: 200/927 | trn loss: 2.3359 | val loss: 2.3297 | acc: 39.06\n",
      "epoch: 61/200 | step: 300/927 | trn loss: 2.3096 | val loss: 2.3690 | acc: 39.49\n",
      "epoch: 61/200 | step: 400/927 | trn loss: 2.3133 | val loss: 2.2648 | acc: 40.40\n",
      "epoch: 61/200 | step: 500/927 | trn loss: 2.3925 | val loss: 2.3228 | acc: 38.46\n",
      "epoch: 61/200 | step: 600/927 | trn loss: 2.3440 | val loss: 2.2658 | acc: 40.64\n",
      "epoch: 61/200 | step: 700/927 | trn loss: 2.3714 | val loss: 2.3174 | acc: 39.13\n",
      "epoch: 61/200 | step: 800/927 | trn loss: 2.4753 | val loss: 2.3531 | acc: 38.46\n",
      "epoch: 61/200 | step: 900/927 | trn loss: 2.3880 | val loss: 2.3337 | acc: 38.03\n",
      "epoch: 62/200 | step: 100/927 | trn loss: 2.2108 | val loss: 2.2932 | acc: 39.37\n",
      "epoch: 62/200 | step: 200/927 | trn loss: 2.3215 | val loss: 2.2799 | acc: 40.70\n",
      "epoch: 62/200 | step: 300/927 | trn loss: 2.3571 | val loss: 2.3158 | acc: 38.15\n",
      "epoch: 62/200 | step: 400/927 | trn loss: 2.3856 | val loss: 2.3424 | acc: 39.73\n",
      "epoch: 62/200 | step: 500/927 | trn loss: 2.2607 | val loss: 2.2817 | acc: 41.68\n",
      "epoch: 62/200 | step: 600/927 | trn loss: 2.4618 | val loss: 2.2585 | acc: 40.34\n",
      "epoch: 62/200 | step: 700/927 | trn loss: 2.3654 | val loss: 2.3310 | acc: 38.64\n",
      "epoch: 62/200 | step: 800/927 | trn loss: 2.2646 | val loss: 2.3216 | acc: 40.22\n",
      "epoch: 62/200 | step: 900/927 | trn loss: 2.3290 | val loss: 2.3099 | acc: 39.67\n",
      "epoch: 63/200 | step: 100/927 | trn loss: 2.2006 | val loss: 2.2641 | acc: 40.64\n",
      "epoch: 63/200 | step: 200/927 | trn loss: 2.3119 | val loss: 2.3077 | acc: 38.94\n",
      "epoch: 63/200 | step: 300/927 | trn loss: 2.1994 | val loss: 2.2854 | acc: 41.37\n",
      "epoch: 63/200 | step: 400/927 | trn loss: 2.3030 | val loss: 2.2287 | acc: 41.80\n",
      "epoch: 63/200 | step: 500/927 | trn loss: 2.3233 | val loss: 2.2406 | acc: 41.49\n",
      "epoch: 63/200 | step: 600/927 | trn loss: 2.2588 | val loss: 2.3010 | acc: 40.40\n",
      "epoch: 63/200 | step: 700/927 | trn loss: 2.4052 | val loss: 2.3115 | acc: 40.28\n",
      "epoch: 63/200 | step: 800/927 | trn loss: 2.3074 | val loss: 2.2484 | acc: 41.01\n",
      "epoch: 63/200 | step: 900/927 | trn loss: 2.2851 | val loss: 2.2861 | acc: 39.61\n",
      "epoch: 64/200 | step: 100/927 | trn loss: 2.1981 | val loss: 2.3111 | acc: 38.46\n",
      "epoch: 64/200 | step: 200/927 | trn loss: 2.2086 | val loss: 2.2999 | acc: 39.13\n",
      "epoch: 64/200 | step: 300/927 | trn loss: 2.2407 | val loss: 2.3104 | acc: 38.88\n",
      "epoch: 64/200 | step: 400/927 | trn loss: 2.1692 | val loss: 2.2540 | acc: 40.95\n",
      "epoch: 64/200 | step: 500/927 | trn loss: 2.2904 | val loss: 2.1955 | acc: 41.62\n",
      "epoch: 64/200 | step: 600/927 | trn loss: 2.1751 | val loss: 2.2059 | acc: 41.92\n",
      "epoch: 64/200 | step: 700/927 | trn loss: 2.3901 | val loss: 2.2758 | acc: 40.22\n",
      "epoch: 64/200 | step: 800/927 | trn loss: 2.2370 | val loss: 2.2494 | acc: 39.98\n",
      "epoch: 64/200 | step: 900/927 | trn loss: 2.3221 | val loss: 2.2261 | acc: 40.64\n",
      "epoch: 65/200 | step: 100/927 | trn loss: 2.1275 | val loss: 2.2072 | acc: 41.86\n",
      "epoch: 65/200 | step: 200/927 | trn loss: 2.2063 | val loss: 2.2451 | acc: 41.43\n",
      "epoch: 65/200 | step: 300/927 | trn loss: 2.1782 | val loss: 2.2582 | acc: 40.10\n",
      "epoch: 65/200 | step: 400/927 | trn loss: 2.1888 | val loss: 2.2434 | acc: 40.95\n",
      "epoch: 65/200 | step: 500/927 | trn loss: 2.2794 | val loss: 2.2321 | acc: 40.10\n",
      "epoch: 65/200 | step: 600/927 | trn loss: 2.2788 | val loss: 2.2032 | acc: 41.92\n",
      "epoch: 65/200 | step: 700/927 | trn loss: 2.2918 | val loss: 2.2225 | acc: 42.16\n",
      "epoch: 65/200 | step: 800/927 | trn loss: 2.1768 | val loss: 2.2228 | acc: 41.01\n",
      "epoch: 65/200 | step: 900/927 | trn loss: 2.2257 | val loss: 2.2676 | acc: 40.46\n",
      "epoch: 66/200 | step: 100/927 | trn loss: 2.1123 | val loss: 2.2610 | acc: 40.83\n",
      "epoch: 66/200 | step: 200/927 | trn loss: 2.1795 | val loss: 2.2636 | acc: 40.46\n",
      "epoch: 66/200 | step: 300/927 | trn loss: 2.1923 | val loss: 2.2244 | acc: 41.74\n",
      "epoch: 66/200 | step: 400/927 | trn loss: 2.2121 | val loss: 2.2605 | acc: 39.49\n",
      "epoch: 66/200 | step: 500/927 | trn loss: 2.1848 | val loss: 2.2122 | acc: 42.65\n",
      "epoch: 66/200 | step: 600/927 | trn loss: 2.1829 | val loss: 2.2631 | acc: 40.70\n",
      "epoch: 66/200 | step: 700/927 | trn loss: 2.2062 | val loss: 2.2071 | acc: 40.64\n",
      "epoch: 66/200 | step: 800/927 | trn loss: 2.2154 | val loss: 2.1941 | acc: 41.25\n",
      "epoch: 66/200 | step: 900/927 | trn loss: 2.1374 | val loss: 2.2289 | acc: 41.25\n",
      "epoch: 67/200 | step: 100/927 | trn loss: 2.0934 | val loss: 2.2591 | acc: 41.31\n",
      "epoch: 67/200 | step: 200/927 | trn loss: 2.1572 | val loss: 2.2353 | acc: 39.91\n",
      "epoch: 67/200 | step: 300/927 | trn loss: 2.1029 | val loss: 2.1964 | acc: 41.43\n",
      "epoch: 67/200 | step: 400/927 | trn loss: 2.1081 | val loss: 2.1698 | acc: 41.98\n",
      "epoch: 67/200 | step: 500/927 | trn loss: 2.1084 | val loss: 2.1826 | acc: 41.74\n",
      "epoch: 67/200 | step: 600/927 | trn loss: 2.2200 | val loss: 2.1398 | acc: 42.77\n",
      "epoch: 67/200 | step: 700/927 | trn loss: 2.0867 | val loss: 2.1389 | acc: 43.44\n",
      "epoch: 67/200 | step: 800/927 | trn loss: 2.2325 | val loss: 2.1917 | acc: 41.68\n",
      "epoch: 67/200 | step: 900/927 | trn loss: 2.1844 | val loss: 2.1769 | acc: 41.56\n",
      "epoch: 68/200 | step: 100/927 | trn loss: 2.0554 | val loss: 2.2123 | acc: 42.10\n",
      "epoch: 68/200 | step: 200/927 | trn loss: 2.1323 | val loss: 2.1667 | acc: 43.07\n",
      "epoch: 68/200 | step: 300/927 | trn loss: 2.0167 | val loss: 2.2319 | acc: 41.86\n",
      "epoch: 68/200 | step: 400/927 | trn loss: 2.1205 | val loss: 2.2218 | acc: 40.83\n",
      "epoch: 68/200 | step: 500/927 | trn loss: 2.1917 | val loss: 2.1635 | acc: 43.99\n",
      "epoch: 68/200 | step: 600/927 | trn loss: 2.0908 | val loss: 2.2137 | acc: 41.80\n",
      "epoch: 68/200 | step: 700/927 | trn loss: 2.0699 | val loss: 2.2464 | acc: 41.25\n",
      "epoch: 68/200 | step: 800/927 | trn loss: 2.1627 | val loss: 2.1123 | acc: 42.65\n",
      "epoch: 69/200 | step: 100/927 | trn loss: 2.0365 | val loss: 2.1835 | acc: 41.86\n",
      "epoch: 69/200 | step: 200/927 | trn loss: 2.0748 | val loss: 2.1778 | acc: 42.47\n",
      "epoch: 69/200 | step: 300/927 | trn loss: 2.0741 | val loss: 2.1935 | acc: 41.49\n",
      "epoch: 69/200 | step: 400/927 | trn loss: 2.0560 | val loss: 2.2281 | acc: 42.22\n",
      "epoch: 69/200 | step: 500/927 | trn loss: 2.0740 | val loss: 2.2423 | acc: 42.28\n",
      "epoch: 69/200 | step: 600/927 | trn loss: 2.1040 | val loss: 2.1244 | acc: 43.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 69/200 | step: 700/927 | trn loss: 2.0935 | val loss: 2.1572 | acc: 43.38\n",
      "epoch: 69/200 | step: 800/927 | trn loss: 2.1536 | val loss: 2.1736 | acc: 43.01\n",
      "epoch: 69/200 | step: 900/927 | trn loss: 2.0760 | val loss: 2.1826 | acc: 42.10\n",
      "epoch: 70/200 | step: 100/927 | trn loss: 2.0110 | val loss: 2.1793 | acc: 42.83\n",
      "epoch: 70/200 | step: 200/927 | trn loss: 1.9986 | val loss: 2.1260 | acc: 42.71\n",
      "epoch: 70/200 | step: 300/927 | trn loss: 2.0095 | val loss: 2.2615 | acc: 41.68\n",
      "epoch: 70/200 | step: 400/927 | trn loss: 1.9902 | val loss: 2.1470 | acc: 44.65\n",
      "epoch: 70/200 | step: 500/927 | trn loss: 2.1079 | val loss: 2.1364 | acc: 43.26\n",
      "epoch: 70/200 | step: 600/927 | trn loss: 2.0402 | val loss: 2.1208 | acc: 43.26\n",
      "epoch: 70/200 | step: 700/927 | trn loss: 2.0461 | val loss: 2.1502 | acc: 43.01\n",
      "epoch: 70/200 | step: 800/927 | trn loss: 2.1111 | val loss: 2.1507 | acc: 41.92\n",
      "epoch: 70/200 | step: 900/927 | trn loss: 2.1148 | val loss: 2.1383 | acc: 42.16\n",
      "epoch: 71/200 | step: 100/927 | trn loss: 1.9841 | val loss: 2.1358 | acc: 42.28\n",
      "epoch: 71/200 | step: 200/927 | trn loss: 2.0004 | val loss: 2.1710 | acc: 42.77\n",
      "epoch: 71/200 | step: 300/927 | trn loss: 2.0837 | val loss: 2.1541 | acc: 41.62\n",
      "epoch: 71/200 | step: 400/927 | trn loss: 2.0347 | val loss: 2.1037 | acc: 44.90\n",
      "epoch: 71/200 | step: 500/927 | trn loss: 2.0646 | val loss: 2.1208 | acc: 43.07\n",
      "epoch: 71/200 | step: 600/927 | trn loss: 2.0469 | val loss: 2.0907 | acc: 44.84\n",
      "epoch: 71/200 | step: 700/927 | trn loss: 1.9989 | val loss: 2.2229 | acc: 43.13\n",
      "epoch: 71/200 | step: 800/927 | trn loss: 2.0965 | val loss: 2.1386 | acc: 43.13\n",
      "epoch: 71/200 | step: 900/927 | trn loss: 2.0652 | val loss: 2.1300 | acc: 44.11\n",
      "epoch: 72/200 | step: 100/927 | trn loss: 1.9422 | val loss: 2.1289 | acc: 44.35\n",
      "epoch: 72/200 | step: 200/927 | trn loss: 2.0133 | val loss: 2.1291 | acc: 42.95\n",
      "epoch: 72/200 | step: 300/927 | trn loss: 1.9783 | val loss: 2.1836 | acc: 43.01\n",
      "epoch: 72/200 | step: 400/927 | trn loss: 1.8973 | val loss: 2.2109 | acc: 40.40\n",
      "epoch: 72/200 | step: 500/927 | trn loss: 2.0074 | val loss: 2.1813 | acc: 42.95\n",
      "epoch: 72/200 | step: 600/927 | trn loss: 2.0628 | val loss: 2.2043 | acc: 42.10\n",
      "epoch: 72/200 | step: 700/927 | trn loss: 2.0116 | val loss: 2.1559 | acc: 42.89\n",
      "epoch: 72/200 | step: 800/927 | trn loss: 2.0304 | val loss: 2.1430 | acc: 42.65\n",
      "epoch: 72/200 | step: 900/927 | trn loss: 2.0952 | val loss: 2.1434 | acc: 41.62\n",
      "epoch: 73/200 | step: 100/927 | trn loss: 1.9182 | val loss: 2.1881 | acc: 41.31\n",
      "epoch: 73/200 | step: 200/927 | trn loss: 1.9481 | val loss: 2.1757 | acc: 44.17\n",
      "epoch: 73/200 | step: 300/927 | trn loss: 2.0592 | val loss: 2.1527 | acc: 42.83\n",
      "epoch: 73/200 | step: 400/927 | trn loss: 1.8833 | val loss: 2.1658 | acc: 42.04\n",
      "epoch: 73/200 | step: 500/927 | trn loss: 1.9510 | val loss: 2.0953 | acc: 43.56\n",
      "epoch: 73/200 | step: 600/927 | trn loss: 2.0394 | val loss: 2.0770 | acc: 43.50\n",
      "epoch: 73/200 | step: 700/927 | trn loss: 1.9526 | val loss: 2.1435 | acc: 42.71\n",
      "epoch: 73/200 | step: 800/927 | trn loss: 2.0546 | val loss: 2.1226 | acc: 43.26\n",
      "epoch: 73/200 | step: 900/927 | trn loss: 2.0271 | val loss: 2.1289 | acc: 42.16\n",
      "epoch: 74/200 | step: 100/927 | trn loss: 1.9201 | val loss: 2.0751 | acc: 44.05\n",
      "epoch: 74/200 | step: 200/927 | trn loss: 1.9015 | val loss: 2.1144 | acc: 42.71\n",
      "epoch: 74/200 | step: 300/927 | trn loss: 2.0122 | val loss: 2.0970 | acc: 45.20\n",
      "epoch: 74/200 | step: 400/927 | trn loss: 1.8665 | val loss: 2.1336 | acc: 43.92\n",
      "epoch: 74/200 | step: 500/927 | trn loss: 1.9801 | val loss: 2.1125 | acc: 45.57\n",
      "epoch: 74/200 | step: 600/927 | trn loss: 2.0033 | val loss: 2.1390 | acc: 43.68\n",
      "epoch: 74/200 | step: 700/927 | trn loss: 1.9953 | val loss: 2.0559 | acc: 46.05\n",
      "epoch: 74/200 | step: 800/927 | trn loss: 1.9344 | val loss: 2.0830 | acc: 44.53\n",
      "epoch: 74/200 | step: 900/927 | trn loss: 2.0280 | val loss: 2.0898 | acc: 44.35\n",
      "epoch: 75/200 | step: 100/927 | trn loss: 1.8858 | val loss: 2.1409 | acc: 44.47\n",
      "epoch: 75/200 | step: 200/927 | trn loss: 1.9366 | val loss: 2.1551 | acc: 42.95\n",
      "epoch: 75/200 | step: 300/927 | trn loss: 1.9382 | val loss: 2.0773 | acc: 45.20\n",
      "epoch: 75/200 | step: 400/927 | trn loss: 1.9340 | val loss: 2.0845 | acc: 45.50\n",
      "epoch: 75/200 | step: 500/927 | trn loss: 1.9880 | val loss: 2.1339 | acc: 43.32\n",
      "epoch: 75/200 | step: 600/927 | trn loss: 1.9211 | val loss: 2.0185 | acc: 44.90\n",
      "epoch: 75/200 | step: 700/927 | trn loss: 1.9166 | val loss: 2.1262 | acc: 43.92\n",
      "epoch: 75/200 | step: 800/927 | trn loss: 1.9708 | val loss: 2.0951 | acc: 43.80\n",
      "epoch: 75/200 | step: 900/927 | trn loss: 1.9105 | val loss: 2.0927 | acc: 44.65\n",
      "epoch: 76/200 | step: 100/927 | trn loss: 1.7963 | val loss: 2.1797 | acc: 42.77\n",
      "epoch: 76/200 | step: 200/927 | trn loss: 1.8851 | val loss: 2.1036 | acc: 43.56\n",
      "epoch: 76/200 | step: 300/927 | trn loss: 1.9189 | val loss: 2.1408 | acc: 45.20\n",
      "epoch: 76/200 | step: 400/927 | trn loss: 1.8541 | val loss: 2.0957 | acc: 45.14\n",
      "epoch: 76/200 | step: 500/927 | trn loss: 1.8907 | val loss: 2.0907 | acc: 43.50\n",
      "epoch: 76/200 | step: 600/927 | trn loss: 1.8877 | val loss: 2.1404 | acc: 42.22\n",
      "epoch: 76/200 | step: 700/927 | trn loss: 2.0283 | val loss: 2.1299 | acc: 43.56\n",
      "epoch: 76/200 | step: 800/927 | trn loss: 1.9093 | val loss: 2.0264 | acc: 45.69\n",
      "epoch: 76/200 | step: 900/927 | trn loss: 1.9470 | val loss: 2.0433 | acc: 45.32\n",
      "epoch: 77/200 | step: 100/927 | trn loss: 1.9353 | val loss: 2.0132 | acc: 45.14\n",
      "epoch: 77/200 | step: 200/927 | trn loss: 1.7753 | val loss: 2.0743 | acc: 45.44\n",
      "epoch: 77/200 | step: 300/927 | trn loss: 1.8396 | val loss: 2.0576 | acc: 45.93\n",
      "epoch: 77/200 | step: 400/927 | trn loss: 1.8670 | val loss: 2.0427 | acc: 45.20\n",
      "epoch: 77/200 | step: 500/927 | trn loss: 1.9456 | val loss: 2.0686 | acc: 45.32\n",
      "epoch: 77/200 | step: 600/927 | trn loss: 1.8883 | val loss: 2.0444 | acc: 45.14\n",
      "epoch: 77/200 | step: 700/927 | trn loss: 1.8700 | val loss: 2.0114 | acc: 46.96\n",
      "epoch: 77/200 | step: 800/927 | trn loss: 1.9438 | val loss: 2.0602 | acc: 44.90\n",
      "epoch: 77/200 | step: 900/927 | trn loss: 1.9002 | val loss: 2.0036 | acc: 46.35\n",
      "epoch: 78/200 | step: 100/927 | trn loss: 1.8185 | val loss: 2.1691 | acc: 43.26\n",
      "epoch: 78/200 | step: 200/927 | trn loss: 1.8290 | val loss: 2.1247 | acc: 45.08\n",
      "epoch: 78/200 | step: 300/927 | trn loss: 1.8181 | val loss: 2.1342 | acc: 44.17\n",
      "epoch: 78/200 | step: 400/927 | trn loss: 1.7761 | val loss: 2.0821 | acc: 44.41\n",
      "epoch: 78/200 | step: 500/927 | trn loss: 1.8600 | val loss: 2.0759 | acc: 44.84\n",
      "epoch: 78/200 | step: 600/927 | trn loss: 1.9163 | val loss: 2.0932 | acc: 44.90\n",
      "epoch: 78/200 | step: 700/927 | trn loss: 1.8348 | val loss: 2.1965 | acc: 42.95\n",
      "epoch: 78/200 | step: 800/927 | trn loss: 1.9461 | val loss: 2.1194 | acc: 44.71\n",
      "epoch: 78/200 | step: 900/927 | trn loss: 1.8807 | val loss: 2.0737 | acc: 45.20\n",
      "epoch: 79/200 | step: 100/927 | trn loss: 1.7621 | val loss: 2.0431 | acc: 46.78\n",
      "epoch: 79/200 | step: 200/927 | trn loss: 1.7904 | val loss: 2.0856 | acc: 45.14\n",
      "epoch: 79/200 | step: 300/927 | trn loss: 1.7955 | val loss: 2.0687 | acc: 45.44\n",
      "epoch: 79/200 | step: 400/927 | trn loss: 1.8516 | val loss: 2.0572 | acc: 44.53\n",
      "epoch: 79/200 | step: 500/927 | trn loss: 1.8585 | val loss: 2.1180 | acc: 45.08\n",
      "epoch: 79/200 | step: 600/927 | trn loss: 1.8874 | val loss: 2.0873 | acc: 45.20\n",
      "epoch: 79/200 | step: 700/927 | trn loss: 1.7892 | val loss: 2.0906 | acc: 43.92\n",
      "epoch: 79/200 | step: 800/927 | trn loss: 1.9095 | val loss: 2.0742 | acc: 44.29\n",
      "epoch: 79/200 | step: 900/927 | trn loss: 1.9132 | val loss: 2.1516 | acc: 42.83\n",
      "epoch: 80/200 | step: 100/927 | trn loss: 1.8176 | val loss: 2.0767 | acc: 45.69\n",
      "epoch: 80/200 | step: 200/927 | trn loss: 1.7694 | val loss: 2.0763 | acc: 45.99\n",
      "epoch: 80/200 | step: 300/927 | trn loss: 1.7906 | val loss: 2.1654 | acc: 44.47\n",
      "epoch: 80/200 | step: 400/927 | trn loss: 1.7833 | val loss: 2.0950 | acc: 44.71\n",
      "epoch: 80/200 | step: 500/927 | trn loss: 1.9281 | val loss: 1.9924 | acc: 46.78\n",
      "epoch: 80/200 | step: 600/927 | trn loss: 1.8804 | val loss: 2.1479 | acc: 43.07\n",
      "epoch: 80/200 | step: 700/927 | trn loss: 1.8554 | val loss: 2.0941 | acc: 44.41\n",
      "epoch: 80/200 | step: 800/927 | trn loss: 1.7825 | val loss: 2.0749 | acc: 45.69\n",
      "epoch: 80/200 | step: 900/927 | trn loss: 1.9303 | val loss: 2.0838 | acc: 43.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 81/200 | step: 100/927 | trn loss: 1.7248 | val loss: 2.0518 | acc: 46.96\n",
      "epoch: 81/200 | step: 200/927 | trn loss: 1.7450 | val loss: 1.9896 | acc: 47.27\n",
      "epoch: 81/200 | step: 300/927 | trn loss: 1.7733 | val loss: 2.0365 | acc: 45.93\n",
      "epoch: 81/200 | step: 400/927 | trn loss: 1.8439 | val loss: 2.0074 | acc: 47.14\n",
      "epoch: 81/200 | step: 500/927 | trn loss: 1.8380 | val loss: 2.0303 | acc: 44.84\n",
      "epoch: 81/200 | step: 600/927 | trn loss: 1.8295 | val loss: 2.1118 | acc: 44.23\n",
      "epoch: 81/200 | step: 700/927 | trn loss: 1.8765 | val loss: 2.0362 | acc: 46.42\n",
      "epoch: 81/200 | step: 800/927 | trn loss: 1.8341 | val loss: 1.9905 | acc: 46.90\n",
      "epoch: 81/200 | step: 900/927 | trn loss: 1.7663 | val loss: 2.0444 | acc: 44.35\n",
      "epoch: 82/200 | step: 100/927 | trn loss: 1.6836 | val loss: 2.0773 | acc: 45.93\n",
      "epoch: 82/200 | step: 200/927 | trn loss: 1.7194 | val loss: 1.9948 | acc: 46.84\n",
      "epoch: 82/200 | step: 300/927 | trn loss: 1.7110 | val loss: 1.9861 | acc: 47.27\n",
      "epoch: 82/200 | step: 400/927 | trn loss: 1.7479 | val loss: 2.0385 | acc: 46.35\n",
      "epoch: 82/200 | step: 500/927 | trn loss: 1.7465 | val loss: 1.9866 | acc: 46.78\n",
      "epoch: 82/200 | step: 600/927 | trn loss: 1.7645 | val loss: 2.0889 | acc: 45.99\n",
      "epoch: 82/200 | step: 700/927 | trn loss: 1.8034 | val loss: 2.0648 | acc: 46.17\n",
      "epoch: 82/200 | step: 800/927 | trn loss: 1.8816 | val loss: 2.0112 | acc: 46.90\n",
      "epoch: 82/200 | step: 900/927 | trn loss: 1.8109 | val loss: 1.9717 | acc: 45.99\n",
      "epoch: 83/200 | step: 100/927 | trn loss: 1.7294 | val loss: 2.0817 | acc: 44.65\n",
      "epoch: 83/200 | step: 200/927 | trn loss: 1.7912 | val loss: 2.0705 | acc: 45.08\n",
      "epoch: 83/200 | step: 300/927 | trn loss: 1.7127 | val loss: 2.0191 | acc: 47.14\n",
      "epoch: 83/200 | step: 400/927 | trn loss: 1.7352 | val loss: 2.0500 | acc: 46.29\n",
      "epoch: 83/200 | step: 500/927 | trn loss: 1.8054 | val loss: 2.0127 | acc: 46.78\n",
      "epoch: 83/200 | step: 600/927 | trn loss: 1.7592 | val loss: 2.0299 | acc: 45.75\n",
      "epoch: 83/200 | step: 700/927 | trn loss: 1.7171 | val loss: 2.0112 | acc: 46.29\n",
      "epoch: 83/200 | step: 800/927 | trn loss: 1.7117 | val loss: 2.0063 | acc: 46.78\n",
      "epoch: 83/200 | step: 900/927 | trn loss: 1.7575 | val loss: 2.0782 | acc: 45.50\n",
      "epoch: 84/200 | step: 100/927 | trn loss: 1.8208 | val loss: 1.9659 | acc: 48.30\n",
      "epoch: 84/200 | step: 200/927 | trn loss: 1.6678 | val loss: 2.0237 | acc: 46.29\n",
      "epoch: 84/200 | step: 300/927 | trn loss: 1.7341 | val loss: 2.0268 | acc: 45.99\n",
      "epoch: 84/200 | step: 400/927 | trn loss: 1.7579 | val loss: 2.0252 | acc: 46.11\n",
      "epoch: 84/200 | step: 500/927 | trn loss: 1.7377 | val loss: 2.0531 | acc: 45.57\n",
      "epoch: 84/200 | step: 600/927 | trn loss: 1.6729 | val loss: 1.9994 | acc: 46.72\n",
      "epoch: 84/200 | step: 700/927 | trn loss: 1.7236 | val loss: 2.1502 | acc: 43.68\n",
      "epoch: 84/200 | step: 800/927 | trn loss: 1.7332 | val loss: 2.0680 | acc: 45.14\n",
      "epoch: 84/200 | step: 900/927 | trn loss: 1.7491 | val loss: 2.0796 | acc: 45.50\n",
      "epoch: 85/200 | step: 100/927 | trn loss: 1.6827 | val loss: 2.0547 | acc: 46.11\n",
      "epoch: 85/200 | step: 200/927 | trn loss: 1.6096 | val loss: 2.0060 | acc: 45.93\n",
      "epoch: 85/200 | step: 300/927 | trn loss: 1.6881 | val loss: 2.0333 | acc: 45.44\n",
      "epoch: 85/200 | step: 400/927 | trn loss: 1.6493 | val loss: 2.0488 | acc: 46.90\n",
      "epoch: 85/200 | step: 500/927 | trn loss: 1.8072 | val loss: 2.0052 | acc: 46.96\n",
      "epoch: 85/200 | step: 600/927 | trn loss: 1.8155 | val loss: 2.0148 | acc: 45.02\n",
      "epoch: 85/200 | step: 700/927 | trn loss: 1.7127 | val loss: 2.0475 | acc: 45.57\n",
      "epoch: 85/200 | step: 800/927 | trn loss: 1.8219 | val loss: 2.0158 | acc: 47.02\n",
      "epoch: 85/200 | step: 900/927 | trn loss: 1.7373 | val loss: 1.9904 | acc: 46.48\n",
      "epoch: 86/200 | step: 100/927 | trn loss: 1.6676 | val loss: 2.0111 | acc: 45.81\n",
      "epoch: 86/200 | step: 200/927 | trn loss: 1.7000 | val loss: 2.0908 | acc: 44.53\n",
      "epoch: 86/200 | step: 300/927 | trn loss: 1.6601 | val loss: 1.9768 | acc: 47.21\n",
      "epoch: 86/200 | step: 400/927 | trn loss: 1.6800 | val loss: 2.0219 | acc: 46.84\n",
      "epoch: 86/200 | step: 500/927 | trn loss: 1.7034 | val loss: 2.0282 | acc: 45.93\n",
      "epoch: 86/200 | step: 600/927 | trn loss: 1.7169 | val loss: 2.0337 | acc: 47.27\n",
      "epoch: 86/200 | step: 700/927 | trn loss: 1.7009 | val loss: 1.9664 | acc: 46.35\n",
      "epoch: 86/200 | step: 800/927 | trn loss: 1.7191 | val loss: 2.0019 | acc: 47.08\n",
      "epoch: 86/200 | step: 900/927 | trn loss: 1.7405 | val loss: 2.0448 | acc: 47.57\n",
      "epoch: 87/200 | step: 100/927 | trn loss: 1.7229 | val loss: 2.0142 | acc: 46.66\n",
      "epoch: 87/200 | step: 200/927 | trn loss: 1.6915 | val loss: 2.0471 | acc: 46.35\n",
      "epoch: 87/200 | step: 300/927 | trn loss: 1.6398 | val loss: 2.0670 | acc: 45.81\n",
      "epoch: 87/200 | step: 400/927 | trn loss: 1.6447 | val loss: 2.0938 | acc: 45.57\n",
      "epoch: 87/200 | step: 500/927 | trn loss: 1.6987 | val loss: 1.9927 | acc: 46.23\n",
      "epoch: 87/200 | step: 600/927 | trn loss: 1.6607 | val loss: 2.0348 | acc: 46.17\n",
      "epoch: 87/200 | step: 700/927 | trn loss: 1.7802 | val loss: 1.9759 | acc: 44.90\n",
      "epoch: 87/200 | step: 800/927 | trn loss: 1.7121 | val loss: 2.0395 | acc: 45.38\n",
      "epoch: 87/200 | step: 900/927 | trn loss: 1.6693 | val loss: 1.9737 | acc: 46.66\n",
      "epoch: 88/200 | step: 100/927 | trn loss: 1.6775 | val loss: 1.9814 | acc: 47.93\n",
      "epoch: 88/200 | step: 200/927 | trn loss: 1.6515 | val loss: 2.0406 | acc: 47.08\n",
      "epoch: 88/200 | step: 300/927 | trn loss: 1.7040 | val loss: 1.9598 | acc: 47.27\n",
      "epoch: 88/200 | step: 400/927 | trn loss: 1.6957 | val loss: 2.0020 | acc: 46.90\n",
      "epoch: 88/200 | step: 500/927 | trn loss: 1.6793 | val loss: 2.0185 | acc: 47.57\n",
      "epoch: 88/200 | step: 600/927 | trn loss: 1.6298 | val loss: 1.9608 | acc: 48.06\n",
      "epoch: 88/200 | step: 700/927 | trn loss: 1.7261 | val loss: 1.9785 | acc: 47.08\n",
      "epoch: 88/200 | step: 800/927 | trn loss: 1.7375 | val loss: 1.9337 | acc: 47.75\n",
      "epoch: 88/200 | step: 900/927 | trn loss: 1.7103 | val loss: 1.9548 | acc: 47.14\n",
      "epoch: 89/200 | step: 100/927 | trn loss: 1.6224 | val loss: 2.0168 | acc: 47.02\n",
      "epoch: 89/200 | step: 200/927 | trn loss: 1.6678 | val loss: 2.0557 | acc: 46.54\n",
      "epoch: 89/200 | step: 300/927 | trn loss: 1.5796 | val loss: 2.0080 | acc: 46.42\n",
      "epoch: 89/200 | step: 400/927 | trn loss: 1.6716 | val loss: 2.0283 | acc: 46.29\n",
      "epoch: 89/200 | step: 500/927 | trn loss: 1.6525 | val loss: 1.9753 | acc: 48.66\n",
      "epoch: 89/200 | step: 600/927 | trn loss: 1.7527 | val loss: 2.0195 | acc: 46.66\n",
      "epoch: 89/200 | step: 700/927 | trn loss: 1.6213 | val loss: 2.0205 | acc: 46.42\n",
      "epoch: 89/200 | step: 800/927 | trn loss: 1.7300 | val loss: 2.0322 | acc: 47.45\n",
      "epoch: 89/200 | step: 900/927 | trn loss: 1.7296 | val loss: 1.9910 | acc: 47.51\n",
      "epoch: 90/200 | step: 100/927 | trn loss: 1.6159 | val loss: 1.9925 | acc: 47.93\n",
      "epoch: 90/200 | step: 200/927 | trn loss: 1.5946 | val loss: 2.0457 | acc: 45.93\n",
      "epoch: 90/200 | step: 300/927 | trn loss: 1.6243 | val loss: 1.9794 | acc: 48.42\n",
      "epoch: 90/200 | step: 400/927 | trn loss: 1.6786 | val loss: 1.9819 | acc: 47.69\n",
      "epoch: 90/200 | step: 500/927 | trn loss: 1.6272 | val loss: 1.9698 | acc: 47.63\n",
      "epoch: 90/200 | step: 600/927 | trn loss: 1.6493 | val loss: 1.9375 | acc: 48.97\n",
      "epoch: 90/200 | step: 700/927 | trn loss: 1.7370 | val loss: 1.9919 | acc: 46.54\n",
      "epoch: 90/200 | step: 800/927 | trn loss: 1.6709 | val loss: 2.0096 | acc: 47.75\n",
      "epoch: 90/200 | step: 900/927 | trn loss: 1.7499 | val loss: 1.9835 | acc: 46.66\n",
      "epoch: 91/200 | step: 100/927 | trn loss: 1.5874 | val loss: 2.0007 | acc: 45.99\n",
      "epoch: 91/200 | step: 200/927 | trn loss: 1.5681 | val loss: 2.0486 | acc: 45.44\n",
      "epoch: 91/200 | step: 300/927 | trn loss: 1.6235 | val loss: 1.9456 | acc: 48.91\n",
      "epoch: 91/200 | step: 400/927 | trn loss: 1.5869 | val loss: 1.9343 | acc: 48.54\n",
      "epoch: 91/200 | step: 500/927 | trn loss: 1.5859 | val loss: 1.9942 | acc: 47.14\n",
      "epoch: 91/200 | step: 600/927 | trn loss: 1.6339 | val loss: 1.9584 | acc: 47.93\n",
      "epoch: 91/200 | step: 700/927 | trn loss: 1.6312 | val loss: 1.9564 | acc: 48.66\n",
      "epoch: 91/200 | step: 800/927 | trn loss: 1.6823 | val loss: 1.9127 | acc: 50.30\n",
      "epoch: 91/200 | step: 900/927 | trn loss: 1.7185 | val loss: 1.9765 | acc: 48.72\n",
      "epoch: 92/200 | step: 100/927 | trn loss: 1.5605 | val loss: 1.9335 | acc: 48.91\n",
      "epoch: 92/200 | step: 200/927 | trn loss: 1.6141 | val loss: 1.9660 | acc: 48.06\n",
      "epoch: 92/200 | step: 300/927 | trn loss: 1.6413 | val loss: 1.9143 | acc: 49.70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 92/200 | step: 400/927 | trn loss: 1.6164 | val loss: 2.0082 | acc: 47.14\n",
      "epoch: 92/200 | step: 500/927 | trn loss: 1.5763 | val loss: 2.0184 | acc: 47.08\n",
      "epoch: 92/200 | step: 600/927 | trn loss: 1.6587 | val loss: 1.9809 | acc: 48.85\n",
      "epoch: 92/200 | step: 700/927 | trn loss: 1.6415 | val loss: 1.9887 | acc: 49.09\n",
      "epoch: 92/200 | step: 800/927 | trn loss: 1.6576 | val loss: 1.9063 | acc: 49.09\n",
      "epoch: 92/200 | step: 900/927 | trn loss: 1.5922 | val loss: 1.9751 | acc: 48.97\n",
      "epoch: 93/200 | step: 100/927 | trn loss: 1.4564 | val loss: 1.9734 | acc: 48.00\n",
      "epoch: 93/200 | step: 200/927 | trn loss: 1.6497 | val loss: 1.9413 | acc: 47.93\n",
      "epoch: 93/200 | step: 300/927 | trn loss: 1.4788 | val loss: 1.9909 | acc: 47.69\n",
      "epoch: 93/200 | step: 400/927 | trn loss: 1.5399 | val loss: 1.9823 | acc: 47.27\n",
      "epoch: 93/200 | step: 500/927 | trn loss: 1.6304 | val loss: 1.9361 | acc: 49.82\n",
      "epoch: 93/200 | step: 600/927 | trn loss: 1.5275 | val loss: 1.9015 | acc: 49.51\n",
      "epoch: 93/200 | step: 700/927 | trn loss: 1.6433 | val loss: 1.8942 | acc: 48.60\n",
      "epoch: 93/200 | step: 800/927 | trn loss: 1.6674 | val loss: 1.8903 | acc: 50.30\n",
      "epoch: 93/200 | step: 900/927 | trn loss: 1.6871 | val loss: 1.9595 | acc: 48.78\n",
      "epoch: 94/200 | step: 100/927 | trn loss: 1.5127 | val loss: 1.9503 | acc: 47.45\n",
      "epoch: 94/200 | step: 200/927 | trn loss: 1.5256 | val loss: 1.9522 | acc: 48.00\n",
      "epoch: 94/200 | step: 300/927 | trn loss: 1.5551 | val loss: 1.9384 | acc: 47.33\n",
      "epoch: 94/200 | step: 400/927 | trn loss: 1.5989 | val loss: 1.9829 | acc: 46.54\n",
      "epoch: 94/200 | step: 500/927 | trn loss: 1.5587 | val loss: 2.0320 | acc: 46.90\n",
      "epoch: 94/200 | step: 600/927 | trn loss: 1.5249 | val loss: 1.9332 | acc: 47.93\n",
      "epoch: 94/200 | step: 700/927 | trn loss: 1.5249 | val loss: 2.0316 | acc: 47.81\n",
      "epoch: 94/200 | step: 800/927 | trn loss: 1.6894 | val loss: 1.9745 | acc: 47.87\n",
      "epoch: 94/200 | step: 900/927 | trn loss: 1.6743 | val loss: 1.9846 | acc: 47.57\n",
      "epoch: 95/200 | step: 100/927 | trn loss: 1.5239 | val loss: 2.0121 | acc: 46.66\n",
      "epoch: 95/200 | step: 200/927 | trn loss: 1.5977 | val loss: 1.9490 | acc: 47.33\n",
      "epoch: 95/200 | step: 300/927 | trn loss: 1.5522 | val loss: 2.0388 | acc: 47.51\n",
      "epoch: 95/200 | step: 400/927 | trn loss: 1.5458 | val loss: 1.9722 | acc: 48.12\n",
      "epoch: 95/200 | step: 500/927 | trn loss: 1.5897 | val loss: 1.9267 | acc: 49.09\n",
      "epoch: 95/200 | step: 600/927 | trn loss: 1.5705 | val loss: 1.9743 | acc: 48.66\n",
      "epoch: 95/200 | step: 700/927 | trn loss: 1.6322 | val loss: 1.9491 | acc: 48.24\n",
      "epoch: 95/200 | step: 800/927 | trn loss: 1.6440 | val loss: 1.8832 | acc: 48.78\n",
      "epoch: 95/200 | step: 900/927 | trn loss: 1.5881 | val loss: 1.8957 | acc: 48.91\n",
      "epoch: 96/200 | step: 100/927 | trn loss: 1.4997 | val loss: 1.9291 | acc: 49.94\n",
      "epoch: 96/200 | step: 200/927 | trn loss: 1.5793 | val loss: 1.9992 | acc: 46.96\n",
      "epoch: 96/200 | step: 300/927 | trn loss: 1.5222 | val loss: 1.9553 | acc: 48.48\n",
      "epoch: 96/200 | step: 400/927 | trn loss: 1.5854 | val loss: 2.0349 | acc: 47.69\n",
      "epoch: 96/200 | step: 500/927 | trn loss: 1.5909 | val loss: 1.9465 | acc: 47.93\n",
      "epoch: 96/200 | step: 600/927 | trn loss: 1.5749 | val loss: 2.0197 | acc: 46.29\n",
      "epoch: 96/200 | step: 700/927 | trn loss: 1.6419 | val loss: 1.9503 | acc: 49.33\n",
      "epoch: 96/200 | step: 800/927 | trn loss: 1.6212 | val loss: 1.8910 | acc: 49.27\n",
      "epoch: 96/200 | step: 900/927 | trn loss: 1.6670 | val loss: 1.9044 | acc: 49.82\n",
      "epoch: 97/200 | step: 100/927 | trn loss: 1.4750 | val loss: 1.9670 | acc: 47.75\n",
      "epoch: 97/200 | step: 200/927 | trn loss: 1.4445 | val loss: 1.9352 | acc: 49.27\n",
      "epoch: 97/200 | step: 300/927 | trn loss: 1.5249 | val loss: 1.9466 | acc: 48.42\n",
      "epoch: 97/200 | step: 400/927 | trn loss: 1.5061 | val loss: 1.9776 | acc: 49.64\n",
      "epoch: 97/200 | step: 500/927 | trn loss: 1.5044 | val loss: 1.9131 | acc: 49.51\n",
      "epoch: 97/200 | step: 600/927 | trn loss: 1.5177 | val loss: 1.9768 | acc: 47.87\n",
      "epoch: 97/200 | step: 700/927 | trn loss: 1.6137 | val loss: 2.0035 | acc: 47.33\n",
      "epoch: 97/200 | step: 800/927 | trn loss: 1.6250 | val loss: 1.9670 | acc: 48.72\n",
      "epoch: 97/200 | step: 900/927 | trn loss: 1.5293 | val loss: 2.0122 | acc: 47.27\n",
      "epoch: 98/200 | step: 100/927 | trn loss: 1.4745 | val loss: 1.9297 | acc: 47.33\n",
      "epoch: 98/200 | step: 200/927 | trn loss: 1.5124 | val loss: 1.9618 | acc: 46.66\n",
      "epoch: 98/200 | step: 300/927 | trn loss: 1.5477 | val loss: 1.8920 | acc: 50.18\n",
      "epoch: 98/200 | step: 400/927 | trn loss: 1.4966 | val loss: 1.9282 | acc: 48.66\n",
      "epoch: 98/200 | step: 500/927 | trn loss: 1.5292 | val loss: 1.9583 | acc: 47.81\n",
      "epoch: 98/200 | step: 600/927 | trn loss: 1.5984 | val loss: 1.9271 | acc: 47.81\n",
      "epoch: 98/200 | step: 700/927 | trn loss: 1.6035 | val loss: 1.9235 | acc: 48.30\n",
      "epoch: 98/200 | step: 800/927 | trn loss: 1.4837 | val loss: 2.0018 | acc: 48.06\n",
      "epoch: 98/200 | step: 900/927 | trn loss: 1.5680 | val loss: 1.9668 | acc: 48.48\n",
      "epoch: 99/200 | step: 100/927 | trn loss: 1.4423 | val loss: 1.9778 | acc: 48.48\n",
      "epoch: 99/200 | step: 200/927 | trn loss: 1.5507 | val loss: 1.9990 | acc: 46.60\n",
      "epoch: 99/200 | step: 300/927 | trn loss: 1.5169 | val loss: 2.0762 | acc: 47.08\n",
      "epoch: 99/200 | step: 400/927 | trn loss: 1.5486 | val loss: 1.9105 | acc: 48.66\n",
      "epoch: 99/200 | step: 500/927 | trn loss: 1.4828 | val loss: 1.9883 | acc: 48.85\n",
      "epoch: 99/200 | step: 600/927 | trn loss: 1.5917 | val loss: 1.9751 | acc: 45.87\n",
      "epoch: 99/200 | step: 700/927 | trn loss: 1.6228 | val loss: 1.8557 | acc: 50.97\n",
      "epoch: 99/200 | step: 800/927 | trn loss: 1.5188 | val loss: 1.9769 | acc: 48.00\n",
      "epoch: 99/200 | step: 900/927 | trn loss: 1.5529 | val loss: 1.8749 | acc: 50.85\n",
      "epoch: 100/200 | step: 100/927 | trn loss: 1.4508 | val loss: 1.9730 | acc: 47.08\n",
      "epoch: 100/200 | step: 200/927 | trn loss: 1.4427 | val loss: 2.0156 | acc: 47.51\n",
      "epoch: 100/200 | step: 300/927 | trn loss: 1.3923 | val loss: 1.9676 | acc: 50.61\n",
      "epoch: 100/200 | step: 400/927 | trn loss: 1.4755 | val loss: 1.9742 | acc: 48.48\n",
      "epoch: 100/200 | step: 500/927 | trn loss: 1.5657 | val loss: 1.9685 | acc: 47.69\n",
      "epoch: 100/200 | step: 600/927 | trn loss: 1.5242 | val loss: 1.9293 | acc: 48.24\n",
      "epoch: 100/200 | step: 700/927 | trn loss: 1.5607 | val loss: 1.9215 | acc: 49.27\n",
      "epoch: 100/200 | step: 800/927 | trn loss: 1.5461 | val loss: 1.8636 | acc: 50.73\n",
      "epoch: 100/200 | step: 900/927 | trn loss: 1.6221 | val loss: 1.9320 | acc: 49.09\n",
      "epoch: 101/200 | step: 100/927 | trn loss: 1.4351 | val loss: 1.7699 | acc: 52.79\n",
      "epoch: 101/200 | step: 200/927 | trn loss: 1.2115 | val loss: 1.7285 | acc: 52.73\n",
      "epoch: 101/200 | step: 300/927 | trn loss: 1.2500 | val loss: 1.7279 | acc: 54.86\n",
      "epoch: 101/200 | step: 400/927 | trn loss: 1.2374 | val loss: 1.6832 | acc: 54.62\n",
      "epoch: 101/200 | step: 500/927 | trn loss: 1.1906 | val loss: 1.6936 | acc: 54.62\n",
      "epoch: 101/200 | step: 600/927 | trn loss: 1.2480 | val loss: 1.7066 | acc: 54.01\n",
      "epoch: 101/200 | step: 700/927 | trn loss: 1.1752 | val loss: 1.7220 | acc: 53.52\n",
      "epoch: 101/200 | step: 800/927 | trn loss: 1.2338 | val loss: 1.6656 | acc: 54.92\n",
      "epoch: 101/200 | step: 900/927 | trn loss: 1.1322 | val loss: 1.6518 | acc: 54.92\n",
      "epoch: 102/200 | step: 100/927 | trn loss: 1.0935 | val loss: 1.7018 | acc: 54.31\n",
      "epoch: 102/200 | step: 200/927 | trn loss: 1.1373 | val loss: 1.6724 | acc: 54.92\n",
      "epoch: 102/200 | step: 300/927 | trn loss: 1.1048 | val loss: 1.7020 | acc: 54.07\n",
      "epoch: 102/200 | step: 400/927 | trn loss: 1.1127 | val loss: 1.7008 | acc: 55.47\n",
      "epoch: 102/200 | step: 500/927 | trn loss: 1.1307 | val loss: 1.7124 | acc: 53.16\n",
      "epoch: 102/200 | step: 600/927 | trn loss: 1.1263 | val loss: 1.6815 | acc: 54.19\n",
      "epoch: 102/200 | step: 700/927 | trn loss: 1.0375 | val loss: 1.6803 | acc: 54.80\n",
      "epoch: 102/200 | step: 800/927 | trn loss: 1.1762 | val loss: 1.6712 | acc: 56.38\n",
      "epoch: 102/200 | step: 900/927 | trn loss: 1.1223 | val loss: 1.6855 | acc: 55.22\n",
      "epoch: 103/200 | step: 100/927 | trn loss: 1.0457 | val loss: 1.6878 | acc: 54.74\n",
      "epoch: 103/200 | step: 200/927 | trn loss: 1.1134 | val loss: 1.6598 | acc: 54.62\n",
      "epoch: 103/200 | step: 300/927 | trn loss: 1.0731 | val loss: 1.6922 | acc: 55.65\n",
      "epoch: 103/200 | step: 400/927 | trn loss: 1.0289 | val loss: 1.6885 | acc: 54.31\n",
      "epoch: 103/200 | step: 500/927 | trn loss: 1.0084 | val loss: 1.6844 | acc: 54.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 103/200 | step: 600/927 | trn loss: 1.0362 | val loss: 1.6809 | acc: 54.56\n",
      "epoch: 103/200 | step: 700/927 | trn loss: 1.1160 | val loss: 1.6849 | acc: 54.62\n",
      "epoch: 103/200 | step: 800/927 | trn loss: 1.0468 | val loss: 1.7156 | acc: 55.16\n",
      "epoch: 103/200 | step: 900/927 | trn loss: 1.1323 | val loss: 1.6857 | acc: 55.22\n",
      "epoch: 104/200 | step: 100/927 | trn loss: 1.0740 | val loss: 1.6819 | acc: 54.19\n",
      "epoch: 104/200 | step: 200/927 | trn loss: 1.0894 | val loss: 1.6703 | acc: 54.92\n",
      "epoch: 104/200 | step: 300/927 | trn loss: 1.0880 | val loss: 1.6876 | acc: 54.01\n",
      "epoch: 104/200 | step: 400/927 | trn loss: 1.0179 | val loss: 1.6861 | acc: 54.74\n",
      "epoch: 104/200 | step: 500/927 | trn loss: 1.0639 | val loss: 1.6643 | acc: 54.62\n",
      "epoch: 104/200 | step: 600/927 | trn loss: 1.0143 | val loss: 1.6746 | acc: 55.41\n",
      "epoch: 104/200 | step: 700/927 | trn loss: 1.0078 | val loss: 1.6958 | acc: 54.98\n",
      "epoch: 104/200 | step: 800/927 | trn loss: 1.1095 | val loss: 1.6653 | acc: 55.16\n",
      "epoch: 104/200 | step: 900/927 | trn loss: 1.0191 | val loss: 1.6736 | acc: 54.92\n",
      "epoch: 105/200 | step: 100/927 | trn loss: 0.9924 | val loss: 1.6365 | acc: 57.59\n",
      "epoch: 105/200 | step: 200/927 | trn loss: 0.9899 | val loss: 1.6795 | acc: 55.71\n",
      "epoch: 105/200 | step: 300/927 | trn loss: 0.9772 | val loss: 1.6847 | acc: 54.74\n",
      "epoch: 105/200 | step: 400/927 | trn loss: 1.0823 | val loss: 1.6849 | acc: 56.14\n",
      "epoch: 105/200 | step: 500/927 | trn loss: 0.9210 | val loss: 1.6776 | acc: 55.95\n",
      "epoch: 105/200 | step: 600/927 | trn loss: 1.0224 | val loss: 1.6716 | acc: 54.62\n",
      "epoch: 105/200 | step: 700/927 | trn loss: 0.9718 | val loss: 1.6812 | acc: 55.41\n",
      "epoch: 105/200 | step: 800/927 | trn loss: 1.0107 | val loss: 1.6834 | acc: 55.35\n",
      "epoch: 105/200 | step: 900/927 | trn loss: 1.0639 | val loss: 1.6513 | acc: 56.80\n",
      "epoch: 106/200 | step: 100/927 | trn loss: 1.0216 | val loss: 1.6552 | acc: 56.32\n",
      "epoch: 106/200 | step: 200/927 | trn loss: 0.9767 | val loss: 1.6744 | acc: 56.14\n",
      "epoch: 106/200 | step: 300/927 | trn loss: 1.0734 | val loss: 1.6374 | acc: 57.11\n",
      "epoch: 106/200 | step: 400/927 | trn loss: 0.9607 | val loss: 1.6572 | acc: 54.98\n",
      "epoch: 106/200 | step: 500/927 | trn loss: 1.0691 | val loss: 1.6764 | acc: 54.92\n",
      "epoch: 106/200 | step: 600/927 | trn loss: 1.0298 | val loss: 1.6738 | acc: 56.87\n",
      "epoch: 106/200 | step: 700/927 | trn loss: 1.0037 | val loss: 1.6545 | acc: 56.01\n",
      "epoch: 106/200 | step: 800/927 | trn loss: 1.0032 | val loss: 1.6775 | acc: 56.26\n",
      "epoch: 106/200 | step: 900/927 | trn loss: 1.0385 | val loss: 1.6388 | acc: 56.26\n",
      "epoch: 107/200 | step: 100/927 | trn loss: 0.9444 | val loss: 1.6752 | acc: 54.80\n",
      "epoch: 107/200 | step: 200/927 | trn loss: 0.9992 | val loss: 1.6600 | acc: 55.47\n",
      "epoch: 107/200 | step: 300/927 | trn loss: 1.0132 | val loss: 1.6590 | acc: 55.77\n",
      "epoch: 107/200 | step: 400/927 | trn loss: 1.0225 | val loss: 1.6482 | acc: 56.32\n",
      "epoch: 107/200 | step: 500/927 | trn loss: 0.9877 | val loss: 1.6426 | acc: 56.56\n",
      "epoch: 107/200 | step: 600/927 | trn loss: 0.9778 | val loss: 1.6477 | acc: 56.14\n",
      "epoch: 107/200 | step: 700/927 | trn loss: 0.9635 | val loss: 1.6495 | acc: 56.01\n",
      "epoch: 107/200 | step: 800/927 | trn loss: 0.9951 | val loss: 1.6559 | acc: 55.16\n",
      "epoch: 107/200 | step: 900/927 | trn loss: 0.9834 | val loss: 1.6326 | acc: 56.32\n",
      "epoch: 108/200 | step: 100/927 | trn loss: 0.9761 | val loss: 1.6718 | acc: 55.53\n",
      "epoch: 108/200 | step: 200/927 | trn loss: 1.0334 | val loss: 1.6417 | acc: 56.38\n",
      "epoch: 108/200 | step: 300/927 | trn loss: 0.9297 | val loss: 1.6420 | acc: 56.99\n",
      "epoch: 108/200 | step: 400/927 | trn loss: 1.0287 | val loss: 1.6541 | acc: 55.53\n",
      "epoch: 108/200 | step: 600/927 | trn loss: 0.9295 | val loss: 1.6275 | acc: 57.65\n",
      "epoch: 108/200 | step: 700/927 | trn loss: 1.0673 | val loss: 1.6461 | acc: 56.74\n",
      "epoch: 108/200 | step: 800/927 | trn loss: 0.9861 | val loss: 1.6650 | acc: 55.71\n",
      "epoch: 108/200 | step: 900/927 | trn loss: 0.9745 | val loss: 1.6509 | acc: 56.44\n",
      "epoch: 109/200 | step: 100/927 | trn loss: 0.9366 | val loss: 1.6751 | acc: 57.72\n",
      "epoch: 109/200 | step: 200/927 | trn loss: 0.9630 | val loss: 1.6543 | acc: 57.41\n",
      "epoch: 109/200 | step: 300/927 | trn loss: 0.9877 | val loss: 1.6588 | acc: 57.35\n",
      "epoch: 109/200 | step: 400/927 | trn loss: 0.8931 | val loss: 1.6361 | acc: 57.78\n",
      "epoch: 109/200 | step: 500/927 | trn loss: 1.0214 | val loss: 1.6587 | acc: 56.20\n",
      "epoch: 109/200 | step: 600/927 | trn loss: 0.9629 | val loss: 1.6616 | acc: 56.26\n",
      "epoch: 109/200 | step: 700/927 | trn loss: 0.9546 | val loss: 1.6441 | acc: 56.50\n",
      "epoch: 109/200 | step: 800/927 | trn loss: 1.0518 | val loss: 1.6278 | acc: 57.35\n",
      "epoch: 109/200 | step: 900/927 | trn loss: 0.9389 | val loss: 1.6340 | acc: 57.23\n",
      "epoch: 110/200 | step: 100/927 | trn loss: 0.9911 | val loss: 1.6061 | acc: 57.72\n",
      "epoch: 110/200 | step: 200/927 | trn loss: 0.9500 | val loss: 1.6328 | acc: 56.93\n",
      "epoch: 110/200 | step: 300/927 | trn loss: 0.9820 | val loss: 1.6364 | acc: 57.35\n",
      "epoch: 110/200 | step: 400/927 | trn loss: 0.9379 | val loss: 1.6078 | acc: 56.87\n",
      "epoch: 110/200 | step: 500/927 | trn loss: 0.9242 | val loss: 1.6476 | acc: 56.01\n",
      "epoch: 110/200 | step: 600/927 | trn loss: 1.0072 | val loss: 1.6655 | acc: 56.08\n",
      "epoch: 110/200 | step: 700/927 | trn loss: 1.0063 | val loss: 1.6483 | acc: 56.08\n",
      "epoch: 110/200 | step: 800/927 | trn loss: 0.9623 | val loss: 1.6143 | acc: 58.51\n",
      "epoch: 110/200 | step: 900/927 | trn loss: 0.9416 | val loss: 1.6438 | acc: 56.44\n",
      "epoch: 111/200 | step: 100/927 | trn loss: 0.9371 | val loss: 1.6537 | acc: 55.95\n",
      "epoch: 111/200 | step: 200/927 | trn loss: 1.0016 | val loss: 1.6360 | acc: 56.87\n",
      "epoch: 111/200 | step: 300/927 | trn loss: 0.9451 | val loss: 1.6523 | acc: 57.29\n",
      "epoch: 111/200 | step: 400/927 | trn loss: 1.0175 | val loss: 1.6271 | acc: 57.05\n",
      "epoch: 111/200 | step: 500/927 | trn loss: 0.9322 | val loss: 1.6177 | acc: 57.11\n",
      "epoch: 111/200 | step: 600/927 | trn loss: 0.8742 | val loss: 1.6580 | acc: 57.11\n",
      "epoch: 111/200 | step: 700/927 | trn loss: 1.0252 | val loss: 1.6239 | acc: 57.11\n",
      "epoch: 111/200 | step: 800/927 | trn loss: 0.9639 | val loss: 1.6296 | acc: 56.80\n",
      "epoch: 111/200 | step: 900/927 | trn loss: 0.9819 | val loss: 1.6199 | acc: 57.35\n",
      "epoch: 112/200 | step: 100/927 | trn loss: 0.9809 | val loss: 1.6481 | acc: 56.01\n",
      "epoch: 112/200 | step: 200/927 | trn loss: 0.9193 | val loss: 1.6275 | acc: 56.87\n",
      "epoch: 112/200 | step: 300/927 | trn loss: 1.0087 | val loss: 1.6434 | acc: 56.50\n",
      "epoch: 112/200 | step: 400/927 | trn loss: 0.9898 | val loss: 1.6263 | acc: 56.38\n",
      "epoch: 112/200 | step: 500/927 | trn loss: 0.9619 | val loss: 1.6479 | acc: 55.53\n",
      "epoch: 112/200 | step: 600/927 | trn loss: 0.8794 | val loss: 1.6188 | acc: 56.93\n",
      "epoch: 112/200 | step: 700/927 | trn loss: 0.9677 | val loss: 1.6109 | acc: 57.47\n",
      "epoch: 112/200 | step: 800/927 | trn loss: 0.9460 | val loss: 1.6282 | acc: 57.53\n",
      "epoch: 112/200 | step: 900/927 | trn loss: 0.9688 | val loss: 1.6565 | acc: 56.56\n",
      "epoch: 113/200 | step: 100/927 | trn loss: 0.8800 | val loss: 1.6115 | acc: 57.23\n",
      "epoch: 113/200 | step: 200/927 | trn loss: 0.8808 | val loss: 1.6251 | acc: 57.84\n",
      "epoch: 113/200 | step: 300/927 | trn loss: 0.8755 | val loss: 1.6706 | acc: 55.95\n",
      "epoch: 113/200 | step: 400/927 | trn loss: 0.9429 | val loss: 1.6726 | acc: 57.53\n",
      "epoch: 113/200 | step: 500/927 | trn loss: 0.9444 | val loss: 1.6619 | acc: 55.59\n",
      "epoch: 113/200 | step: 600/927 | trn loss: 0.9461 | val loss: 1.6266 | acc: 57.96\n",
      "epoch: 113/200 | step: 700/927 | trn loss: 0.9167 | val loss: 1.6062 | acc: 57.65\n",
      "epoch: 113/200 | step: 800/927 | trn loss: 0.9825 | val loss: 1.6498 | acc: 57.96\n",
      "epoch: 113/200 | step: 900/927 | trn loss: 0.9305 | val loss: 1.6578 | acc: 57.47\n",
      "epoch: 114/200 | step: 100/927 | trn loss: 0.8445 | val loss: 1.6825 | acc: 56.20\n",
      "epoch: 114/200 | step: 200/927 | trn loss: 0.9375 | val loss: 1.6393 | acc: 56.14\n",
      "epoch: 114/200 | step: 300/927 | trn loss: 0.9328 | val loss: 1.6601 | acc: 56.56\n",
      "epoch: 114/200 | step: 400/927 | trn loss: 1.0095 | val loss: 1.6751 | acc: 55.35\n",
      "epoch: 114/200 | step: 500/927 | trn loss: 0.8705 | val loss: 1.6530 | acc: 56.99\n",
      "epoch: 114/200 | step: 600/927 | trn loss: 0.9617 | val loss: 1.6554 | acc: 56.01\n",
      "epoch: 114/200 | step: 700/927 | trn loss: 0.9245 | val loss: 1.6376 | acc: 57.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 114/200 | step: 800/927 | trn loss: 0.8984 | val loss: 1.6284 | acc: 56.44\n",
      "epoch: 114/200 | step: 900/927 | trn loss: 0.8529 | val loss: 1.6280 | acc: 57.05\n",
      "epoch: 115/200 | step: 100/927 | trn loss: 0.8747 | val loss: 1.6678 | acc: 55.29\n",
      "epoch: 115/200 | step: 200/927 | trn loss: 0.8814 | val loss: 1.6471 | acc: 58.20\n",
      "epoch: 115/200 | step: 300/927 | trn loss: 0.9236 | val loss: 1.6432 | acc: 56.93\n",
      "epoch: 115/200 | step: 400/927 | trn loss: 0.9007 | val loss: 1.6728 | acc: 55.77\n",
      "epoch: 115/200 | step: 500/927 | trn loss: 0.9188 | val loss: 1.6660 | acc: 57.53\n",
      "epoch: 115/200 | step: 600/927 | trn loss: 0.9353 | val loss: 1.6755 | acc: 56.80\n",
      "epoch: 115/200 | step: 700/927 | trn loss: 0.9517 | val loss: 1.6530 | acc: 57.35\n",
      "epoch: 115/200 | step: 800/927 | trn loss: 0.9196 | val loss: 1.6790 | acc: 56.62\n",
      "epoch: 115/200 | step: 900/927 | trn loss: 0.9120 | val loss: 1.6389 | acc: 57.72\n",
      "epoch: 116/200 | step: 100/927 | trn loss: 0.8993 | val loss: 1.6373 | acc: 56.08\n",
      "epoch: 116/200 | step: 200/927 | trn loss: 0.9216 | val loss: 1.6428 | acc: 56.26\n",
      "epoch: 116/200 | step: 300/927 | trn loss: 0.9322 | val loss: 1.6483 | acc: 57.17\n",
      "epoch: 116/200 | step: 400/927 | trn loss: 0.8756 | val loss: 1.6368 | acc: 56.44\n",
      "epoch: 116/200 | step: 500/927 | trn loss: 1.0036 | val loss: 1.6197 | acc: 58.08\n",
      "epoch: 116/200 | step: 600/927 | trn loss: 0.9160 | val loss: 1.6558 | acc: 56.80\n",
      "epoch: 116/200 | step: 700/927 | trn loss: 0.8778 | val loss: 1.6250 | acc: 57.17\n",
      "epoch: 116/200 | step: 800/927 | trn loss: 0.9561 | val loss: 1.6294 | acc: 56.44\n",
      "epoch: 116/200 | step: 900/927 | trn loss: 0.8636 | val loss: 1.6206 | acc: 57.11\n",
      "epoch: 117/200 | step: 100/927 | trn loss: 0.9325 | val loss: 1.6336 | acc: 56.01\n",
      "epoch: 117/200 | step: 200/927 | trn loss: 0.8767 | val loss: 1.6234 | acc: 57.35\n",
      "epoch: 117/200 | step: 300/927 | trn loss: 0.9235 | val loss: 1.6553 | acc: 56.87\n",
      "epoch: 117/200 | step: 400/927 | trn loss: 0.8952 | val loss: 1.6219 | acc: 55.95\n",
      "epoch: 117/200 | step: 500/927 | trn loss: 0.8880 | val loss: 1.6260 | acc: 58.26\n",
      "epoch: 117/200 | step: 600/927 | trn loss: 0.8743 | val loss: 1.6389 | acc: 57.05\n",
      "epoch: 117/200 | step: 700/927 | trn loss: 0.8619 | val loss: 1.6612 | acc: 56.87\n",
      "epoch: 117/200 | step: 800/927 | trn loss: 0.9302 | val loss: 1.6369 | acc: 57.41\n",
      "epoch: 117/200 | step: 900/927 | trn loss: 1.0001 | val loss: 1.6208 | acc: 57.29\n",
      "epoch: 118/200 | step: 100/927 | trn loss: 0.8622 | val loss: 1.6290 | acc: 57.35\n",
      "epoch: 118/200 | step: 200/927 | trn loss: 0.8419 | val loss: 1.6387 | acc: 57.11\n",
      "epoch: 118/200 | step: 300/927 | trn loss: 0.8328 | val loss: 1.6370 | acc: 57.11\n",
      "epoch: 118/200 | step: 400/927 | trn loss: 0.8628 | val loss: 1.6392 | acc: 57.84\n",
      "epoch: 118/200 | step: 500/927 | trn loss: 0.9181 | val loss: 1.6332 | acc: 57.35\n",
      "epoch: 118/200 | step: 600/927 | trn loss: 0.8517 | val loss: 1.6453 | acc: 57.11\n",
      "epoch: 118/200 | step: 700/927 | trn loss: 0.9293 | val loss: 1.6356 | acc: 56.99\n",
      "epoch: 118/200 | step: 800/927 | trn loss: 0.8632 | val loss: 1.6597 | acc: 55.83\n",
      "epoch: 118/200 | step: 900/927 | trn loss: 0.9185 | val loss: 1.5921 | acc: 58.20\n",
      "epoch: 119/200 | step: 100/927 | trn loss: 0.8889 | val loss: 1.6230 | acc: 56.32\n",
      "epoch: 119/200 | step: 200/927 | trn loss: 0.8188 | val loss: 1.6310 | acc: 56.26\n",
      "epoch: 119/200 | step: 300/927 | trn loss: 0.8522 | val loss: 1.6441 | acc: 56.87\n",
      "epoch: 119/200 | step: 400/927 | trn loss: 0.8762 | val loss: 1.6320 | acc: 57.53\n",
      "epoch: 119/200 | step: 500/927 | trn loss: 0.8907 | val loss: 1.6132 | acc: 57.11\n",
      "epoch: 119/200 | step: 600/927 | trn loss: 0.8935 | val loss: 1.6440 | acc: 57.23\n",
      "epoch: 119/200 | step: 700/927 | trn loss: 0.8607 | val loss: 1.6454 | acc: 56.87\n",
      "epoch: 119/200 | step: 800/927 | trn loss: 0.8750 | val loss: 1.6376 | acc: 56.74\n",
      "epoch: 119/200 | step: 900/927 | trn loss: 0.8447 | val loss: 1.6243 | acc: 57.17\n",
      "epoch: 120/200 | step: 100/927 | trn loss: 0.8770 | val loss: 1.6108 | acc: 58.14\n",
      "epoch: 120/200 | step: 200/927 | trn loss: 0.8646 | val loss: 1.6567 | acc: 57.05\n",
      "epoch: 120/200 | step: 300/927 | trn loss: 0.9240 | val loss: 1.6346 | acc: 57.78\n",
      "epoch: 120/200 | step: 400/927 | trn loss: 0.9549 | val loss: 1.6476 | acc: 57.65\n",
      "epoch: 120/200 | step: 500/927 | trn loss: 0.9206 | val loss: 1.6360 | acc: 56.74\n",
      "epoch: 120/200 | step: 600/927 | trn loss: 0.8931 | val loss: 1.6302 | acc: 58.26\n",
      "epoch: 120/200 | step: 700/927 | trn loss: 0.9078 | val loss: 1.6441 | acc: 57.47\n",
      "epoch: 120/200 | step: 800/927 | trn loss: 0.9248 | val loss: 1.6534 | acc: 56.87\n",
      "epoch: 120/200 | step: 900/927 | trn loss: 0.8796 | val loss: 1.6375 | acc: 57.47\n",
      "epoch: 121/200 | step: 100/927 | trn loss: 0.9378 | val loss: 1.6276 | acc: 56.87\n",
      "epoch: 121/200 | step: 200/927 | trn loss: 0.7968 | val loss: 1.6405 | acc: 57.05\n",
      "epoch: 121/200 | step: 300/927 | trn loss: 0.8985 | val loss: 1.6197 | acc: 56.62\n",
      "epoch: 121/200 | step: 400/927 | trn loss: 0.8526 | val loss: 1.6270 | acc: 57.59\n",
      "epoch: 121/200 | step: 500/927 | trn loss: 0.9018 | val loss: 1.6576 | acc: 55.89\n",
      "epoch: 121/200 | step: 600/927 | trn loss: 0.8488 | val loss: 1.6392 | acc: 56.62\n",
      "epoch: 121/200 | step: 700/927 | trn loss: 0.8255 | val loss: 1.6225 | acc: 56.99\n",
      "epoch: 121/200 | step: 800/927 | trn loss: 0.9108 | val loss: 1.6499 | acc: 56.80\n",
      "epoch: 121/200 | step: 900/927 | trn loss: 0.8734 | val loss: 1.6497 | acc: 56.74\n",
      "epoch: 122/200 | step: 100/927 | trn loss: 0.9380 | val loss: 1.6339 | acc: 56.08\n",
      "epoch: 122/200 | step: 200/927 | trn loss: 0.9201 | val loss: 1.6132 | acc: 57.72\n",
      "epoch: 122/200 | step: 300/927 | trn loss: 0.8409 | val loss: 1.6795 | acc: 56.08\n",
      "epoch: 122/200 | step: 400/927 | trn loss: 0.7741 | val loss: 1.6288 | acc: 56.68\n",
      "epoch: 122/200 | step: 500/927 | trn loss: 0.8744 | val loss: 1.6131 | acc: 58.38\n",
      "epoch: 122/200 | step: 600/927 | trn loss: 0.8690 | val loss: 1.6265 | acc: 57.41\n",
      "epoch: 122/200 | step: 700/927 | trn loss: 0.9368 | val loss: 1.6607 | acc: 57.47\n",
      "epoch: 122/200 | step: 800/927 | trn loss: 0.8209 | val loss: 1.6422 | acc: 57.29\n",
      "epoch: 122/200 | step: 900/927 | trn loss: 0.8460 | val loss: 1.6640 | acc: 56.87\n",
      "epoch: 123/200 | step: 100/927 | trn loss: 0.8712 | val loss: 1.6519 | acc: 57.05\n",
      "epoch: 123/200 | step: 200/927 | trn loss: 0.8487 | val loss: 1.6739 | acc: 56.56\n",
      "epoch: 123/200 | step: 300/927 | trn loss: 0.8238 | val loss: 1.6465 | acc: 57.23\n",
      "epoch: 123/200 | step: 400/927 | trn loss: 0.8921 | val loss: 1.6296 | acc: 58.02\n",
      "epoch: 123/200 | step: 500/927 | trn loss: 0.8372 | val loss: 1.6481 | acc: 57.17\n",
      "epoch: 123/200 | step: 600/927 | trn loss: 0.8681 | val loss: 1.6403 | acc: 57.17\n",
      "epoch: 123/200 | step: 700/927 | trn loss: 0.8684 | val loss: 1.6605 | acc: 57.72\n",
      "epoch: 123/200 | step: 800/927 | trn loss: 0.8430 | val loss: 1.6619 | acc: 57.23\n",
      "epoch: 123/200 | step: 900/927 | trn loss: 0.8360 | val loss: 1.6325 | acc: 57.11\n",
      "epoch: 124/200 | step: 100/927 | trn loss: 0.8110 | val loss: 1.6474 | acc: 57.11\n",
      "epoch: 124/200 | step: 200/927 | trn loss: 0.8685 | val loss: 1.6529 | acc: 57.05\n",
      "epoch: 124/200 | step: 300/927 | trn loss: 0.8194 | val loss: 1.6257 | acc: 57.72\n",
      "epoch: 124/200 | step: 400/927 | trn loss: 0.9333 | val loss: 1.6289 | acc: 56.87\n",
      "epoch: 124/200 | step: 500/927 | trn loss: 0.9099 | val loss: 1.6282 | acc: 57.41\n",
      "epoch: 124/200 | step: 600/927 | trn loss: 0.8911 | val loss: 1.6373 | acc: 56.93\n",
      "epoch: 124/200 | step: 700/927 | trn loss: 0.8098 | val loss: 1.6552 | acc: 57.65\n",
      "epoch: 124/200 | step: 800/927 | trn loss: 0.7876 | val loss: 1.6616 | acc: 56.68\n",
      "epoch: 124/200 | step: 900/927 | trn loss: 0.8786 | val loss: 1.6356 | acc: 57.05\n",
      "epoch: 125/200 | step: 100/927 | trn loss: 0.8828 | val loss: 1.6440 | acc: 56.50\n",
      "epoch: 125/200 | step: 200/927 | trn loss: 0.8033 | val loss: 1.6596 | acc: 57.53\n",
      "epoch: 125/200 | step: 300/927 | trn loss: 0.8345 | val loss: 1.6376 | acc: 56.38\n",
      "epoch: 125/200 | step: 400/927 | trn loss: 0.9038 | val loss: 1.6406 | acc: 55.47\n",
      "epoch: 125/200 | step: 500/927 | trn loss: 0.8120 | val loss: 1.6468 | acc: 57.65\n",
      "epoch: 125/200 | step: 600/927 | trn loss: 0.8426 | val loss: 1.6337 | acc: 56.50\n",
      "epoch: 125/200 | step: 700/927 | trn loss: 0.8228 | val loss: 1.6629 | acc: 56.32\n",
      "epoch: 125/200 | step: 800/927 | trn loss: 0.8429 | val loss: 1.6397 | acc: 56.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 125/200 | step: 900/927 | trn loss: 0.8282 | val loss: 1.6429 | acc: 57.59\n",
      "epoch: 126/200 | step: 100/927 | trn loss: 0.8427 | val loss: 1.6516 | acc: 56.62\n",
      "epoch: 126/200 | step: 200/927 | trn loss: 0.8467 | val loss: 1.6403 | acc: 58.26\n",
      "epoch: 126/200 | step: 300/927 | trn loss: 0.8006 | val loss: 1.6403 | acc: 57.23\n",
      "epoch: 126/200 | step: 400/927 | trn loss: 0.8786 | val loss: 1.6581 | acc: 56.87\n",
      "epoch: 126/200 | step: 500/927 | trn loss: 0.8144 | val loss: 1.6606 | acc: 57.11\n",
      "epoch: 126/200 | step: 600/927 | trn loss: 0.8608 | val loss: 1.6425 | acc: 57.17\n",
      "epoch: 126/200 | step: 700/927 | trn loss: 0.8742 | val loss: 1.6450 | acc: 57.59\n",
      "epoch: 126/200 | step: 800/927 | trn loss: 0.8190 | val loss: 1.6203 | acc: 58.51\n",
      "epoch: 126/200 | step: 900/927 | trn loss: 0.9148 | val loss: 1.6485 | acc: 57.84\n",
      "epoch: 127/200 | step: 100/927 | trn loss: 0.8035 | val loss: 1.6158 | acc: 58.02\n",
      "epoch: 127/200 | step: 200/927 | trn loss: 0.8204 | val loss: 1.6244 | acc: 57.72\n",
      "epoch: 127/200 | step: 300/927 | trn loss: 0.8268 | val loss: 1.6386 | acc: 57.47\n",
      "epoch: 127/200 | step: 400/927 | trn loss: 0.8404 | val loss: 1.6419 | acc: 57.47\n",
      "epoch: 127/200 | step: 500/927 | trn loss: 0.9081 | val loss: 1.6799 | acc: 56.20\n",
      "epoch: 127/200 | step: 600/927 | trn loss: 0.8551 | val loss: 1.6497 | acc: 57.47\n",
      "epoch: 127/200 | step: 700/927 | trn loss: 0.8091 | val loss: 1.6796 | acc: 57.05\n",
      "epoch: 127/200 | step: 800/927 | trn loss: 0.7564 | val loss: 1.6595 | acc: 56.44\n",
      "epoch: 127/200 | step: 900/927 | trn loss: 0.8610 | val loss: 1.6345 | acc: 56.50\n",
      "epoch: 128/200 | step: 100/927 | trn loss: 0.8325 | val loss: 1.6640 | acc: 56.74\n",
      "epoch: 128/200 | step: 200/927 | trn loss: 0.8742 | val loss: 1.6317 | acc: 57.53\n",
      "epoch: 128/200 | step: 300/927 | trn loss: 0.8477 | val loss: 1.6656 | acc: 57.05\n",
      "epoch: 128/200 | step: 400/927 | trn loss: 0.8142 | val loss: 1.6475 | acc: 57.65\n",
      "epoch: 128/200 | step: 500/927 | trn loss: 0.8161 | val loss: 1.6596 | acc: 57.23\n",
      "epoch: 128/200 | step: 600/927 | trn loss: 0.8099 | val loss: 1.6668 | acc: 56.50\n",
      "epoch: 128/200 | step: 700/927 | trn loss: 0.8747 | val loss: 1.6192 | acc: 57.65\n",
      "epoch: 128/200 | step: 800/927 | trn loss: 0.8836 | val loss: 1.6284 | acc: 58.20\n",
      "epoch: 128/200 | step: 900/927 | trn loss: 0.8547 | val loss: 1.6380 | acc: 58.14\n",
      "epoch: 129/200 | step: 100/927 | trn loss: 0.8199 | val loss: 1.5897 | acc: 57.53\n",
      "epoch: 129/200 | step: 200/927 | trn loss: 0.8086 | val loss: 1.6464 | acc: 57.53\n",
      "epoch: 129/200 | step: 300/927 | trn loss: 0.8162 | val loss: 1.6212 | acc: 57.29\n",
      "epoch: 129/200 | step: 400/927 | trn loss: 0.8904 | val loss: 1.6364 | acc: 57.96\n",
      "epoch: 129/200 | step: 500/927 | trn loss: 0.8192 | val loss: 1.6464 | acc: 58.69\n",
      "epoch: 129/200 | step: 600/927 | trn loss: 0.8320 | val loss: 1.6406 | acc: 56.32\n",
      "epoch: 129/200 | step: 700/927 | trn loss: 0.9118 | val loss: 1.6493 | acc: 56.87\n",
      "epoch: 129/200 | step: 800/927 | trn loss: 0.8387 | val loss: 1.6723 | acc: 57.41\n",
      "epoch: 129/200 | step: 900/927 | trn loss: 0.7925 | val loss: 1.6429 | acc: 57.47\n",
      "epoch: 130/200 | step: 100/927 | trn loss: 0.8142 | val loss: 1.6887 | acc: 56.93\n",
      "epoch: 130/200 | step: 200/927 | trn loss: 0.8624 | val loss: 1.6151 | acc: 57.47\n",
      "epoch: 130/200 | step: 300/927 | trn loss: 0.8665 | val loss: 1.6227 | acc: 56.99\n",
      "epoch: 130/200 | step: 400/927 | trn loss: 0.8596 | val loss: 1.6039 | acc: 58.08\n",
      "epoch: 130/200 | step: 500/927 | trn loss: 0.8141 | val loss: 1.6563 | acc: 56.80\n",
      "epoch: 130/200 | step: 600/927 | trn loss: 0.8245 | val loss: 1.6248 | acc: 58.81\n",
      "epoch: 130/200 | step: 700/927 | trn loss: 0.8539 | val loss: 1.5817 | acc: 58.81\n",
      "epoch: 130/200 | step: 800/927 | trn loss: 0.8693 | val loss: 1.6275 | acc: 57.23\n",
      "epoch: 130/200 | step: 900/927 | trn loss: 0.7904 | val loss: 1.6305 | acc: 58.63\n",
      "epoch: 131/200 | step: 100/927 | trn loss: 0.8296 | val loss: 1.6198 | acc: 58.08\n",
      "epoch: 131/200 | step: 200/927 | trn loss: 0.8525 | val loss: 1.6123 | acc: 57.72\n",
      "epoch: 131/200 | step: 300/927 | trn loss: 0.8591 | val loss: 1.6317 | acc: 56.38\n",
      "epoch: 131/200 | step: 400/927 | trn loss: 0.8271 | val loss: 1.6254 | acc: 58.20\n",
      "epoch: 131/200 | step: 500/927 | trn loss: 0.8241 | val loss: 1.6274 | acc: 57.53\n",
      "epoch: 131/200 | step: 600/927 | trn loss: 0.7993 | val loss: 1.6097 | acc: 58.20\n",
      "epoch: 131/200 | step: 700/927 | trn loss: 0.8231 | val loss: 1.6189 | acc: 56.80\n",
      "epoch: 131/200 | step: 800/927 | trn loss: 0.8708 | val loss: 1.6211 | acc: 58.57\n",
      "epoch: 131/200 | step: 900/927 | trn loss: 0.8704 | val loss: 1.6170 | acc: 57.17\n",
      "epoch: 132/200 | step: 100/927 | trn loss: 0.8428 | val loss: 1.6133 | acc: 56.62\n",
      "epoch: 132/200 | step: 200/927 | trn loss: 0.8026 | val loss: 1.6182 | acc: 58.63\n",
      "epoch: 132/200 | step: 300/927 | trn loss: 0.8580 | val loss: 1.6066 | acc: 57.35\n",
      "epoch: 132/200 | step: 400/927 | trn loss: 0.8239 | val loss: 1.5982 | acc: 58.57\n",
      "epoch: 132/200 | step: 500/927 | trn loss: 0.7943 | val loss: 1.6125 | acc: 56.74\n",
      "epoch: 132/200 | step: 600/927 | trn loss: 0.8038 | val loss: 1.6355 | acc: 57.17\n",
      "epoch: 132/200 | step: 700/927 | trn loss: 0.8148 | val loss: 1.6301 | acc: 56.56\n",
      "epoch: 132/200 | step: 800/927 | trn loss: 0.8170 | val loss: 1.6322 | acc: 57.53\n",
      "epoch: 132/200 | step: 900/927 | trn loss: 0.8279 | val loss: 1.5978 | acc: 58.87\n",
      "epoch: 133/200 | step: 100/927 | trn loss: 0.7698 | val loss: 1.6015 | acc: 58.57\n",
      "epoch: 133/200 | step: 200/927 | trn loss: 0.7931 | val loss: 1.6308 | acc: 57.05\n",
      "epoch: 133/200 | step: 300/927 | trn loss: 0.8880 | val loss: 1.6123 | acc: 58.81\n",
      "epoch: 133/200 | step: 400/927 | trn loss: 0.7952 | val loss: 1.6231 | acc: 56.99\n",
      "epoch: 133/200 | step: 500/927 | trn loss: 0.9042 | val loss: 1.5931 | acc: 58.08\n",
      "epoch: 133/200 | step: 600/927 | trn loss: 0.7928 | val loss: 1.5722 | acc: 57.84\n",
      "epoch: 133/200 | step: 700/927 | trn loss: 0.8357 | val loss: 1.6110 | acc: 58.20\n",
      "epoch: 133/200 | step: 800/927 | trn loss: 0.7805 | val loss: 1.6346 | acc: 56.68\n",
      "epoch: 133/200 | step: 900/927 | trn loss: 0.8635 | val loss: 1.6193 | acc: 57.78\n",
      "epoch: 134/200 | step: 100/927 | trn loss: 0.8137 | val loss: 1.6389 | acc: 58.20\n",
      "epoch: 134/200 | step: 200/927 | trn loss: 0.7243 | val loss: 1.6633 | acc: 57.47\n",
      "epoch: 134/200 | step: 300/927 | trn loss: 0.8106 | val loss: 1.6107 | acc: 58.38\n",
      "epoch: 134/200 | step: 400/927 | trn loss: 0.7496 | val loss: 1.6240 | acc: 57.72\n",
      "epoch: 134/200 | step: 500/927 | trn loss: 0.8348 | val loss: 1.6373 | acc: 57.96\n",
      "epoch: 134/200 | step: 600/927 | trn loss: 0.7966 | val loss: 1.6391 | acc: 57.59\n",
      "epoch: 134/200 | step: 700/927 | trn loss: 0.7957 | val loss: 1.6208 | acc: 56.80\n",
      "epoch: 134/200 | step: 800/927 | trn loss: 0.8449 | val loss: 1.6788 | acc: 56.99\n",
      "epoch: 134/200 | step: 900/927 | trn loss: 0.8230 | val loss: 1.6385 | acc: 58.14\n",
      "epoch: 135/200 | step: 100/927 | trn loss: 0.7723 | val loss: 1.5855 | acc: 58.44\n",
      "epoch: 135/200 | step: 200/927 | trn loss: 0.8028 | val loss: 1.6081 | acc: 58.02\n",
      "epoch: 135/200 | step: 300/927 | trn loss: 0.7530 | val loss: 1.6083 | acc: 57.47\n",
      "epoch: 135/200 | step: 400/927 | trn loss: 0.7761 | val loss: 1.5993 | acc: 58.69\n",
      "epoch: 135/200 | step: 500/927 | trn loss: 0.8475 | val loss: 1.6164 | acc: 57.29\n",
      "epoch: 135/200 | step: 600/927 | trn loss: 0.8626 | val loss: 1.6478 | acc: 58.14\n",
      "epoch: 135/200 | step: 700/927 | trn loss: 0.8297 | val loss: 1.6574 | acc: 56.93\n",
      "epoch: 135/200 | step: 800/927 | trn loss: 0.8060 | val loss: 1.6521 | acc: 57.41\n",
      "epoch: 135/200 | step: 900/927 | trn loss: 0.7841 | val loss: 1.6501 | acc: 57.11\n",
      "epoch: 136/200 | step: 100/927 | trn loss: 0.7739 | val loss: 1.6283 | acc: 57.41\n",
      "epoch: 136/200 | step: 200/927 | trn loss: 0.7332 | val loss: 1.6574 | acc: 57.41\n",
      "epoch: 136/200 | step: 300/927 | trn loss: 0.7630 | val loss: 1.6534 | acc: 57.23\n",
      "epoch: 136/200 | step: 400/927 | trn loss: 0.7881 | val loss: 1.6707 | acc: 55.77\n",
      "epoch: 136/200 | step: 500/927 | trn loss: 0.8051 | val loss: 1.6505 | acc: 56.87\n",
      "epoch: 136/200 | step: 600/927 | trn loss: 0.8340 | val loss: 1.6356 | acc: 57.29\n",
      "epoch: 136/200 | step: 700/927 | trn loss: 0.7971 | val loss: 1.6611 | acc: 56.62\n",
      "epoch: 136/200 | step: 800/927 | trn loss: 0.7866 | val loss: 1.6186 | acc: 58.51\n",
      "epoch: 136/200 | step: 900/927 | trn loss: 0.7664 | val loss: 1.6355 | acc: 57.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 137/200 | step: 100/927 | trn loss: 0.7591 | val loss: 1.6313 | acc: 58.32\n",
      "epoch: 137/200 | step: 200/927 | trn loss: 0.7261 | val loss: 1.6498 | acc: 56.87\n",
      "epoch: 137/200 | step: 300/927 | trn loss: 0.8271 | val loss: 1.6708 | acc: 56.62\n",
      "epoch: 137/200 | step: 400/927 | trn loss: 0.8262 | val loss: 1.6563 | acc: 56.93\n",
      "epoch: 137/200 | step: 500/927 | trn loss: 0.7438 | val loss: 1.6513 | acc: 56.50\n",
      "epoch: 137/200 | step: 600/927 | trn loss: 0.7791 | val loss: 1.6606 | acc: 55.22\n",
      "epoch: 137/200 | step: 700/927 | trn loss: 0.7883 | val loss: 1.6331 | acc: 57.72\n",
      "epoch: 137/200 | step: 800/927 | trn loss: 0.7778 | val loss: 1.6280 | acc: 57.41\n",
      "epoch: 137/200 | step: 900/927 | trn loss: 0.7787 | val loss: 1.6186 | acc: 57.78\n",
      "epoch: 138/200 | step: 100/927 | trn loss: 0.7830 | val loss: 1.6242 | acc: 56.32\n",
      "epoch: 138/200 | step: 200/927 | trn loss: 0.8044 | val loss: 1.6647 | acc: 56.80\n",
      "epoch: 138/200 | step: 300/927 | trn loss: 0.8384 | val loss: 1.6500 | acc: 57.90\n",
      "epoch: 138/200 | step: 400/927 | trn loss: 0.7401 | val loss: 1.6518 | acc: 56.80\n",
      "epoch: 138/200 | step: 500/927 | trn loss: 0.8194 | val loss: 1.6203 | acc: 58.32\n",
      "epoch: 138/200 | step: 600/927 | trn loss: 0.8162 | val loss: 1.6495 | acc: 58.26\n",
      "epoch: 138/200 | step: 700/927 | trn loss: 0.7616 | val loss: 1.6489 | acc: 57.47\n",
      "epoch: 138/200 | step: 800/927 | trn loss: 0.8007 | val loss: 1.6333 | acc: 57.65\n",
      "epoch: 138/200 | step: 900/927 | trn loss: 0.8183 | val loss: 1.6234 | acc: 57.11\n",
      "epoch: 139/200 | step: 100/927 | trn loss: 0.8133 | val loss: 1.6445 | acc: 57.47\n",
      "epoch: 139/200 | step: 200/927 | trn loss: 0.8188 | val loss: 1.6321 | acc: 57.11\n",
      "epoch: 139/200 | step: 300/927 | trn loss: 0.7256 | val loss: 1.6402 | acc: 58.63\n",
      "epoch: 139/200 | step: 400/927 | trn loss: 0.7488 | val loss: 1.6629 | acc: 57.35\n",
      "epoch: 139/200 | step: 500/927 | trn loss: 0.7731 | val loss: 1.6531 | acc: 56.87\n",
      "epoch: 139/200 | step: 600/927 | trn loss: 0.7952 | val loss: 1.6709 | acc: 58.20\n",
      "epoch: 139/200 | step: 700/927 | trn loss: 0.7624 | val loss: 1.6196 | acc: 58.69\n",
      "epoch: 139/200 | step: 800/927 | trn loss: 0.8160 | val loss: 1.6143 | acc: 58.32\n",
      "epoch: 139/200 | step: 900/927 | trn loss: 0.8477 | val loss: 1.6697 | acc: 56.93\n",
      "epoch: 140/200 | step: 100/927 | trn loss: 0.8334 | val loss: 1.6354 | acc: 57.41\n",
      "epoch: 140/200 | step: 200/927 | trn loss: 0.7576 | val loss: 1.6335 | acc: 56.87\n",
      "epoch: 140/200 | step: 300/927 | trn loss: 0.8030 | val loss: 1.6024 | acc: 58.57\n",
      "epoch: 140/200 | step: 400/927 | trn loss: 0.8173 | val loss: 1.6054 | acc: 57.59\n",
      "epoch: 140/200 | step: 500/927 | trn loss: 0.7675 | val loss: 1.6595 | acc: 57.23\n",
      "epoch: 140/200 | step: 600/927 | trn loss: 0.7658 | val loss: 1.6112 | acc: 57.84\n",
      "epoch: 140/200 | step: 700/927 | trn loss: 0.8179 | val loss: 1.6609 | acc: 58.20\n",
      "epoch: 140/200 | step: 800/927 | trn loss: 0.7767 | val loss: 1.6206 | acc: 58.14\n",
      "epoch: 140/200 | step: 900/927 | trn loss: 0.7891 | val loss: 1.6475 | acc: 58.20\n",
      "epoch: 141/200 | step: 100/927 | trn loss: 0.7979 | val loss: 1.6074 | acc: 58.02\n",
      "epoch: 141/200 | step: 200/927 | trn loss: 0.7975 | val loss: 1.6480 | acc: 58.32\n",
      "epoch: 141/200 | step: 300/927 | trn loss: 0.7249 | val loss: 1.6523 | acc: 57.05\n",
      "epoch: 141/200 | step: 400/927 | trn loss: 0.7120 | val loss: 1.6457 | acc: 57.59\n",
      "epoch: 141/200 | step: 500/927 | trn loss: 0.7467 | val loss: 1.6673 | acc: 58.20\n",
      "epoch: 141/200 | step: 600/927 | trn loss: 0.7923 | val loss: 1.6438 | acc: 57.72\n",
      "epoch: 141/200 | step: 700/927 | trn loss: 0.8010 | val loss: 1.6553 | acc: 57.47\n",
      "epoch: 141/200 | step: 800/927 | trn loss: 0.8233 | val loss: 1.6498 | acc: 57.53\n",
      "epoch: 141/200 | step: 900/927 | trn loss: 0.8089 | val loss: 1.6509 | acc: 57.05\n",
      "epoch: 142/200 | step: 100/927 | trn loss: 0.8057 | val loss: 1.6340 | acc: 57.78\n",
      "epoch: 142/200 | step: 200/927 | trn loss: 0.7172 | val loss: 1.6399 | acc: 56.56\n",
      "epoch: 142/200 | step: 300/927 | trn loss: 0.6764 | val loss: 1.6920 | acc: 56.68\n",
      "epoch: 142/200 | step: 400/927 | trn loss: 0.7504 | val loss: 1.6934 | acc: 55.95\n",
      "epoch: 142/200 | step: 500/927 | trn loss: 0.7704 | val loss: 1.6642 | acc: 58.20\n",
      "epoch: 142/200 | step: 600/927 | trn loss: 0.8221 | val loss: 1.6331 | acc: 56.99\n",
      "epoch: 142/200 | step: 700/927 | trn loss: 0.7668 | val loss: 1.6686 | acc: 56.74\n",
      "epoch: 142/200 | step: 800/927 | trn loss: 0.7534 | val loss: 1.6656 | acc: 58.44\n",
      "epoch: 142/200 | step: 900/927 | trn loss: 0.7792 | val loss: 1.6364 | acc: 58.26\n",
      "epoch: 143/200 | step: 100/927 | trn loss: 0.7456 | val loss: 1.6457 | acc: 58.08\n",
      "epoch: 143/200 | step: 200/927 | trn loss: 0.7899 | val loss: 1.6567 | acc: 57.05\n",
      "epoch: 143/200 | step: 300/927 | trn loss: 0.7676 | val loss: 1.6429 | acc: 58.51\n",
      "epoch: 143/200 | step: 400/927 | trn loss: 0.7103 | val loss: 1.6477 | acc: 58.26\n",
      "epoch: 143/200 | step: 500/927 | trn loss: 0.7024 | val loss: 1.6628 | acc: 57.65\n",
      "epoch: 143/200 | step: 600/927 | trn loss: 0.7185 | val loss: 1.6301 | acc: 57.53\n",
      "epoch: 143/200 | step: 700/927 | trn loss: 0.7854 | val loss: 1.6511 | acc: 57.41\n",
      "epoch: 143/200 | step: 800/927 | trn loss: 0.8032 | val loss: 1.6464 | acc: 58.69\n",
      "epoch: 143/200 | step: 900/927 | trn loss: 0.8182 | val loss: 1.6167 | acc: 58.51\n",
      "epoch: 144/200 | step: 100/927 | trn loss: 0.7609 | val loss: 1.6479 | acc: 56.68\n",
      "epoch: 144/200 | step: 200/927 | trn loss: 0.7511 | val loss: 1.6063 | acc: 58.69\n",
      "epoch: 144/200 | step: 300/927 | trn loss: 0.7930 | val loss: 1.6479 | acc: 57.29\n",
      "epoch: 144/200 | step: 400/927 | trn loss: 0.7754 | val loss: 1.6122 | acc: 58.38\n",
      "epoch: 144/200 | step: 500/927 | trn loss: 0.7312 | val loss: 1.6371 | acc: 57.96\n",
      "epoch: 144/200 | step: 600/927 | trn loss: 0.7536 | val loss: 1.6390 | acc: 57.84\n",
      "epoch: 144/200 | step: 700/927 | trn loss: 0.7461 | val loss: 1.6613 | acc: 57.35\n",
      "epoch: 144/200 | step: 800/927 | trn loss: 0.7698 | val loss: 1.6071 | acc: 58.08\n",
      "epoch: 144/200 | step: 900/927 | trn loss: 0.8441 | val loss: 1.5976 | acc: 58.63\n",
      "epoch: 145/200 | step: 100/927 | trn loss: 0.7854 | val loss: 1.6064 | acc: 57.78\n",
      "epoch: 145/200 | step: 200/927 | trn loss: 0.7279 | val loss: 1.6177 | acc: 58.99\n",
      "epoch: 145/200 | step: 300/927 | trn loss: 0.7134 | val loss: 1.6400 | acc: 57.65\n",
      "epoch: 145/200 | step: 400/927 | trn loss: 0.8025 | val loss: 1.6664 | acc: 56.99\n",
      "epoch: 145/200 | step: 500/927 | trn loss: 0.8168 | val loss: 1.6384 | acc: 57.84\n",
      "epoch: 145/200 | step: 600/927 | trn loss: 0.7772 | val loss: 1.6408 | acc: 57.84\n",
      "epoch: 145/200 | step: 700/927 | trn loss: 0.7829 | val loss: 1.6712 | acc: 57.84\n",
      "epoch: 145/200 | step: 800/927 | trn loss: 0.8066 | val loss: 1.6380 | acc: 57.05\n",
      "epoch: 145/200 | step: 900/927 | trn loss: 0.7397 | val loss: 1.6541 | acc: 57.41\n",
      "epoch: 146/200 | step: 100/927 | trn loss: 0.7695 | val loss: 1.6399 | acc: 57.11\n",
      "epoch: 146/200 | step: 200/927 | trn loss: 0.7693 | val loss: 1.6035 | acc: 58.08\n",
      "epoch: 146/200 | step: 300/927 | trn loss: 0.7540 | val loss: 1.6169 | acc: 57.72\n",
      "epoch: 146/200 | step: 400/927 | trn loss: 0.7212 | val loss: 1.6196 | acc: 58.14\n",
      "epoch: 146/200 | step: 500/927 | trn loss: 0.7449 | val loss: 1.6375 | acc: 57.29\n",
      "epoch: 146/200 | step: 600/927 | trn loss: 0.7597 | val loss: 1.6687 | acc: 56.80\n",
      "epoch: 146/200 | step: 700/927 | trn loss: 0.7913 | val loss: 1.6173 | acc: 57.35\n",
      "epoch: 146/200 | step: 800/927 | trn loss: 0.7331 | val loss: 1.6322 | acc: 57.41\n",
      "epoch: 146/200 | step: 900/927 | trn loss: 0.7748 | val loss: 1.6144 | acc: 58.02\n",
      "epoch: 147/200 | step: 100/927 | trn loss: 0.7034 | val loss: 1.6281 | acc: 56.93\n",
      "epoch: 147/200 | step: 200/927 | trn loss: 0.6470 | val loss: 1.6477 | acc: 56.93\n",
      "epoch: 147/200 | step: 300/927 | trn loss: 0.8238 | val loss: 1.6176 | acc: 58.26\n",
      "epoch: 147/200 | step: 400/927 | trn loss: 0.6998 | val loss: 1.6190 | acc: 57.90\n",
      "epoch: 147/200 | step: 500/927 | trn loss: 0.7609 | val loss: 1.6425 | acc: 58.51\n",
      "epoch: 147/200 | step: 600/927 | trn loss: 0.7229 | val loss: 1.6052 | acc: 58.14\n",
      "epoch: 147/200 | step: 700/927 | trn loss: 0.7445 | val loss: 1.6399 | acc: 58.20\n",
      "epoch: 147/200 | step: 800/927 | trn loss: 0.8017 | val loss: 1.6534 | acc: 57.78\n",
      "epoch: 147/200 | step: 900/927 | trn loss: 0.7942 | val loss: 1.6434 | acc: 57.41\n",
      "epoch: 148/200 | step: 100/927 | trn loss: 0.7140 | val loss: 1.6446 | acc: 56.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 148/200 | step: 200/927 | trn loss: 0.7365 | val loss: 1.7089 | acc: 56.80\n",
      "epoch: 148/200 | step: 300/927 | trn loss: 0.8061 | val loss: 1.6549 | acc: 57.65\n",
      "epoch: 148/200 | step: 400/927 | trn loss: 0.7510 | val loss: 1.6465 | acc: 58.14\n",
      "epoch: 148/200 | step: 500/927 | trn loss: 0.7412 | val loss: 1.6164 | acc: 58.14\n",
      "epoch: 148/200 | step: 600/927 | trn loss: 0.6741 | val loss: 1.6548 | acc: 57.41\n",
      "epoch: 148/200 | step: 700/927 | trn loss: 0.7417 | val loss: 1.6446 | acc: 57.59\n",
      "epoch: 148/200 | step: 800/927 | trn loss: 0.7404 | val loss: 1.6592 | acc: 56.56\n",
      "epoch: 148/200 | step: 900/927 | trn loss: 0.7410 | val loss: 1.5931 | acc: 58.51\n",
      "epoch: 149/200 | step: 100/927 | trn loss: 0.7906 | val loss: 1.6501 | acc: 56.80\n",
      "epoch: 149/200 | step: 200/927 | trn loss: 0.7596 | val loss: 1.6570 | acc: 57.23\n",
      "epoch: 149/200 | step: 300/927 | trn loss: 0.7766 | val loss: 1.6660 | acc: 57.65\n",
      "epoch: 149/200 | step: 400/927 | trn loss: 0.7122 | val loss: 1.6959 | acc: 56.62\n",
      "epoch: 149/200 | step: 500/927 | trn loss: 0.7179 | val loss: 1.6719 | acc: 56.62\n",
      "epoch: 149/200 | step: 600/927 | trn loss: 0.7642 | val loss: 1.6333 | acc: 57.47\n",
      "epoch: 149/200 | step: 700/927 | trn loss: 0.7561 | val loss: 1.6417 | acc: 58.14\n",
      "epoch: 149/200 | step: 800/927 | trn loss: 0.7292 | val loss: 1.6477 | acc: 57.47\n",
      "epoch: 149/200 | step: 900/927 | trn loss: 0.7872 | val loss: 1.6460 | acc: 57.29\n",
      "epoch: 150/200 | step: 100/927 | trn loss: 0.8086 | val loss: 1.6961 | acc: 56.08\n",
      "epoch: 150/200 | step: 200/927 | trn loss: 0.6902 | val loss: 1.6751 | acc: 56.87\n",
      "epoch: 150/200 | step: 300/927 | trn loss: 0.7204 | val loss: 1.7058 | acc: 56.56\n",
      "epoch: 150/200 | step: 400/927 | trn loss: 0.7756 | val loss: 1.6583 | acc: 56.93\n",
      "epoch: 150/200 | step: 500/927 | trn loss: 0.7219 | val loss: 1.6575 | acc: 57.65\n",
      "epoch: 150/200 | step: 600/927 | trn loss: 0.7948 | val loss: 1.6736 | acc: 57.84\n",
      "epoch: 150/200 | step: 700/927 | trn loss: 0.7616 | val loss: 1.6515 | acc: 55.77\n",
      "epoch: 150/200 | step: 800/927 | trn loss: 0.8060 | val loss: 1.6623 | acc: 57.59\n",
      "epoch: 150/200 | step: 900/927 | trn loss: 0.7343 | val loss: 1.6374 | acc: 58.08\n",
      "epoch: 151/200 | step: 100/927 | trn loss: 0.7327 | val loss: 1.6426 | acc: 57.23\n",
      "epoch: 151/200 | step: 200/927 | trn loss: 0.6950 | val loss: 1.6339 | acc: 58.51\n",
      "epoch: 151/200 | step: 300/927 | trn loss: 0.7036 | val loss: 1.6301 | acc: 57.96\n",
      "epoch: 151/200 | step: 400/927 | trn loss: 0.7252 | val loss: 1.6294 | acc: 57.53\n",
      "epoch: 151/200 | step: 500/927 | trn loss: 0.6512 | val loss: 1.6020 | acc: 57.53\n",
      "epoch: 151/200 | step: 600/927 | trn loss: 0.7177 | val loss: 1.6271 | acc: 57.35\n",
      "epoch: 151/200 | step: 700/927 | trn loss: 0.7013 | val loss: 1.6574 | acc: 57.23\n",
      "epoch: 151/200 | step: 800/927 | trn loss: 0.6875 | val loss: 1.6149 | acc: 57.72\n",
      "epoch: 151/200 | step: 900/927 | trn loss: 0.7054 | val loss: 1.5918 | acc: 58.08\n",
      "epoch: 152/200 | step: 100/927 | trn loss: 0.7119 | val loss: 1.6219 | acc: 57.41\n",
      "epoch: 152/200 | step: 200/927 | trn loss: 0.6989 | val loss: 1.6181 | acc: 56.99\n",
      "epoch: 152/200 | step: 300/927 | trn loss: 0.6583 | val loss: 1.6509 | acc: 58.26\n",
      "epoch: 152/200 | step: 400/927 | trn loss: 0.6941 | val loss: 1.6177 | acc: 56.93\n",
      "epoch: 152/200 | step: 500/927 | trn loss: 0.7084 | val loss: 1.6245 | acc: 57.72\n",
      "epoch: 152/200 | step: 600/927 | trn loss: 0.6856 | val loss: 1.6161 | acc: 58.99\n",
      "epoch: 152/200 | step: 700/927 | trn loss: 0.6788 | val loss: 1.6113 | acc: 57.90\n",
      "epoch: 152/200 | step: 800/927 | trn loss: 0.6175 | val loss: 1.6204 | acc: 58.20\n",
      "epoch: 152/200 | step: 900/927 | trn loss: 0.7357 | val loss: 1.6062 | acc: 57.65\n",
      "epoch: 153/200 | step: 100/927 | trn loss: 0.7668 | val loss: 1.6022 | acc: 59.84\n",
      "epoch: 153/200 | step: 200/927 | trn loss: 0.6834 | val loss: 1.6433 | acc: 58.20\n",
      "epoch: 153/200 | step: 300/927 | trn loss: 0.6670 | val loss: 1.5886 | acc: 58.63\n",
      "epoch: 153/200 | step: 400/927 | trn loss: 0.6691 | val loss: 1.6239 | acc: 57.41\n",
      "epoch: 153/200 | step: 500/927 | trn loss: 0.7465 | val loss: 1.6379 | acc: 57.65\n",
      "epoch: 153/200 | step: 600/927 | trn loss: 0.6769 | val loss: 1.5915 | acc: 57.53\n",
      "epoch: 153/200 | step: 700/927 | trn loss: 0.6896 | val loss: 1.6050 | acc: 58.02\n",
      "epoch: 153/200 | step: 800/927 | trn loss: 0.6387 | val loss: 1.6205 | acc: 58.32\n",
      "epoch: 153/200 | step: 900/927 | trn loss: 0.7359 | val loss: 1.6208 | acc: 57.96\n",
      "epoch: 154/200 | step: 100/927 | trn loss: 0.7016 | val loss: 1.6173 | acc: 57.84\n",
      "epoch: 154/200 | step: 200/927 | trn loss: 0.6253 | val loss: 1.6194 | acc: 57.78\n",
      "epoch: 154/200 | step: 300/927 | trn loss: 0.6841 | val loss: 1.5941 | acc: 59.36\n",
      "epoch: 154/200 | step: 400/927 | trn loss: 0.7425 | val loss: 1.6079 | acc: 58.69\n",
      "epoch: 154/200 | step: 500/927 | trn loss: 0.6872 | val loss: 1.6220 | acc: 58.93\n",
      "epoch: 154/200 | step: 600/927 | trn loss: 0.7084 | val loss: 1.6029 | acc: 57.29\n",
      "epoch: 154/200 | step: 700/927 | trn loss: 0.6710 | val loss: 1.6217 | acc: 58.08\n",
      "epoch: 154/200 | step: 800/927 | trn loss: 0.7105 | val loss: 1.6280 | acc: 58.57\n",
      "epoch: 154/200 | step: 900/927 | trn loss: 0.6876 | val loss: 1.6363 | acc: 57.17\n",
      "epoch: 155/200 | step: 100/927 | trn loss: 0.6748 | val loss: 1.5979 | acc: 58.81\n",
      "epoch: 155/200 | step: 200/927 | trn loss: 0.6854 | val loss: 1.6047 | acc: 58.57\n",
      "epoch: 155/200 | step: 300/927 | trn loss: 0.7570 | val loss: 1.6265 | acc: 58.26\n",
      "epoch: 155/200 | step: 400/927 | trn loss: 0.6884 | val loss: 1.5867 | acc: 58.02\n",
      "epoch: 155/200 | step: 500/927 | trn loss: 0.6848 | val loss: 1.6090 | acc: 58.63\n",
      "epoch: 155/200 | step: 600/927 | trn loss: 0.6752 | val loss: 1.6056 | acc: 58.99\n",
      "epoch: 155/200 | step: 700/927 | trn loss: 0.7088 | val loss: 1.5843 | acc: 59.30\n",
      "epoch: 155/200 | step: 800/927 | trn loss: 0.6726 | val loss: 1.6277 | acc: 58.81\n",
      "epoch: 155/200 | step: 900/927 | trn loss: 0.6658 | val loss: 1.6434 | acc: 56.93\n",
      "epoch: 156/200 | step: 100/927 | trn loss: 0.6903 | val loss: 1.6200 | acc: 58.93\n",
      "epoch: 156/200 | step: 200/927 | trn loss: 0.6903 | val loss: 1.6403 | acc: 58.51\n",
      "epoch: 156/200 | step: 300/927 | trn loss: 0.6842 | val loss: 1.6153 | acc: 58.63\n",
      "epoch: 156/200 | step: 400/927 | trn loss: 0.7308 | val loss: 1.6316 | acc: 58.38\n",
      "epoch: 156/200 | step: 500/927 | trn loss: 0.7151 | val loss: 1.5903 | acc: 58.75\n",
      "epoch: 156/200 | step: 600/927 | trn loss: 0.6621 | val loss: 1.6221 | acc: 57.65\n",
      "epoch: 156/200 | step: 700/927 | trn loss: 0.6765 | val loss: 1.6063 | acc: 57.65\n",
      "epoch: 156/200 | step: 800/927 | trn loss: 0.6616 | val loss: 1.6042 | acc: 57.84\n",
      "epoch: 156/200 | step: 900/927 | trn loss: 0.6959 | val loss: 1.5985 | acc: 57.65\n",
      "epoch: 157/200 | step: 100/927 | trn loss: 0.6441 | val loss: 1.6148 | acc: 58.14\n",
      "epoch: 157/200 | step: 200/927 | trn loss: 0.6936 | val loss: 1.6106 | acc: 59.42\n",
      "epoch: 157/200 | step: 300/927 | trn loss: 0.6360 | val loss: 1.5943 | acc: 58.57\n",
      "epoch: 157/200 | step: 400/927 | trn loss: 0.6359 | val loss: 1.5873 | acc: 58.20\n",
      "epoch: 157/200 | step: 500/927 | trn loss: 0.6676 | val loss: 1.6033 | acc: 58.20\n",
      "epoch: 157/200 | step: 600/927 | trn loss: 0.6633 | val loss: 1.6238 | acc: 57.65\n",
      "epoch: 157/200 | step: 700/927 | trn loss: 0.6533 | val loss: 1.6434 | acc: 58.51\n",
      "epoch: 157/200 | step: 800/927 | trn loss: 0.7169 | val loss: 1.6140 | acc: 58.26\n",
      "epoch: 157/200 | step: 900/927 | trn loss: 0.7043 | val loss: 1.6259 | acc: 57.96\n",
      "epoch: 158/200 | step: 100/927 | trn loss: 0.6481 | val loss: 1.5903 | acc: 58.44\n",
      "epoch: 158/200 | step: 200/927 | trn loss: 0.6695 | val loss: 1.6009 | acc: 58.32\n",
      "epoch: 158/200 | step: 300/927 | trn loss: 0.6521 | val loss: 1.6206 | acc: 57.23\n",
      "epoch: 158/200 | step: 400/927 | trn loss: 0.6186 | val loss: 1.6177 | acc: 57.72\n",
      "epoch: 158/200 | step: 500/927 | trn loss: 0.6812 | val loss: 1.6085 | acc: 57.84\n",
      "epoch: 158/200 | step: 600/927 | trn loss: 0.6825 | val loss: 1.6186 | acc: 58.08\n",
      "epoch: 158/200 | step: 700/927 | trn loss: 0.6224 | val loss: 1.6151 | acc: 57.72\n",
      "epoch: 158/200 | step: 800/927 | trn loss: 0.6709 | val loss: 1.6371 | acc: 57.41\n",
      "epoch: 158/200 | step: 900/927 | trn loss: 0.6693 | val loss: 1.6324 | acc: 57.84\n",
      "epoch: 159/200 | step: 100/927 | trn loss: 0.6497 | val loss: 1.6363 | acc: 58.81\n",
      "epoch: 159/200 | step: 200/927 | trn loss: 0.6860 | val loss: 1.6035 | acc: 57.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 159/200 | step: 300/927 | trn loss: 0.6546 | val loss: 1.6104 | acc: 57.17\n",
      "epoch: 159/200 | step: 400/927 | trn loss: 0.6861 | val loss: 1.6326 | acc: 57.96\n",
      "epoch: 159/200 | step: 500/927 | trn loss: 0.6572 | val loss: 1.6147 | acc: 57.96\n",
      "epoch: 159/200 | step: 600/927 | trn loss: 0.6989 | val loss: 1.5948 | acc: 59.30\n",
      "epoch: 159/200 | step: 700/927 | trn loss: 0.6870 | val loss: 1.6411 | acc: 58.26\n",
      "epoch: 159/200 | step: 800/927 | trn loss: 0.6880 | val loss: 1.6052 | acc: 58.20\n",
      "epoch: 159/200 | step: 900/927 | trn loss: 0.6512 | val loss: 1.6460 | acc: 56.32\n",
      "epoch: 160/200 | step: 100/927 | trn loss: 0.6881 | val loss: 1.6230 | acc: 58.69\n",
      "epoch: 160/200 | step: 200/927 | trn loss: 0.6408 | val loss: 1.5926 | acc: 57.65\n",
      "epoch: 160/200 | step: 300/927 | trn loss: 0.6836 | val loss: 1.5977 | acc: 59.05\n",
      "epoch: 160/200 | step: 400/927 | trn loss: 0.6486 | val loss: 1.6150 | acc: 58.26\n",
      "epoch: 160/200 | step: 500/927 | trn loss: 0.6151 | val loss: 1.6304 | acc: 57.72\n",
      "epoch: 160/200 | step: 600/927 | trn loss: 0.6258 | val loss: 1.5922 | acc: 58.99\n",
      "epoch: 160/200 | step: 700/927 | trn loss: 0.6495 | val loss: 1.5942 | acc: 58.14\n",
      "epoch: 160/200 | step: 800/927 | trn loss: 0.7146 | val loss: 1.6146 | acc: 58.08\n",
      "epoch: 160/200 | step: 900/927 | trn loss: 0.6672 | val loss: 1.6379 | acc: 57.96\n",
      "epoch: 161/200 | step: 100/927 | trn loss: 0.6960 | val loss: 1.6548 | acc: 58.57\n",
      "epoch: 161/200 | step: 200/927 | trn loss: 0.6790 | val loss: 1.5917 | acc: 58.20\n",
      "epoch: 161/200 | step: 300/927 | trn loss: 0.7021 | val loss: 1.6179 | acc: 57.90\n",
      "epoch: 161/200 | step: 400/927 | trn loss: 0.7013 | val loss: 1.5997 | acc: 58.57\n",
      "epoch: 161/200 | step: 500/927 | trn loss: 0.6537 | val loss: 1.6138 | acc: 57.35\n",
      "epoch: 161/200 | step: 600/927 | trn loss: 0.6880 | val loss: 1.6011 | acc: 58.02\n",
      "epoch: 161/200 | step: 700/927 | trn loss: 0.6952 | val loss: 1.6138 | acc: 58.32\n",
      "epoch: 161/200 | step: 800/927 | trn loss: 0.7006 | val loss: 1.6324 | acc: 57.72\n",
      "epoch: 161/200 | step: 900/927 | trn loss: 0.6508 | val loss: 1.5975 | acc: 57.90\n",
      "epoch: 162/200 | step: 100/927 | trn loss: 0.7177 | val loss: 1.6527 | acc: 58.08\n",
      "epoch: 162/200 | step: 200/927 | trn loss: 0.6338 | val loss: 1.6270 | acc: 57.35\n",
      "epoch: 162/200 | step: 300/927 | trn loss: 0.6774 | val loss: 1.5939 | acc: 57.78\n",
      "epoch: 162/200 | step: 400/927 | trn loss: 0.7347 | val loss: 1.6096 | acc: 57.11\n",
      "epoch: 162/200 | step: 500/927 | trn loss: 0.6582 | val loss: 1.6292 | acc: 59.48\n",
      "epoch: 162/200 | step: 600/927 | trn loss: 0.6612 | val loss: 1.5950 | acc: 58.93\n",
      "epoch: 162/200 | step: 700/927 | trn loss: 0.6518 | val loss: 1.6388 | acc: 57.90\n",
      "epoch: 162/200 | step: 800/927 | trn loss: 0.6995 | val loss: 1.6092 | acc: 56.68\n",
      "epoch: 162/200 | step: 900/927 | trn loss: 0.6844 | val loss: 1.6064 | acc: 58.08\n",
      "epoch: 163/200 | step: 100/927 | trn loss: 0.6396 | val loss: 1.5994 | acc: 58.38\n",
      "epoch: 163/200 | step: 200/927 | trn loss: 0.6963 | val loss: 1.6104 | acc: 57.84\n",
      "epoch: 163/200 | step: 300/927 | trn loss: 0.7119 | val loss: 1.6084 | acc: 58.51\n",
      "epoch: 163/200 | step: 400/927 | trn loss: 0.6585 | val loss: 1.6129 | acc: 58.14\n",
      "epoch: 163/200 | step: 500/927 | trn loss: 0.7068 | val loss: 1.5976 | acc: 59.11\n",
      "epoch: 163/200 | step: 600/927 | trn loss: 0.6677 | val loss: 1.5873 | acc: 58.87\n",
      "epoch: 163/200 | step: 700/927 | trn loss: 0.7069 | val loss: 1.6238 | acc: 57.65\n",
      "epoch: 163/200 | step: 800/927 | trn loss: 0.6506 | val loss: 1.6460 | acc: 57.72\n",
      "epoch: 163/200 | step: 900/927 | trn loss: 0.6586 | val loss: 1.5664 | acc: 58.44\n",
      "epoch: 164/200 | step: 100/927 | trn loss: 0.6161 | val loss: 1.6072 | acc: 58.14\n",
      "epoch: 164/200 | step: 200/927 | trn loss: 0.7071 | val loss: 1.6345 | acc: 58.14\n",
      "epoch: 164/200 | step: 300/927 | trn loss: 0.7037 | val loss: 1.5818 | acc: 58.44\n",
      "epoch: 164/200 | step: 400/927 | trn loss: 0.7265 | val loss: 1.6164 | acc: 58.63\n",
      "epoch: 164/200 | step: 500/927 | trn loss: 0.6603 | val loss: 1.6138 | acc: 57.47\n",
      "epoch: 164/200 | step: 600/927 | trn loss: 0.6996 | val loss: 1.6118 | acc: 58.32\n",
      "epoch: 164/200 | step: 700/927 | trn loss: 0.6559 | val loss: 1.6154 | acc: 58.57\n",
      "epoch: 164/200 | step: 800/927 | trn loss: 0.6528 | val loss: 1.6334 | acc: 56.93\n",
      "epoch: 164/200 | step: 900/927 | trn loss: 0.7320 | val loss: 1.6315 | acc: 57.29\n",
      "epoch: 165/200 | step: 100/927 | trn loss: 0.7293 | val loss: 1.6142 | acc: 57.11\n",
      "epoch: 165/200 | step: 200/927 | trn loss: 0.6965 | val loss: 1.6068 | acc: 57.96\n",
      "epoch: 165/200 | step: 300/927 | trn loss: 0.6872 | val loss: 1.6170 | acc: 57.47\n",
      "epoch: 165/200 | step: 400/927 | trn loss: 0.6791 | val loss: 1.6189 | acc: 57.84\n",
      "epoch: 165/200 | step: 500/927 | trn loss: 0.6674 | val loss: 1.6409 | acc: 57.96\n",
      "epoch: 165/200 | step: 600/927 | trn loss: 0.6747 | val loss: 1.6329 | acc: 57.47\n",
      "epoch: 165/200 | step: 700/927 | trn loss: 0.6237 | val loss: 1.6029 | acc: 58.14\n",
      "epoch: 165/200 | step: 800/927 | trn loss: 0.6746 | val loss: 1.5907 | acc: 57.72\n",
      "epoch: 165/200 | step: 900/927 | trn loss: 0.6313 | val loss: 1.5881 | acc: 58.99\n",
      "epoch: 166/200 | step: 200/927 | trn loss: 0.6794 | val loss: 1.6071 | acc: 58.14\n",
      "epoch: 166/200 | step: 300/927 | trn loss: 0.6523 | val loss: 1.6202 | acc: 57.41\n",
      "epoch: 166/200 | step: 400/927 | trn loss: 0.6955 | val loss: 1.6014 | acc: 58.44\n",
      "epoch: 166/200 | step: 500/927 | trn loss: 0.6245 | val loss: 1.5854 | acc: 58.87\n",
      "epoch: 166/200 | step: 600/927 | trn loss: 0.6607 | val loss: 1.6104 | acc: 58.20\n",
      "epoch: 166/200 | step: 700/927 | trn loss: 0.6585 | val loss: 1.6200 | acc: 57.78\n",
      "epoch: 166/200 | step: 800/927 | trn loss: 0.6732 | val loss: 1.5921 | acc: 60.21\n",
      "epoch: 166/200 | step: 900/927 | trn loss: 0.6453 | val loss: 1.6200 | acc: 57.05\n",
      "epoch: 167/200 | step: 100/927 | trn loss: 0.6655 | val loss: 1.6104 | acc: 58.32\n",
      "epoch: 167/200 | step: 200/927 | trn loss: 0.6520 | val loss: 1.6225 | acc: 57.84\n",
      "epoch: 167/200 | step: 300/927 | trn loss: 0.6795 | val loss: 1.6276 | acc: 58.32\n",
      "epoch: 167/200 | step: 400/927 | trn loss: 0.6568 | val loss: 1.6187 | acc: 58.75\n",
      "epoch: 167/200 | step: 500/927 | trn loss: 0.6531 | val loss: 1.6381 | acc: 58.44\n",
      "epoch: 167/200 | step: 600/927 | trn loss: 0.6634 | val loss: 1.6197 | acc: 58.81\n",
      "epoch: 167/200 | step: 700/927 | trn loss: 0.6762 | val loss: 1.5909 | acc: 58.38\n",
      "epoch: 167/200 | step: 800/927 | trn loss: 0.7178 | val loss: 1.5962 | acc: 59.60\n",
      "epoch: 167/200 | step: 900/927 | trn loss: 0.6541 | val loss: 1.5649 | acc: 59.48\n",
      "epoch: 168/200 | step: 100/927 | trn loss: 0.6559 | val loss: 1.5985 | acc: 58.69\n",
      "epoch: 168/200 | step: 200/927 | trn loss: 0.7233 | val loss: 1.6196 | acc: 59.11\n",
      "epoch: 168/200 | step: 300/927 | trn loss: 0.6941 | val loss: 1.5886 | acc: 56.99\n",
      "epoch: 168/200 | step: 400/927 | trn loss: 0.7027 | val loss: 1.6050 | acc: 58.93\n",
      "epoch: 168/200 | step: 500/927 | trn loss: 0.6408 | val loss: 1.6059 | acc: 57.96\n",
      "epoch: 168/200 | step: 600/927 | trn loss: 0.6945 | val loss: 1.6076 | acc: 57.65\n",
      "epoch: 168/200 | step: 700/927 | trn loss: 0.6340 | val loss: 1.6141 | acc: 58.69\n",
      "epoch: 168/200 | step: 800/927 | trn loss: 0.7041 | val loss: 1.6174 | acc: 57.53\n",
      "epoch: 168/200 | step: 900/927 | trn loss: 0.6054 | val loss: 1.5965 | acc: 58.81\n",
      "epoch: 169/200 | step: 100/927 | trn loss: 0.6686 | val loss: 1.6028 | acc: 58.57\n",
      "epoch: 169/200 | step: 200/927 | trn loss: 0.6149 | val loss: 1.6208 | acc: 58.32\n",
      "epoch: 169/200 | step: 300/927 | trn loss: 0.7477 | val loss: 1.6297 | acc: 58.81\n",
      "epoch: 169/200 | step: 400/927 | trn loss: 0.6487 | val loss: 1.6118 | acc: 57.59\n",
      "epoch: 169/200 | step: 500/927 | trn loss: 0.6415 | val loss: 1.6248 | acc: 58.38\n",
      "epoch: 169/200 | step: 600/927 | trn loss: 0.6332 | val loss: 1.6143 | acc: 58.63\n",
      "epoch: 169/200 | step: 700/927 | trn loss: 0.6249 | val loss: 1.6160 | acc: 58.02\n",
      "epoch: 169/200 | step: 800/927 | trn loss: 0.6205 | val loss: 1.5961 | acc: 58.99\n",
      "epoch: 169/200 | step: 900/927 | trn loss: 0.6562 | val loss: 1.5777 | acc: 58.57\n",
      "epoch: 170/200 | step: 100/927 | trn loss: 0.6855 | val loss: 1.6219 | acc: 57.59\n",
      "epoch: 170/200 | step: 200/927 | trn loss: 0.6543 | val loss: 1.6349 | acc: 57.65\n",
      "epoch: 170/200 | step: 300/927 | trn loss: 0.6915 | val loss: 1.6219 | acc: 58.69\n",
      "epoch: 170/200 | step: 400/927 | trn loss: 0.6582 | val loss: 1.6291 | acc: 58.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 170/200 | step: 500/927 | trn loss: 0.6771 | val loss: 1.6235 | acc: 57.96\n",
      "epoch: 170/200 | step: 600/927 | trn loss: 0.6409 | val loss: 1.6322 | acc: 58.20\n",
      "epoch: 170/200 | step: 700/927 | trn loss: 0.6973 | val loss: 1.5702 | acc: 58.20\n",
      "epoch: 170/200 | step: 800/927 | trn loss: 0.6619 | val loss: 1.6180 | acc: 57.72\n",
      "epoch: 170/200 | step: 900/927 | trn loss: 0.6702 | val loss: 1.6220 | acc: 57.84\n",
      "epoch: 171/200 | step: 100/927 | trn loss: 0.6092 | val loss: 1.6314 | acc: 58.69\n",
      "epoch: 171/200 | step: 200/927 | trn loss: 0.5997 | val loss: 1.6023 | acc: 58.02\n",
      "epoch: 171/200 | step: 300/927 | trn loss: 0.6319 | val loss: 1.6274 | acc: 58.14\n",
      "epoch: 171/200 | step: 400/927 | trn loss: 0.6640 | val loss: 1.6320 | acc: 57.84\n",
      "epoch: 171/200 | step: 500/927 | trn loss: 0.7298 | val loss: 1.6226 | acc: 57.53\n",
      "epoch: 171/200 | step: 600/927 | trn loss: 0.6761 | val loss: 1.5939 | acc: 58.08\n",
      "epoch: 171/200 | step: 700/927 | trn loss: 0.6519 | val loss: 1.5896 | acc: 57.35\n",
      "epoch: 171/200 | step: 800/927 | trn loss: 0.7009 | val loss: 1.6093 | acc: 58.93\n",
      "epoch: 171/200 | step: 900/927 | trn loss: 0.6771 | val loss: 1.6033 | acc: 58.51\n",
      "epoch: 172/200 | step: 100/927 | trn loss: 0.6688 | val loss: 1.6080 | acc: 57.96\n",
      "epoch: 172/200 | step: 200/927 | trn loss: 0.6979 | val loss: 1.6239 | acc: 57.78\n",
      "epoch: 172/200 | step: 300/927 | trn loss: 0.6308 | val loss: 1.5995 | acc: 57.96\n",
      "epoch: 172/200 | step: 400/927 | trn loss: 0.6377 | val loss: 1.6112 | acc: 58.81\n",
      "epoch: 172/200 | step: 500/927 | trn loss: 0.6763 | val loss: 1.5978 | acc: 59.05\n",
      "epoch: 172/200 | step: 600/927 | trn loss: 0.7115 | val loss: 1.5834 | acc: 59.36\n",
      "epoch: 172/200 | step: 700/927 | trn loss: 0.6676 | val loss: 1.6164 | acc: 58.32\n",
      "epoch: 172/200 | step: 800/927 | trn loss: 0.6815 | val loss: 1.5877 | acc: 57.65\n",
      "epoch: 172/200 | step: 900/927 | trn loss: 0.6810 | val loss: 1.6282 | acc: 58.51\n",
      "epoch: 173/200 | step: 100/927 | trn loss: 0.6757 | val loss: 1.6040 | acc: 58.81\n",
      "epoch: 173/200 | step: 200/927 | trn loss: 0.6510 | val loss: 1.6310 | acc: 57.78\n",
      "epoch: 173/200 | step: 300/927 | trn loss: 0.6657 | val loss: 1.6244 | acc: 58.20\n",
      "epoch: 173/200 | step: 400/927 | trn loss: 0.6450 | val loss: 1.6043 | acc: 58.93\n",
      "epoch: 173/200 | step: 500/927 | trn loss: 0.6293 | val loss: 1.5964 | acc: 58.20\n",
      "epoch: 173/200 | step: 600/927 | trn loss: 0.7287 | val loss: 1.6114 | acc: 57.35\n",
      "epoch: 173/200 | step: 700/927 | trn loss: 0.6556 | val loss: 1.6064 | acc: 58.75\n",
      "epoch: 173/200 | step: 800/927 | trn loss: 0.6313 | val loss: 1.5893 | acc: 57.78\n",
      "epoch: 173/200 | step: 900/927 | trn loss: 0.6887 | val loss: 1.6231 | acc: 59.11\n",
      "epoch: 174/200 | step: 100/927 | trn loss: 0.5958 | val loss: 1.6245 | acc: 58.75\n",
      "epoch: 174/200 | step: 200/927 | trn loss: 0.6684 | val loss: 1.6241 | acc: 58.26\n",
      "epoch: 174/200 | step: 300/927 | trn loss: 0.6261 | val loss: 1.6251 | acc: 58.14\n",
      "epoch: 174/200 | step: 400/927 | trn loss: 0.6796 | val loss: 1.6336 | acc: 57.35\n",
      "epoch: 174/200 | step: 500/927 | trn loss: 0.6680 | val loss: 1.6127 | acc: 57.59\n",
      "epoch: 174/200 | step: 600/927 | trn loss: 0.6685 | val loss: 1.6242 | acc: 58.63\n",
      "epoch: 174/200 | step: 700/927 | trn loss: 0.6729 | val loss: 1.5894 | acc: 58.69\n",
      "epoch: 174/200 | step: 800/927 | trn loss: 0.7127 | val loss: 1.6310 | acc: 57.84\n",
      "epoch: 174/200 | step: 900/927 | trn loss: 0.6649 | val loss: 1.6294 | acc: 57.17\n",
      "epoch: 175/200 | step: 100/927 | trn loss: 0.6721 | val loss: 1.6216 | acc: 58.20\n",
      "epoch: 175/200 | step: 200/927 | trn loss: 0.6209 | val loss: 1.6089 | acc: 57.72\n",
      "epoch: 175/200 | step: 300/927 | trn loss: 0.7311 | val loss: 1.6138 | acc: 58.38\n",
      "epoch: 175/200 | step: 400/927 | trn loss: 0.6695 | val loss: 1.5965 | acc: 58.02\n",
      "epoch: 175/200 | step: 500/927 | trn loss: 0.6703 | val loss: 1.6237 | acc: 58.20\n",
      "epoch: 175/200 | step: 600/927 | trn loss: 0.6533 | val loss: 1.6176 | acc: 58.26\n",
      "epoch: 175/200 | step: 700/927 | trn loss: 0.6465 | val loss: 1.6160 | acc: 58.63\n",
      "epoch: 175/200 | step: 800/927 | trn loss: 0.6526 | val loss: 1.6385 | acc: 59.05\n",
      "epoch: 175/200 | step: 900/927 | trn loss: 0.6520 | val loss: 1.6203 | acc: 57.90\n",
      "epoch: 176/200 | step: 100/927 | trn loss: 0.6888 | val loss: 1.5919 | acc: 57.59\n",
      "epoch: 176/200 | step: 200/927 | trn loss: 0.7349 | val loss: 1.6553 | acc: 58.38\n",
      "epoch: 176/200 | step: 300/927 | trn loss: 0.6673 | val loss: 1.6330 | acc: 58.44\n",
      "epoch: 176/200 | step: 400/927 | trn loss: 0.6887 | val loss: 1.6165 | acc: 58.20\n",
      "epoch: 176/200 | step: 500/927 | trn loss: 0.6818 | val loss: 1.5992 | acc: 58.81\n",
      "epoch: 176/200 | step: 600/927 | trn loss: 0.6195 | val loss: 1.6096 | acc: 58.69\n",
      "epoch: 176/200 | step: 700/927 | trn loss: 0.6496 | val loss: 1.6096 | acc: 57.84\n",
      "epoch: 176/200 | step: 800/927 | trn loss: 0.6770 | val loss: 1.6030 | acc: 57.96\n",
      "epoch: 176/200 | step: 900/927 | trn loss: 0.6922 | val loss: 1.6351 | acc: 57.47\n",
      "epoch: 177/200 | step: 100/927 | trn loss: 0.6875 | val loss: 1.6220 | acc: 58.69\n",
      "epoch: 177/200 | step: 200/927 | trn loss: 0.6147 | val loss: 1.5960 | acc: 57.29\n",
      "epoch: 177/200 | step: 300/927 | trn loss: 0.6781 | val loss: 1.6127 | acc: 59.42\n",
      "epoch: 177/200 | step: 400/927 | trn loss: 0.6053 | val loss: 1.6511 | acc: 57.23\n",
      "epoch: 177/200 | step: 500/927 | trn loss: 0.6669 | val loss: 1.6176 | acc: 57.72\n",
      "epoch: 177/200 | step: 600/927 | trn loss: 0.6486 | val loss: 1.6270 | acc: 58.32\n",
      "epoch: 177/200 | step: 700/927 | trn loss: 0.6322 | val loss: 1.5996 | acc: 58.38\n",
      "epoch: 177/200 | step: 800/927 | trn loss: 0.6648 | val loss: 1.6188 | acc: 58.63\n",
      "epoch: 177/200 | step: 900/927 | trn loss: 0.6899 | val loss: 1.6083 | acc: 58.69\n",
      "epoch: 178/200 | step: 100/927 | trn loss: 0.6540 | val loss: 1.6295 | acc: 57.05\n",
      "epoch: 178/200 | step: 200/927 | trn loss: 0.6742 | val loss: 1.5804 | acc: 58.69\n",
      "epoch: 178/200 | step: 400/927 | trn loss: 0.6469 | val loss: 1.6124 | acc: 58.44\n",
      "epoch: 178/200 | step: 500/927 | trn loss: 0.7119 | val loss: 1.6101 | acc: 59.78\n",
      "epoch: 178/200 | step: 600/927 | trn loss: 0.6944 | val loss: 1.6164 | acc: 58.63\n",
      "epoch: 178/200 | step: 700/927 | trn loss: 0.6091 | val loss: 1.6085 | acc: 57.47\n",
      "epoch: 178/200 | step: 800/927 | trn loss: 0.6711 | val loss: 1.6179 | acc: 58.87\n",
      "epoch: 178/200 | step: 900/927 | trn loss: 0.6629 | val loss: 1.5973 | acc: 58.20\n",
      "epoch: 179/200 | step: 100/927 | trn loss: 0.6917 | val loss: 1.6399 | acc: 58.02\n",
      "epoch: 179/200 | step: 200/927 | trn loss: 0.6143 | val loss: 1.6290 | acc: 57.78\n",
      "epoch: 179/200 | step: 300/927 | trn loss: 0.6506 | val loss: 1.6124 | acc: 59.42\n",
      "epoch: 179/200 | step: 400/927 | trn loss: 0.6383 | val loss: 1.5903 | acc: 58.51\n",
      "epoch: 179/200 | step: 500/927 | trn loss: 0.6653 | val loss: 1.6030 | acc: 58.26\n",
      "epoch: 179/200 | step: 600/927 | trn loss: 0.6541 | val loss: 1.6141 | acc: 57.90\n",
      "epoch: 179/200 | step: 700/927 | trn loss: 0.6694 | val loss: 1.5750 | acc: 58.75\n",
      "epoch: 179/200 | step: 800/927 | trn loss: 0.6825 | val loss: 1.5940 | acc: 57.41\n",
      "epoch: 179/200 | step: 900/927 | trn loss: 0.6629 | val loss: 1.6266 | acc: 58.20\n",
      "epoch: 180/200 | step: 100/927 | trn loss: 0.6491 | val loss: 1.6178 | acc: 57.96\n",
      "epoch: 180/200 | step: 200/927 | trn loss: 0.6155 | val loss: 1.5993 | acc: 58.38\n",
      "epoch: 180/200 | step: 300/927 | trn loss: 0.6258 | val loss: 1.5870 | acc: 58.44\n",
      "epoch: 180/200 | step: 400/927 | trn loss: 0.7240 | val loss: 1.6110 | acc: 59.54\n",
      "epoch: 180/200 | step: 500/927 | trn loss: 0.6560 | val loss: 1.6574 | acc: 59.17\n",
      "epoch: 180/200 | step: 600/927 | trn loss: 0.6401 | val loss: 1.5929 | acc: 58.75\n",
      "epoch: 180/200 | step: 700/927 | trn loss: 0.6217 | val loss: 1.5973 | acc: 58.14\n",
      "epoch: 180/200 | step: 800/927 | trn loss: 0.6217 | val loss: 1.6317 | acc: 57.35\n",
      "epoch: 180/200 | step: 900/927 | trn loss: 0.6343 | val loss: 1.6324 | acc: 58.69\n",
      "epoch: 181/200 | step: 100/927 | trn loss: 0.6977 | val loss: 1.6237 | acc: 58.32\n",
      "epoch: 181/200 | step: 200/927 | trn loss: 0.6839 | val loss: 1.6041 | acc: 58.20\n",
      "epoch: 181/200 | step: 300/927 | trn loss: 0.6386 | val loss: 1.6039 | acc: 59.05\n",
      "epoch: 181/200 | step: 400/927 | trn loss: 0.6636 | val loss: 1.6381 | acc: 58.08\n",
      "epoch: 181/200 | step: 500/927 | trn loss: 0.6556 | val loss: 1.6084 | acc: 57.53\n",
      "epoch: 181/200 | step: 600/927 | trn loss: 0.6401 | val loss: 1.6080 | acc: 57.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 181/200 | step: 700/927 | trn loss: 0.6800 | val loss: 1.6032 | acc: 58.99\n",
      "epoch: 181/200 | step: 800/927 | trn loss: 0.6758 | val loss: 1.5943 | acc: 58.14\n",
      "epoch: 181/200 | step: 900/927 | trn loss: 0.6365 | val loss: 1.6174 | acc: 58.51\n",
      "epoch: 182/200 | step: 100/927 | trn loss: 0.7062 | val loss: 1.6354 | acc: 57.47\n",
      "epoch: 182/200 | step: 200/927 | trn loss: 0.6761 | val loss: 1.6053 | acc: 58.69\n",
      "epoch: 182/200 | step: 300/927 | trn loss: 0.6929 | val loss: 1.6126 | acc: 58.02\n",
      "epoch: 182/200 | step: 400/927 | trn loss: 0.6856 | val loss: 1.5892 | acc: 59.54\n",
      "epoch: 182/200 | step: 500/927 | trn loss: 0.6924 | val loss: 1.5968 | acc: 58.69\n",
      "epoch: 182/200 | step: 600/927 | trn loss: 0.6347 | val loss: 1.5886 | acc: 58.69\n",
      "epoch: 182/200 | step: 700/927 | trn loss: 0.7445 | val loss: 1.6349 | acc: 58.81\n",
      "epoch: 182/200 | step: 800/927 | trn loss: 0.6474 | val loss: 1.6230 | acc: 57.05\n",
      "epoch: 182/200 | step: 900/927 | trn loss: 0.6630 | val loss: 1.6280 | acc: 58.93\n",
      "epoch: 183/200 | step: 100/927 | trn loss: 0.6697 | val loss: 1.6036 | acc: 59.11\n",
      "epoch: 183/200 | step: 200/927 | trn loss: 0.6954 | val loss: 1.6088 | acc: 56.99\n",
      "epoch: 183/200 | step: 300/927 | trn loss: 0.6075 | val loss: 1.6361 | acc: 58.81\n",
      "epoch: 183/200 | step: 400/927 | trn loss: 0.6724 | val loss: 1.6215 | acc: 58.51\n",
      "epoch: 183/200 | step: 500/927 | trn loss: 0.6327 | val loss: 1.6068 | acc: 59.05\n",
      "epoch: 183/200 | step: 600/927 | trn loss: 0.6317 | val loss: 1.5952 | acc: 58.51\n",
      "epoch: 183/200 | step: 700/927 | trn loss: 0.6617 | val loss: 1.6386 | acc: 57.41\n",
      "epoch: 183/200 | step: 800/927 | trn loss: 0.6468 | val loss: 1.6067 | acc: 58.81\n",
      "epoch: 183/200 | step: 900/927 | trn loss: 0.6102 | val loss: 1.6274 | acc: 57.78\n",
      "epoch: 184/200 | step: 100/927 | trn loss: 0.7225 | val loss: 1.6287 | acc: 57.90\n",
      "epoch: 184/200 | step: 200/927 | trn loss: 0.6566 | val loss: 1.6127 | acc: 58.44\n",
      "epoch: 184/200 | step: 300/927 | trn loss: 0.6037 | val loss: 1.6256 | acc: 59.05\n",
      "epoch: 184/200 | step: 400/927 | trn loss: 0.6890 | val loss: 1.6518 | acc: 57.72\n",
      "epoch: 184/200 | step: 500/927 | trn loss: 0.6569 | val loss: 1.6645 | acc: 57.41\n",
      "epoch: 184/200 | step: 600/927 | trn loss: 0.6502 | val loss: 1.6163 | acc: 57.78\n",
      "epoch: 184/200 | step: 700/927 | trn loss: 0.6417 | val loss: 1.6239 | acc: 58.32\n",
      "epoch: 184/200 | step: 800/927 | trn loss: 0.6733 | val loss: 1.6023 | acc: 58.20\n",
      "epoch: 184/200 | step: 900/927 | trn loss: 0.6404 | val loss: 1.6417 | acc: 57.59\n",
      "epoch: 185/200 | step: 100/927 | trn loss: 0.6356 | val loss: 1.6454 | acc: 58.20\n",
      "epoch: 185/200 | step: 200/927 | trn loss: 0.6462 | val loss: 1.6526 | acc: 58.99\n",
      "epoch: 185/200 | step: 300/927 | trn loss: 0.7552 | val loss: 1.6257 | acc: 57.65\n",
      "epoch: 185/200 | step: 400/927 | trn loss: 0.6323 | val loss: 1.6051 | acc: 58.38\n",
      "epoch: 185/200 | step: 500/927 | trn loss: 0.6756 | val loss: 1.5995 | acc: 58.57\n",
      "epoch: 185/200 | step: 600/927 | trn loss: 0.6689 | val loss: 1.6169 | acc: 57.78\n",
      "epoch: 185/200 | step: 700/927 | trn loss: 0.6458 | val loss: 1.5994 | acc: 58.75\n",
      "epoch: 185/200 | step: 800/927 | trn loss: 0.6740 | val loss: 1.6469 | acc: 58.38\n",
      "epoch: 185/200 | step: 900/927 | trn loss: 0.6378 | val loss: 1.6234 | acc: 58.51\n",
      "epoch: 186/200 | step: 100/927 | trn loss: 0.6455 | val loss: 1.6349 | acc: 58.26\n",
      "epoch: 186/200 | step: 200/927 | trn loss: 0.6627 | val loss: 1.6355 | acc: 58.69\n",
      "epoch: 186/200 | step: 300/927 | trn loss: 0.5857 | val loss: 1.6271 | acc: 57.59\n",
      "epoch: 186/200 | step: 400/927 | trn loss: 0.6714 | val loss: 1.6162 | acc: 58.20\n",
      "epoch: 186/200 | step: 500/927 | trn loss: 0.7147 | val loss: 1.6140 | acc: 58.93\n",
      "epoch: 186/200 | step: 600/927 | trn loss: 0.6915 | val loss: 1.6208 | acc: 57.72\n",
      "epoch: 186/200 | step: 700/927 | trn loss: 0.6766 | val loss: 1.6151 | acc: 58.02\n",
      "epoch: 186/200 | step: 800/927 | trn loss: 0.6268 | val loss: 1.5835 | acc: 58.08\n",
      "epoch: 186/200 | step: 900/927 | trn loss: 0.6586 | val loss: 1.6160 | acc: 58.93\n",
      "epoch: 187/200 | step: 100/927 | trn loss: 0.6589 | val loss: 1.6428 | acc: 57.53\n",
      "epoch: 187/200 | step: 200/927 | trn loss: 0.6288 | val loss: 1.6171 | acc: 59.23\n",
      "epoch: 187/200 | step: 300/927 | trn loss: 0.7312 | val loss: 1.5991 | acc: 58.26\n",
      "epoch: 187/200 | step: 400/927 | trn loss: 0.6069 | val loss: 1.6249 | acc: 58.02\n",
      "epoch: 187/200 | step: 500/927 | trn loss: 0.6486 | val loss: 1.6259 | acc: 57.65\n",
      "epoch: 187/200 | step: 600/927 | trn loss: 0.7030 | val loss: 1.6387 | acc: 59.11\n",
      "epoch: 187/200 | step: 700/927 | trn loss: 0.6361 | val loss: 1.6073 | acc: 57.78\n",
      "epoch: 187/200 | step: 800/927 | trn loss: 0.7352 | val loss: 1.5934 | acc: 58.38\n",
      "epoch: 187/200 | step: 900/927 | trn loss: 0.6429 | val loss: 1.6434 | acc: 58.26\n",
      "epoch: 188/200 | step: 100/927 | trn loss: 0.5804 | val loss: 1.6110 | acc: 58.57\n",
      "epoch: 188/200 | step: 200/927 | trn loss: 0.6341 | val loss: 1.6013 | acc: 58.44\n",
      "epoch: 188/200 | step: 300/927 | trn loss: 0.6271 | val loss: 1.6234 | acc: 57.47\n",
      "epoch: 188/200 | step: 400/927 | trn loss: 0.6238 | val loss: 1.6165 | acc: 60.02\n",
      "epoch: 188/200 | step: 500/927 | trn loss: 0.6850 | val loss: 1.6000 | acc: 58.02\n",
      "epoch: 188/200 | step: 600/927 | trn loss: 0.7073 | val loss: 1.5992 | acc: 59.60\n",
      "epoch: 188/200 | step: 700/927 | trn loss: 0.7071 | val loss: 1.6254 | acc: 59.60\n",
      "epoch: 188/200 | step: 800/927 | trn loss: 0.6921 | val loss: 1.5776 | acc: 59.90\n",
      "epoch: 188/200 | step: 900/927 | trn loss: 0.6494 | val loss: 1.6365 | acc: 58.51\n",
      "epoch: 189/200 | step: 100/927 | trn loss: 0.6261 | val loss: 1.5976 | acc: 58.63\n",
      "epoch: 189/200 | step: 200/927 | trn loss: 0.5982 | val loss: 1.6270 | acc: 58.38\n",
      "epoch: 189/200 | step: 300/927 | trn loss: 0.6559 | val loss: 1.6398 | acc: 58.69\n",
      "epoch: 189/200 | step: 400/927 | trn loss: 0.6820 | val loss: 1.6099 | acc: 58.32\n",
      "epoch: 189/200 | step: 500/927 | trn loss: 0.7012 | val loss: 1.6090 | acc: 58.14\n",
      "epoch: 189/200 | step: 600/927 | trn loss: 0.7035 | val loss: 1.6194 | acc: 59.17\n",
      "epoch: 189/200 | step: 700/927 | trn loss: 0.6159 | val loss: 1.6022 | acc: 58.14\n",
      "epoch: 189/200 | step: 800/927 | trn loss: 0.6844 | val loss: 1.6185 | acc: 57.65\n",
      "epoch: 189/200 | step: 900/927 | trn loss: 0.6826 | val loss: 1.5844 | acc: 57.35\n",
      "epoch: 190/200 | step: 100/927 | trn loss: 0.6481 | val loss: 1.6054 | acc: 58.57\n",
      "epoch: 190/200 | step: 200/927 | trn loss: 0.6321 | val loss: 1.5857 | acc: 58.69\n",
      "epoch: 190/200 | step: 300/927 | trn loss: 0.6006 | val loss: 1.6247 | acc: 57.41\n",
      "epoch: 190/200 | step: 400/927 | trn loss: 0.6929 | val loss: 1.6524 | acc: 57.41\n",
      "epoch: 190/200 | step: 500/927 | trn loss: 0.6751 | val loss: 1.6146 | acc: 57.17\n",
      "epoch: 190/200 | step: 600/927 | trn loss: 0.6705 | val loss: 1.6331 | acc: 57.96\n",
      "epoch: 190/200 | step: 700/927 | trn loss: 0.6546 | val loss: 1.5944 | acc: 59.36\n",
      "epoch: 190/200 | step: 800/927 | trn loss: 0.6787 | val loss: 1.6487 | acc: 57.29\n",
      "epoch: 190/200 | step: 900/927 | trn loss: 0.6466 | val loss: 1.5898 | acc: 59.11\n",
      "epoch: 191/200 | step: 100/927 | trn loss: 0.6831 | val loss: 1.6163 | acc: 59.54\n",
      "epoch: 191/200 | step: 200/927 | trn loss: 0.6955 | val loss: 1.5831 | acc: 57.59\n",
      "epoch: 191/200 | step: 300/927 | trn loss: 0.6589 | val loss: 1.5790 | acc: 59.30\n",
      "epoch: 191/200 | step: 400/927 | trn loss: 0.6757 | val loss: 1.6108 | acc: 58.87\n",
      "epoch: 191/200 | step: 500/927 | trn loss: 0.6010 | val loss: 1.6187 | acc: 58.26\n",
      "epoch: 191/200 | step: 600/927 | trn loss: 0.6421 | val loss: 1.6316 | acc: 58.99\n",
      "epoch: 191/200 | step: 700/927 | trn loss: 0.6358 | val loss: 1.6147 | acc: 58.26\n",
      "epoch: 191/200 | step: 800/927 | trn loss: 0.5901 | val loss: 1.6025 | acc: 59.48\n",
      "epoch: 191/200 | step: 900/927 | trn loss: 0.6475 | val loss: 1.6375 | acc: 58.26\n",
      "epoch: 192/200 | step: 100/927 | trn loss: 0.5925 | val loss: 1.6392 | acc: 58.93\n",
      "epoch: 192/200 | step: 200/927 | trn loss: 0.6632 | val loss: 1.6207 | acc: 58.93\n",
      "epoch: 192/200 | step: 300/927 | trn loss: 0.6729 | val loss: 1.5981 | acc: 58.63\n",
      "epoch: 192/200 | step: 400/927 | trn loss: 0.6428 | val loss: 1.6434 | acc: 58.20\n",
      "epoch: 192/200 | step: 500/927 | trn loss: 0.6529 | val loss: 1.6204 | acc: 58.44\n",
      "epoch: 192/200 | step: 600/927 | trn loss: 0.6328 | val loss: 1.6191 | acc: 58.51\n",
      "epoch: 192/200 | step: 700/927 | trn loss: 0.6877 | val loss: 1.6277 | acc: 58.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 192/200 | step: 800/927 | trn loss: 0.6700 | val loss: 1.6322 | acc: 58.75\n",
      "epoch: 192/200 | step: 900/927 | trn loss: 0.6297 | val loss: 1.6152 | acc: 58.57\n",
      "epoch: 193/200 | step: 100/927 | trn loss: 0.6436 | val loss: 1.6157 | acc: 58.57\n",
      "epoch: 193/200 | step: 200/927 | trn loss: 0.6609 | val loss: 1.6087 | acc: 58.38\n",
      "epoch: 193/200 | step: 300/927 | trn loss: 0.6079 | val loss: 1.6221 | acc: 57.96\n",
      "epoch: 193/200 | step: 400/927 | trn loss: 0.6718 | val loss: 1.6302 | acc: 58.87\n",
      "epoch: 193/200 | step: 500/927 | trn loss: 0.6597 | val loss: 1.6237 | acc: 58.32\n",
      "epoch: 193/200 | step: 600/927 | trn loss: 0.6448 | val loss: 1.6209 | acc: 57.65\n",
      "epoch: 193/200 | step: 700/927 | trn loss: 0.6360 | val loss: 1.6041 | acc: 57.65\n",
      "epoch: 193/200 | step: 800/927 | trn loss: 0.6886 | val loss: 1.6239 | acc: 57.35\n",
      "epoch: 193/200 | step: 900/927 | trn loss: 0.6101 | val loss: 1.6270 | acc: 59.17\n",
      "epoch: 194/200 | step: 100/927 | trn loss: 0.5982 | val loss: 1.6272 | acc: 57.65\n",
      "epoch: 194/200 | step: 200/927 | trn loss: 0.6577 | val loss: 1.6179 | acc: 58.38\n",
      "epoch: 194/200 | step: 300/927 | trn loss: 0.7167 | val loss: 1.6590 | acc: 57.17\n",
      "epoch: 194/200 | step: 400/927 | trn loss: 0.6459 | val loss: 1.6468 | acc: 57.84\n",
      "epoch: 194/200 | step: 500/927 | trn loss: 0.6779 | val loss: 1.6378 | acc: 57.78\n",
      "epoch: 194/200 | step: 600/927 | trn loss: 0.6385 | val loss: 1.6470 | acc: 57.05\n",
      "epoch: 194/200 | step: 700/927 | trn loss: 0.6179 | val loss: 1.6353 | acc: 58.81\n",
      "epoch: 194/200 | step: 800/927 | trn loss: 0.6124 | val loss: 1.6058 | acc: 58.69\n",
      "epoch: 194/200 | step: 900/927 | trn loss: 0.6870 | val loss: 1.6331 | acc: 58.81\n",
      "epoch: 195/200 | step: 100/927 | trn loss: 0.6391 | val loss: 1.6314 | acc: 58.32\n",
      "epoch: 195/200 | step: 200/927 | trn loss: 0.6678 | val loss: 1.6145 | acc: 58.02\n",
      "epoch: 195/200 | step: 300/927 | trn loss: 0.6873 | val loss: 1.6523 | acc: 58.44\n",
      "epoch: 195/200 | step: 400/927 | trn loss: 0.6256 | val loss: 1.6379 | acc: 57.47\n",
      "epoch: 195/200 | step: 500/927 | trn loss: 0.6735 | val loss: 1.6051 | acc: 58.08\n",
      "epoch: 195/200 | step: 600/927 | trn loss: 0.6384 | val loss: 1.6054 | acc: 57.96\n",
      "epoch: 195/200 | step: 700/927 | trn loss: 0.6661 | val loss: 1.6143 | acc: 58.20\n",
      "epoch: 195/200 | step: 800/927 | trn loss: 0.6435 | val loss: 1.6018 | acc: 58.63\n",
      "epoch: 195/200 | step: 900/927 | trn loss: 0.6986 | val loss: 1.6321 | acc: 58.20\n",
      "epoch: 196/200 | step: 100/927 | trn loss: 0.6174 | val loss: 1.6297 | acc: 58.20\n",
      "epoch: 196/200 | step: 200/927 | trn loss: 0.6827 | val loss: 1.6010 | acc: 58.38\n",
      "epoch: 196/200 | step: 300/927 | trn loss: 0.6418 | val loss: 1.6035 | acc: 57.23\n",
      "epoch: 196/200 | step: 400/927 | trn loss: 0.6711 | val loss: 1.6209 | acc: 56.68\n",
      "epoch: 196/200 | step: 500/927 | trn loss: 0.6786 | val loss: 1.6006 | acc: 58.38\n",
      "epoch: 196/200 | step: 600/927 | trn loss: 0.5941 | val loss: 1.6218 | acc: 57.59\n",
      "epoch: 196/200 | step: 700/927 | trn loss: 0.6253 | val loss: 1.6321 | acc: 58.32\n",
      "epoch: 196/200 | step: 800/927 | trn loss: 0.6071 | val loss: 1.6010 | acc: 58.38\n",
      "epoch: 196/200 | step: 900/927 | trn loss: 0.6690 | val loss: 1.5912 | acc: 58.57\n",
      "epoch: 197/200 | step: 100/927 | trn loss: 0.6725 | val loss: 1.6281 | acc: 58.57\n",
      "epoch: 197/200 | step: 200/927 | trn loss: 0.6769 | val loss: 1.6208 | acc: 57.90\n",
      "epoch: 197/200 | step: 300/927 | trn loss: 0.6114 | val loss: 1.6232 | acc: 58.38\n",
      "epoch: 197/200 | step: 400/927 | trn loss: 0.6642 | val loss: 1.6171 | acc: 58.87\n",
      "epoch: 197/200 | step: 500/927 | trn loss: 0.6386 | val loss: 1.6293 | acc: 58.51\n",
      "epoch: 197/200 | step: 600/927 | trn loss: 0.6363 | val loss: 1.5958 | acc: 58.32\n",
      "epoch: 197/200 | step: 700/927 | trn loss: 0.6253 | val loss: 1.6365 | acc: 57.53\n",
      "epoch: 197/200 | step: 800/927 | trn loss: 0.6628 | val loss: 1.6272 | acc: 58.93\n",
      "epoch: 197/200 | step: 900/927 | trn loss: 0.6637 | val loss: 1.6035 | acc: 59.36\n",
      "epoch: 198/200 | step: 100/927 | trn loss: 0.6215 | val loss: 1.6359 | acc: 59.23\n",
      "epoch: 198/200 | step: 200/927 | trn loss: 0.6279 | val loss: 1.6057 | acc: 58.63\n",
      "epoch: 198/200 | step: 300/927 | trn loss: 0.6510 | val loss: 1.5945 | acc: 58.81\n",
      "epoch: 198/200 | step: 400/927 | trn loss: 0.6347 | val loss: 1.6139 | acc: 58.32\n",
      "epoch: 198/200 | step: 500/927 | trn loss: 0.6536 | val loss: 1.6228 | acc: 57.65\n",
      "epoch: 198/200 | step: 600/927 | trn loss: 0.6408 | val loss: 1.6229 | acc: 58.75\n",
      "epoch: 198/200 | step: 700/927 | trn loss: 0.6268 | val loss: 1.6012 | acc: 59.66\n",
      "epoch: 198/200 | step: 800/927 | trn loss: 0.6557 | val loss: 1.6171 | acc: 58.32\n",
      "epoch: 198/200 | step: 900/927 | trn loss: 0.6070 | val loss: 1.6171 | acc: 58.14\n",
      "epoch: 199/200 | step: 100/927 | trn loss: 0.6111 | val loss: 1.6372 | acc: 58.38\n",
      "epoch: 199/200 | step: 200/927 | trn loss: 0.6475 | val loss: 1.6084 | acc: 59.30\n",
      "epoch: 199/200 | step: 300/927 | trn loss: 0.6795 | val loss: 1.6337 | acc: 58.32\n",
      "epoch: 199/200 | step: 400/927 | trn loss: 0.6236 | val loss: 1.6248 | acc: 58.32\n",
      "epoch: 199/200 | step: 500/927 | trn loss: 0.6268 | val loss: 1.6255 | acc: 58.57\n",
      "epoch: 199/200 | step: 600/927 | trn loss: 0.7104 | val loss: 1.6360 | acc: 57.72\n",
      "epoch: 199/200 | step: 700/927 | trn loss: 0.5961 | val loss: 1.6192 | acc: 58.26\n",
      "epoch: 199/200 | step: 800/927 | trn loss: 0.6220 | val loss: 1.6104 | acc: 58.26\n",
      "epoch: 199/200 | step: 900/927 | trn loss: 0.6634 | val loss: 1.5802 | acc: 58.44\n",
      "epoch: 200/200 | step: 100/927 | trn loss: 0.6217 | val loss: 1.6038 | acc: 58.32\n",
      "epoch: 200/200 | step: 200/927 | trn loss: 0.6935 | val loss: 1.6317 | acc: 57.72\n",
      "epoch: 200/200 | step: 300/927 | trn loss: 0.6924 | val loss: 1.6445 | acc: 57.90\n",
      "epoch: 200/200 | step: 400/927 | trn loss: 0.6355 | val loss: 1.5956 | acc: 58.99\n",
      "epoch: 200/200 | step: 500/927 | trn loss: 0.6643 | val loss: 1.6311 | acc: 58.38\n",
      "epoch: 200/200 | step: 600/927 | trn loss: 0.6526 | val loss: 1.6004 | acc: 58.93\n",
      "epoch: 200/200 | step: 700/927 | trn loss: 0.6278 | val loss: 1.6137 | acc: 57.53\n",
      "epoch: 200/200 | step: 800/927 | trn loss: 0.6758 | val loss: 1.5993 | acc: 58.26\n",
      "epoch: 200/200 | step: 900/927 | trn loss: 0.6632 | val loss: 1.6024 | acc: 58.32\n",
      "\n",
      "training finished!\n",
      "model saved!\n"
     ]
    }
   ],
   "source": [
    "# loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# backpropagation method\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(resnet.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0002)\n",
    "# hyper-parameters\n",
    "num_epochs = 200\n",
    "num_batches = len(train_loader)\n",
    "\n",
    "trn_loss_list = []\n",
    "val_loss_list = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    adjust_learning_rate(optimizer,epoch)\n",
    "    trn_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        x, label = data\n",
    "        x = x.to(device)\n",
    "        label = label.to(device)\n",
    "        # grad init\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        model_output = resnet(x)\n",
    "        # calculate loss\n",
    "        loss = criterion(model_output, label)\n",
    "        # back propagation \n",
    "        loss.backward()\n",
    "        # weight update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # trn_loss summary\n",
    "        trn_loss += loss.item()\n",
    "        # del (memory issue)\n",
    "        del loss\n",
    "        del model_output\n",
    "        \n",
    "        # 학습과정 출력\n",
    "        if (i+1) % 100 == 0: # every 100 mini-batches\n",
    "            with torch.no_grad(): # very very very very important!!!\n",
    "                val_loss = 0.0\n",
    "                corr_num = 0\n",
    "                total_num = 0\n",
    "                for j, val in enumerate(val_loader):\n",
    "                    val_x, val_label = val\n",
    "                    val_x = val_x.to(device)\n",
    "                    val_label =val_label.to(device)\n",
    "                    val_output = resnet(val_x)\n",
    "                    v_loss = criterion(val_output, val_label)\n",
    "                    val_loss += v_loss\n",
    "                    \n",
    "                    model_label = val_output.argmax(dim=1)\n",
    "                    corr = val_label[val_label == model_label].size(0)\n",
    "                    corr_num += corr\n",
    "                    total_num += val_label.size(0)\n",
    "            \n",
    "            print(\"epoch: {}/{} | step: {}/{} | trn loss: {:.4f} | val loss: {:.4f} | acc: {:.2f}\".format(\n",
    "                epoch+1, num_epochs, i+1, num_batches, trn_loss / 100, val_loss / len(val_loader), (corr_num / total_num) * 100\n",
    "            ))            \n",
    "            \n",
    "            trn_loss_list.append(trn_loss/100)\n",
    "            val_loss_list.append(val_loss/len(val_loader))\n",
    "            trn_loss = 0.0\n",
    "\n",
    "print(\"training finished!\")\n",
    "\n",
    "# save\n",
    "PATH = \"./resnet18_05.pt\"\n",
    "torch.save(resnet.state_dict(), PATH)\n",
    "\n",
    "print(\"model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 59.82\n"
     ]
    }
   ],
   "source": [
    "# resnet 18 , learning rate : start = 0.01 -> 100 : 0.001 -> 150 : 0.0001\n",
    "# test acc\n",
    "with torch.no_grad():\n",
    "    corr_num = 0\n",
    "    total_num = 0\n",
    "    for j, val in enumerate(test_loader):\n",
    "        val_x, val_label = val\n",
    "        val_x = val_x.to(device)\n",
    "        val_label =val_label.to(device)\n",
    "        val_output = resnet(val_x)\n",
    "        model_label = val_output.argmax(dim=1)rere\n",
    "        corr = val_label[val_label == model_label].size(0)\n",
    "        corr_num += corr\n",
    "        total_num += val_label.size(0)\n",
    "\n",
    "print(\"test_acc: {:.2f}\".format(corr_num / total_num * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "plt.plot(trn_loss_list, label=\"train_loss\")\n",
    "plt.legend()\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 CONDA (NGC/PyTorch 20.06) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
