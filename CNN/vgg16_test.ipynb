{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "# from collections import OrderedDict\n",
    "%matplotlib inline\n",
    "\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계층적인 폴더 구조를 갖고 있는 데이터셋을 불러올때 사용 : 폴더 이름 = 클래스 명\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device =='cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another\n",
    "dataset = ImageFolder('data/Images')\n",
    "\n",
    "test_pct = 0.2\n",
    "test_size = int(len(dataset)*test_pct)\n",
    "dataset_size = len(dataset) - test_size\n",
    "\n",
    "val_pct = 0.1\n",
    "val_size = int(dataset_size*val_pct)\n",
    "train_size = dataset_size - val_size\n",
    "\n",
    "train, val, test = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# custom Dataset\n",
    "class DogData(Dataset) :\n",
    "    def __init__(self, ds, transform = None) :\n",
    "        self.ds = ds\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self) :\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, idx) :\n",
    "        img, label = self.ds[idx]\n",
    "        if self.transform :\n",
    "            img = self.transform(img)\n",
    "            return img, label\n",
    "\n",
    "        \n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225]) \n",
    "                                      ])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "                                     transforms.Resize(255), \n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                                     transforms.Resize(255), \n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "train = DogData(train, train_transforms)\n",
    "val = DogData(val, val_transforms)\n",
    "test = DogData(test, test_transforms)\n",
    "\n",
    "\n",
    "num_workers = 4\n",
    "batch_size = 64\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(train, batch_size=batch_size, \n",
    "                                              num_workers=num_workers, shuffle=True)\n",
    "valLoader = torch.utils.data.DataLoader(val, batch_size=batch_size, \n",
    "                                            num_workers=num_workers, shuffle=True)\n",
    "testLoader = torch.utils.data.DataLoader(test, batch_size=batch_size,\n",
    "                                             num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13 + 3 =vgg16\n",
    "#  cfgs = {\n",
    "#     'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "#     'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "#     'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "#     'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "# }\n",
    "\n",
    "\n",
    "# cfg = [32,32,'M', 64,64,128,128,128,'M',256,256,256,512,512,512,'M'] \n",
    "cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, features, num_classes=120, init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        \n",
    "        self.features = features #convolution layer : make layers로 생성\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7)) #이미지가 7 by 7보다 작을경우에는 안 사용해도됨 \n",
    "        \n",
    "        # classifier : fully connected layer \n",
    "        self.classifier = nn.Sequential(\n",
    "            # 32 32 -> 16 8 \"4\" (maxpooling 3번 거치기 때문) 224 224 -> 112 56 28\n",
    "            nn.Linear(512 * 7 * 7, 4096), # custom시 이미지 사이즈가 다를경우 꼭 수정을 해줘야함(연산수가 달라질경우)\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )#FC layer\n",
    "        \n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x) #Convolution \n",
    "        x = self.avgpool(x) # avgpool\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.classifier(x) #FC layer\n",
    "        return x\n",
    "    \n",
    "    # 층의 특성에 맞게 weight initialize\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d): # m이 conv2d일때 kaiming_normal로 초기화\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:   # VGG에서는 bias값이 0\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    \n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "                     \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# 'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "# make_layers(cfg['A'], batch_norm = False)\n",
    "# v가 64 : conv\n",
    "# conv2d = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n",
    "# layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "# v가 M : Maxpooling\n",
    "# layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgglayers = make_layers(cfg)\n",
    "vgg16 = VGG(vgglayers,120,True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=120, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.7774e+23,  5.5214e+23,  7.0989e+23,  1.0826e+23, -1.1422e+24,\n",
      "          7.6211e+23,  3.9726e+23, -1.3046e+23, -9.3284e+23, -1.2667e+23,\n",
      "         -8.1710e+23,  4.8197e+22,  2.4528e+23,  8.2498e+23,  3.4407e+23,\n",
      "          7.4692e+23,  9.8521e+23,  2.2952e+23,  4.6769e+23,  3.6899e+23,\n",
      "         -1.1700e+23, -7.4617e+23, -4.2211e+23,  4.8915e+23,  1.1324e+24,\n",
      "         -7.8808e+23, -1.7534e+23,  6.5910e+23, -3.5116e+23, -3.0873e+23,\n",
      "         -3.5973e+23,  3.6252e+23,  7.8478e+23,  1.0634e+23,  8.3136e+22,\n",
      "         -3.1975e+23,  4.9043e+22, -5.4063e+22, -1.8189e+22,  4.9896e+23,\n",
      "         -6.1553e+23, -7.4439e+23, -2.9864e+23, -1.1136e+24,  2.9101e+23,\n",
      "         -5.7728e+23, -1.0350e+24,  6.8227e+23, -8.0062e+23, -3.6225e+23,\n",
      "          4.8273e+23, -5.8835e+23,  5.6608e+23,  2.7205e+23, -1.5942e+24,\n",
      "         -6.4562e+22,  8.5271e+23, -7.0943e+22, -4.4291e+23,  9.0556e+23,\n",
      "          3.2836e+22,  3.2436e+23, -8.9679e+21, -4.2663e+23, -2.8048e+23,\n",
      "          5.1562e+22,  4.6088e+23, -5.0934e+23, -4.2020e+23, -2.0996e+23,\n",
      "         -1.6129e+23, -1.8362e+23, -1.7013e+23,  8.8220e+23, -1.1202e+24,\n",
      "         -3.3666e+23,  1.5301e+23, -7.9104e+23, -4.8001e+23, -7.8688e+23,\n",
      "          4.2098e+23, -3.0164e+23,  9.1748e+23, -3.0703e+23,  7.1807e+21,\n",
      "          1.8271e+23, -2.7769e+23,  3.6603e+23, -1.4382e+24,  2.6094e+23,\n",
      "          8.9296e+23, -5.3666e+23, -1.1818e+24, -4.4390e+23,  2.2310e+23,\n",
      "         -1.5535e+23, -7.3098e+23,  2.2026e+23,  1.5409e+24, -9.9862e+23,\n",
      "         -1.0081e+23,  5.4781e+23, -1.0287e+23,  2.0371e+23,  3.9068e+23,\n",
      "         -2.4110e+23, -2.5097e+23, -6.1965e+23,  4.7072e+23, -7.1196e+22,\n",
      "         -2.2655e+23,  6.2645e+23, -5.2790e+23,  1.2316e+24,  2.9495e+23,\n",
      "         -6.9854e+22,  9.3140e+23,  1.5672e+24, -9.5784e+23, -5.0971e+23]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# test 실행 \n",
    "a=torch.Tensor(1,3,224,224).to(device)\n",
    "out = vgg16(a)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate를 단계적으로 줄여주는 방법\n",
    "# epoch 100 -> lr/10, 150 -> lr/10\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate\n",
    "    if epoch >= 100:\n",
    "        lr /= 10\n",
    "    if epoch >= 150:\n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331281126ad44dda9981d310115938b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/200 | step: 100/232 | trn loss: 4.7881 | val loss: 4.7866 | acc: 0.91\n",
      "epoch: 1/200 | step: 200/232 | trn loss: 4.7844 | val loss: 4.7844 | acc: 0.85\n",
      "epoch: 2/200 | step: 100/232 | trn loss: 4.7767 | val loss: 4.7826 | acc: 1.03\n",
      "epoch: 2/200 | step: 200/232 | trn loss: 4.7649 | val loss: 4.7734 | acc: 1.09\n",
      "epoch: 3/200 | step: 100/232 | trn loss: 4.7529 | val loss: 4.7427 | acc: 1.28\n",
      "epoch: 3/200 | step: 200/232 | trn loss: 4.7350 | val loss: 4.7203 | acc: 1.52\n",
      "epoch: 4/200 | step: 100/232 | trn loss: 4.7250 | val loss: 4.7142 | acc: 1.70\n",
      "epoch: 4/200 | step: 200/232 | trn loss: 4.7181 | val loss: 4.7291 | acc: 1.76\n",
      "epoch: 5/200 | step: 100/232 | trn loss: 4.7175 | val loss: 4.6849 | acc: 1.76\n",
      "epoch: 5/200 | step: 200/232 | trn loss: 4.7067 | val loss: 4.6644 | acc: 1.52\n",
      "epoch: 6/200 | step: 100/232 | trn loss: 4.6779 | val loss: 4.6509 | acc: 2.00\n",
      "epoch: 6/200 | step: 200/232 | trn loss: 4.6695 | val loss: 4.6000 | acc: 2.43\n",
      "epoch: 7/200 | step: 100/232 | trn loss: 4.6496 | val loss: 4.6051 | acc: 2.67\n",
      "epoch: 7/200 | step: 200/232 | trn loss: 4.6436 | val loss: 4.5577 | acc: 2.43\n",
      "epoch: 8/200 | step: 100/232 | trn loss: 4.6124 | val loss: 4.5332 | acc: 2.79\n",
      "epoch: 8/200 | step: 200/232 | trn loss: 4.6110 | val loss: 4.5191 | acc: 2.73\n",
      "epoch: 9/200 | step: 100/232 | trn loss: 4.5821 | val loss: 4.4657 | acc: 3.10\n",
      "epoch: 9/200 | step: 200/232 | trn loss: 4.5422 | val loss: 4.5000 | acc: 3.40\n",
      "epoch: 10/200 | step: 100/232 | trn loss: 4.5078 | val loss: 4.4547 | acc: 3.46\n",
      "epoch: 10/200 | step: 200/232 | trn loss: 4.4971 | val loss: 4.4608 | acc: 3.71\n",
      "epoch: 11/200 | step: 100/232 | trn loss: 4.4706 | val loss: 4.4349 | acc: 3.46\n",
      "epoch: 11/200 | step: 200/232 | trn loss: 4.4687 | val loss: 4.3436 | acc: 4.37\n",
      "epoch: 12/200 | step: 100/232 | trn loss: 4.3918 | val loss: 4.2867 | acc: 4.74\n",
      "epoch: 12/200 | step: 200/232 | trn loss: 4.3684 | val loss: 4.2321 | acc: 5.41\n",
      "epoch: 13/200 | step: 100/232 | trn loss: 4.3421 | val loss: 4.2216 | acc: 5.16\n",
      "epoch: 13/200 | step: 200/232 | trn loss: 4.3523 | val loss: 4.1653 | acc: 6.01\n",
      "epoch: 14/200 | step: 100/232 | trn loss: 4.2924 | val loss: 4.1393 | acc: 5.95\n",
      "epoch: 14/200 | step: 200/232 | trn loss: 4.2956 | val loss: 4.1306 | acc: 5.59\n",
      "epoch: 15/200 | step: 100/232 | trn loss: 4.2497 | val loss: 4.1293 | acc: 5.53\n",
      "epoch: 15/200 | step: 200/232 | trn loss: 4.2600 | val loss: 4.0681 | acc: 7.41\n",
      "epoch: 16/200 | step: 100/232 | trn loss: 4.2409 | val loss: 4.0411 | acc: 6.26\n",
      "epoch: 16/200 | step: 200/232 | trn loss: 4.1924 | val loss: 3.9898 | acc: 6.68\n",
      "epoch: 17/200 | step: 100/232 | trn loss: 4.1639 | val loss: 4.0262 | acc: 6.68\n",
      "epoch: 17/200 | step: 200/232 | trn loss: 4.2018 | val loss: 4.0637 | acc: 6.08\n",
      "epoch: 18/200 | step: 100/232 | trn loss: 4.1223 | val loss: 3.9531 | acc: 7.65\n",
      "epoch: 18/200 | step: 200/232 | trn loss: 4.1490 | val loss: 3.8923 | acc: 8.63\n",
      "epoch: 19/200 | step: 100/232 | trn loss: 4.1034 | val loss: 3.9181 | acc: 8.14\n",
      "epoch: 19/200 | step: 200/232 | trn loss: 4.0708 | val loss: 3.8607 | acc: 8.99\n",
      "epoch: 20/200 | step: 100/232 | trn loss: 4.0582 | val loss: 3.9101 | acc: 8.69\n",
      "epoch: 20/200 | step: 200/232 | trn loss: 4.0700 | val loss: 3.8280 | acc: 10.21\n",
      "epoch: 21/200 | step: 100/232 | trn loss: 4.0171 | val loss: 3.8436 | acc: 8.32\n",
      "epoch: 21/200 | step: 200/232 | trn loss: 4.0082 | val loss: 3.7670 | acc: 11.06\n",
      "epoch: 22/200 | step: 100/232 | trn loss: 3.9737 | val loss: 4.0301 | acc: 7.47\n",
      "epoch: 22/200 | step: 200/232 | trn loss: 3.9794 | val loss: 3.7277 | acc: 11.24\n",
      "epoch: 23/200 | step: 100/232 | trn loss: 3.9136 | val loss: 3.7999 | acc: 10.69\n",
      "epoch: 23/200 | step: 200/232 | trn loss: 3.9233 | val loss: 3.8100 | acc: 9.11\n",
      "epoch: 24/200 | step: 100/232 | trn loss: 3.8735 | val loss: 3.6067 | acc: 11.73\n",
      "epoch: 24/200 | step: 200/232 | trn loss: 3.9122 | val loss: 3.7375 | acc: 10.81\n",
      "epoch: 25/200 | step: 100/232 | trn loss: 3.8536 | val loss: 3.7143 | acc: 11.12\n",
      "epoch: 25/200 | step: 200/232 | trn loss: 3.8048 | val loss: 3.6826 | acc: 11.18\n",
      "epoch: 26/200 | step: 100/232 | trn loss: 3.7926 | val loss: 3.5771 | acc: 12.70\n",
      "epoch: 26/200 | step: 200/232 | trn loss: 3.8007 | val loss: 3.5255 | acc: 13.43\n",
      "epoch: 27/200 | step: 100/232 | trn loss: 3.7507 | val loss: 3.5300 | acc: 12.70\n",
      "epoch: 27/200 | step: 200/232 | trn loss: 3.7377 | val loss: 3.5920 | acc: 12.70\n",
      "epoch: 28/200 | step: 100/232 | trn loss: 3.6878 | val loss: 3.4724 | acc: 14.46\n",
      "epoch: 28/200 | step: 200/232 | trn loss: 3.6703 | val loss: 3.4304 | acc: 15.13\n",
      "epoch: 29/200 | step: 100/232 | trn loss: 3.6562 | val loss: 3.3913 | acc: 15.74\n",
      "epoch: 29/200 | step: 200/232 | trn loss: 3.6629 | val loss: 3.3301 | acc: 17.01\n",
      "epoch: 30/200 | step: 100/232 | trn loss: 3.6230 | val loss: 3.5793 | acc: 12.27\n",
      "epoch: 30/200 | step: 200/232 | trn loss: 3.5950 | val loss: 3.3722 | acc: 15.80\n",
      "epoch: 31/200 | step: 100/232 | trn loss: 3.5816 | val loss: 3.2496 | acc: 16.95\n",
      "epoch: 31/200 | step: 200/232 | trn loss: 3.5768 | val loss: 3.3886 | acc: 15.19\n",
      "epoch: 32/200 | step: 100/232 | trn loss: 3.5004 | val loss: 3.1940 | acc: 18.23\n",
      "epoch: 32/200 | step: 200/232 | trn loss: 3.5444 | val loss: 3.2841 | acc: 17.62\n",
      "epoch: 33/200 | step: 100/232 | trn loss: 3.4550 | val loss: 3.1723 | acc: 19.20\n",
      "epoch: 33/200 | step: 200/232 | trn loss: 3.4336 | val loss: 3.1806 | acc: 19.99\n",
      "epoch: 34/200 | step: 100/232 | trn loss: 3.4262 | val loss: 3.1129 | acc: 20.84\n",
      "epoch: 34/200 | step: 200/232 | trn loss: 3.3980 | val loss: 3.1861 | acc: 19.99\n",
      "epoch: 35/200 | step: 100/232 | trn loss: 3.4158 | val loss: 3.1324 | acc: 20.53\n",
      "epoch: 35/200 | step: 200/232 | trn loss: 3.3541 | val loss: 3.0824 | acc: 22.66\n",
      "epoch: 36/200 | step: 100/232 | trn loss: 3.3448 | val loss: 3.0474 | acc: 21.02\n",
      "epoch: 36/200 | step: 200/232 | trn loss: 3.3281 | val loss: 2.9454 | acc: 22.36\n",
      "epoch: 37/200 | step: 100/232 | trn loss: 3.2716 | val loss: 2.8643 | acc: 24.48\n",
      "epoch: 37/200 | step: 200/232 | trn loss: 3.2958 | val loss: 3.0484 | acc: 22.24\n",
      "epoch: 38/200 | step: 100/232 | trn loss: 3.1807 | val loss: 2.9606 | acc: 22.90\n",
      "epoch: 38/200 | step: 200/232 | trn loss: 3.2498 | val loss: 2.8346 | acc: 25.33\n",
      "epoch: 39/200 | step: 100/232 | trn loss: 3.1729 | val loss: 2.8121 | acc: 27.16\n",
      "epoch: 39/200 | step: 200/232 | trn loss: 3.1935 | val loss: 2.8586 | acc: 27.28\n",
      "epoch: 40/200 | step: 100/232 | trn loss: 3.1438 | val loss: 2.8144 | acc: 27.28\n",
      "epoch: 40/200 | step: 200/232 | trn loss: 3.1449 | val loss: 2.9344 | acc: 26.18\n",
      "epoch: 41/200 | step: 100/232 | trn loss: 3.0919 | val loss: 2.6751 | acc: 28.43\n",
      "epoch: 41/200 | step: 200/232 | trn loss: 3.1086 | val loss: 2.7604 | acc: 27.04\n",
      "epoch: 42/200 | step: 100/232 | trn loss: 3.0516 | val loss: 2.7175 | acc: 28.80\n",
      "epoch: 42/200 | step: 200/232 | trn loss: 3.0318 | val loss: 2.7059 | acc: 29.65\n",
      "epoch: 43/200 | step: 100/232 | trn loss: 2.9751 | val loss: 2.7652 | acc: 28.13\n",
      "epoch: 43/200 | step: 200/232 | trn loss: 2.9883 | val loss: 2.5991 | acc: 29.65\n",
      "epoch: 44/200 | step: 100/232 | trn loss: 2.8975 | val loss: 2.6005 | acc: 30.56\n",
      "epoch: 44/200 | step: 200/232 | trn loss: 2.9755 | val loss: 2.6395 | acc: 29.83\n",
      "epoch: 45/200 | step: 100/232 | trn loss: 2.8838 | val loss: 2.5037 | acc: 33.78\n",
      "epoch: 45/200 | step: 200/232 | trn loss: 2.9140 | val loss: 2.5456 | acc: 31.71\n",
      "epoch: 46/200 | step: 100/232 | trn loss: 2.9004 | val loss: 2.6063 | acc: 30.32\n",
      "epoch: 46/200 | step: 200/232 | trn loss: 2.8354 | val loss: 2.4780 | acc: 33.35\n",
      "epoch: 47/200 | step: 100/232 | trn loss: 2.8364 | val loss: 2.5002 | acc: 33.11\n",
      "epoch: 47/200 | step: 200/232 | trn loss: 2.7918 | val loss: 2.4451 | acc: 35.54\n",
      "epoch: 48/200 | step: 100/232 | trn loss: 2.7659 | val loss: 2.4702 | acc: 33.90\n",
      "epoch: 48/200 | step: 200/232 | trn loss: 2.7506 | val loss: 2.4258 | acc: 35.30\n",
      "epoch: 49/200 | step: 100/232 | trn loss: 2.7508 | val loss: 2.4431 | acc: 32.81\n",
      "epoch: 49/200 | step: 200/232 | trn loss: 2.7206 | val loss: 2.3294 | acc: 36.51\n",
      "epoch: 50/200 | step: 100/232 | trn loss: 2.7100 | val loss: 2.4920 | acc: 32.93\n",
      "epoch: 50/200 | step: 200/232 | trn loss: 2.6907 | val loss: 2.4091 | acc: 35.66\n",
      "epoch: 51/200 | step: 100/232 | trn loss: 2.6569 | val loss: 2.4098 | acc: 34.02\n",
      "epoch: 51/200 | step: 200/232 | trn loss: 2.6686 | val loss: 2.3796 | acc: 36.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 52/200 | step: 100/232 | trn loss: 2.6163 | val loss: 2.3002 | acc: 35.97\n",
      "epoch: 52/200 | step: 200/232 | trn loss: 2.6029 | val loss: 2.3638 | acc: 35.78\n",
      "epoch: 53/200 | step: 100/232 | trn loss: 2.5621 | val loss: 2.3141 | acc: 37.36\n",
      "epoch: 53/200 | step: 200/232 | trn loss: 2.5496 | val loss: 2.1391 | acc: 41.62\n",
      "epoch: 54/200 | step: 100/232 | trn loss: 2.4892 | val loss: 2.2752 | acc: 37.48\n",
      "epoch: 54/200 | step: 200/232 | trn loss: 2.5365 | val loss: 2.3514 | acc: 37.85\n",
      "epoch: 55/200 | step: 100/232 | trn loss: 2.4836 | val loss: 2.2217 | acc: 39.13\n",
      "epoch: 55/200 | step: 200/232 | trn loss: 2.5182 | val loss: 2.1618 | acc: 40.46\n",
      "epoch: 56/200 | step: 100/232 | trn loss: 2.4570 | val loss: 2.1727 | acc: 40.34\n",
      "epoch: 56/200 | step: 200/232 | trn loss: 2.4799 | val loss: 2.1387 | acc: 40.83\n",
      "epoch: 57/200 | step: 100/232 | trn loss: 2.4081 | val loss: 2.1421 | acc: 41.19\n",
      "epoch: 57/200 | step: 200/232 | trn loss: 2.4534 | val loss: 2.2062 | acc: 39.37\n",
      "epoch: 58/200 | step: 100/232 | trn loss: 2.4000 | val loss: 2.1086 | acc: 40.77\n",
      "epoch: 58/200 | step: 200/232 | trn loss: 2.3564 | val loss: 2.0767 | acc: 42.89\n",
      "epoch: 59/200 | step: 100/232 | trn loss: 2.4000 | val loss: 2.0530 | acc: 43.07\n",
      "epoch: 59/200 | step: 200/232 | trn loss: 2.3636 | val loss: 2.1067 | acc: 41.13\n",
      "epoch: 60/200 | step: 100/232 | trn loss: 2.3408 | val loss: 1.9909 | acc: 44.53\n",
      "epoch: 60/200 | step: 200/232 | trn loss: 2.3504 | val loss: 2.1551 | acc: 40.58\n",
      "epoch: 61/200 | step: 100/232 | trn loss: 2.3290 | val loss: 1.9876 | acc: 43.74\n",
      "epoch: 61/200 | step: 200/232 | trn loss: 2.2612 | val loss: 2.0622 | acc: 43.68\n",
      "epoch: 62/200 | step: 100/232 | trn loss: 2.2791 | val loss: 1.9073 | acc: 46.29\n",
      "epoch: 62/200 | step: 200/232 | trn loss: 2.2769 | val loss: 2.0329 | acc: 44.11\n",
      "epoch: 63/200 | step: 100/232 | trn loss: 2.2558 | val loss: 2.0024 | acc: 43.92\n",
      "epoch: 63/200 | step: 200/232 | trn loss: 2.2407 | val loss: 2.0872 | acc: 43.01\n",
      "epoch: 64/200 | step: 100/232 | trn loss: 2.2397 | val loss: 1.9941 | acc: 45.26\n",
      "epoch: 64/200 | step: 200/232 | trn loss: 2.2341 | val loss: 1.8793 | acc: 46.72\n",
      "epoch: 65/200 | step: 100/232 | trn loss: 2.1408 | val loss: 1.9561 | acc: 46.35\n",
      "epoch: 65/200 | step: 200/232 | trn loss: 2.2142 | val loss: 2.0604 | acc: 43.50\n",
      "epoch: 66/200 | step: 100/232 | trn loss: 2.1649 | val loss: 1.9453 | acc: 46.60\n",
      "epoch: 66/200 | step: 200/232 | trn loss: 2.1724 | val loss: 1.9103 | acc: 46.17\n",
      "epoch: 67/200 | step: 100/232 | trn loss: 2.1686 | val loss: 1.9018 | acc: 46.35\n",
      "epoch: 67/200 | step: 200/232 | trn loss: 2.1650 | val loss: 1.9401 | acc: 44.90\n",
      "epoch: 68/200 | step: 100/232 | trn loss: 2.0944 | val loss: 1.8648 | acc: 47.02\n",
      "epoch: 68/200 | step: 200/232 | trn loss: 2.1464 | val loss: 1.9348 | acc: 45.32\n",
      "epoch: 69/200 | step: 100/232 | trn loss: 2.0798 | val loss: 1.8094 | acc: 46.23\n",
      "epoch: 69/200 | step: 200/232 | trn loss: 2.1050 | val loss: 1.8947 | acc: 46.29\n",
      "epoch: 70/200 | step: 100/232 | trn loss: 2.0795 | val loss: 1.7892 | acc: 48.30\n",
      "epoch: 70/200 | step: 200/232 | trn loss: 2.1030 | val loss: 1.8137 | acc: 48.30\n",
      "epoch: 71/200 | step: 100/232 | trn loss: 2.0059 | val loss: 1.9065 | acc: 47.57\n",
      "epoch: 71/200 | step: 200/232 | trn loss: 2.0206 | val loss: 1.8533 | acc: 47.02\n",
      "epoch: 72/200 | step: 100/232 | trn loss: 2.0446 | val loss: 1.7962 | acc: 49.57\n",
      "epoch: 72/200 | step: 200/232 | trn loss: 2.0352 | val loss: 1.7699 | acc: 50.06\n",
      "epoch: 73/200 | step: 100/232 | trn loss: 1.9771 | val loss: 1.8286 | acc: 49.21\n",
      "epoch: 73/200 | step: 200/232 | trn loss: 1.9969 | val loss: 1.8066 | acc: 48.78\n",
      "epoch: 74/200 | step: 100/232 | trn loss: 1.9743 | val loss: 1.6903 | acc: 52.13\n",
      "epoch: 74/200 | step: 200/232 | trn loss: 2.0413 | val loss: 1.8378 | acc: 48.42\n",
      "epoch: 75/200 | step: 100/232 | trn loss: 1.9174 | val loss: 1.7212 | acc: 51.46\n",
      "epoch: 75/200 | step: 200/232 | trn loss: 2.0178 | val loss: 1.7211 | acc: 50.12\n",
      "epoch: 76/200 | step: 100/232 | trn loss: 1.9744 | val loss: 1.7464 | acc: 49.64\n",
      "epoch: 76/200 | step: 200/232 | trn loss: 1.9726 | val loss: 1.8304 | acc: 50.85\n",
      "epoch: 77/200 | step: 100/232 | trn loss: 1.9501 | val loss: 1.7456 | acc: 51.03\n",
      "epoch: 77/200 | step: 200/232 | trn loss: 1.9513 | val loss: 1.7578 | acc: 50.30\n",
      "epoch: 78/200 | step: 100/232 | trn loss: 1.9021 | val loss: 1.8201 | acc: 49.33\n",
      "epoch: 78/200 | step: 200/232 | trn loss: 1.9420 | val loss: 1.8367 | acc: 49.51\n",
      "epoch: 79/200 | step: 100/232 | trn loss: 1.8950 | val loss: 1.7732 | acc: 50.67\n",
      "epoch: 79/200 | step: 200/232 | trn loss: 1.8987 | val loss: 1.6812 | acc: 51.22\n",
      "epoch: 80/200 | step: 100/232 | trn loss: 1.8642 | val loss: 1.6101 | acc: 53.71\n",
      "epoch: 80/200 | step: 200/232 | trn loss: 1.9241 | val loss: 1.7101 | acc: 49.82\n",
      "epoch: 81/200 | step: 100/232 | trn loss: 1.8357 | val loss: 1.7139 | acc: 52.49\n",
      "epoch: 81/200 | step: 200/232 | trn loss: 1.8512 | val loss: 1.6602 | acc: 53.22\n",
      "epoch: 82/200 | step: 100/232 | trn loss: 1.8659 | val loss: 1.6729 | acc: 52.07\n",
      "epoch: 82/200 | step: 200/232 | trn loss: 1.8096 | val loss: 1.7346 | acc: 51.82\n",
      "epoch: 83/200 | step: 100/232 | trn loss: 1.8355 | val loss: 1.6768 | acc: 55.22\n",
      "epoch: 83/200 | step: 200/232 | trn loss: 1.8304 | val loss: 1.6151 | acc: 53.22\n",
      "epoch: 84/200 | step: 100/232 | trn loss: 1.7647 | val loss: 1.6877 | acc: 53.22\n",
      "epoch: 84/200 | step: 200/232 | trn loss: 1.8254 | val loss: 1.7008 | acc: 52.79\n",
      "epoch: 85/200 | step: 100/232 | trn loss: 1.7826 | val loss: 1.7453 | acc: 52.07\n",
      "epoch: 85/200 | step: 200/232 | trn loss: 1.8060 | val loss: 1.8019 | acc: 51.03\n",
      "epoch: 86/200 | step: 100/232 | trn loss: 1.7733 | val loss: 1.6817 | acc: 53.16\n",
      "epoch: 86/200 | step: 200/232 | trn loss: 1.7905 | val loss: 1.6498 | acc: 53.95\n",
      "epoch: 87/200 | step: 100/232 | trn loss: 1.7483 | val loss: 1.6609 | acc: 55.10\n",
      "epoch: 87/200 | step: 200/232 | trn loss: 1.7215 | val loss: 1.7067 | acc: 52.73\n",
      "epoch: 88/200 | step: 100/232 | trn loss: 1.7817 | val loss: 1.5853 | acc: 54.62\n",
      "epoch: 88/200 | step: 200/232 | trn loss: 1.7553 | val loss: 1.5818 | acc: 54.25\n",
      "epoch: 89/200 | step: 100/232 | trn loss: 1.7254 | val loss: 1.7185 | acc: 52.25\n",
      "epoch: 89/200 | step: 200/232 | trn loss: 1.7482 | val loss: 1.6071 | acc: 56.14\n",
      "epoch: 90/200 | step: 100/232 | trn loss: 1.7053 | val loss: 1.6073 | acc: 54.56\n",
      "epoch: 90/200 | step: 200/232 | trn loss: 1.7153 | val loss: 1.6082 | acc: 54.25\n",
      "epoch: 91/200 | step: 100/232 | trn loss: 1.6951 | val loss: 1.6105 | acc: 53.89\n",
      "epoch: 91/200 | step: 200/232 | trn loss: 1.7608 | val loss: 1.5501 | acc: 56.01\n",
      "epoch: 92/200 | step: 100/232 | trn loss: 1.7118 | val loss: 1.5863 | acc: 54.01\n",
      "epoch: 92/200 | step: 200/232 | trn loss: 1.7230 | val loss: 1.5890 | acc: 56.14\n",
      "epoch: 93/200 | step: 100/232 | trn loss: 1.6689 | val loss: 1.6474 | acc: 55.22\n",
      "epoch: 93/200 | step: 200/232 | trn loss: 1.6596 | val loss: 1.6667 | acc: 54.62\n",
      "epoch: 94/200 | step: 100/232 | trn loss: 1.6529 | val loss: 1.6079 | acc: 55.35\n",
      "epoch: 94/200 | step: 200/232 | trn loss: 1.6968 | val loss: 1.6176 | acc: 54.31\n",
      "epoch: 95/200 | step: 100/232 | trn loss: 1.6487 | val loss: 1.6245 | acc: 56.08\n",
      "epoch: 95/200 | step: 200/232 | trn loss: 1.6462 | val loss: 1.6686 | acc: 54.50\n",
      "epoch: 96/200 | step: 100/232 | trn loss: 1.6398 | val loss: 1.5690 | acc: 56.74\n",
      "epoch: 96/200 | step: 200/232 | trn loss: 1.6392 | val loss: 1.5749 | acc: 57.11\n",
      "epoch: 97/200 | step: 100/232 | trn loss: 1.6304 | val loss: 1.6075 | acc: 56.87\n",
      "epoch: 97/200 | step: 200/232 | trn loss: 1.6281 | val loss: 1.5825 | acc: 56.20\n",
      "epoch: 98/200 | step: 100/232 | trn loss: 1.6160 | val loss: 1.6100 | acc: 54.37\n",
      "epoch: 98/200 | step: 200/232 | trn loss: 1.6204 | val loss: 1.6090 | acc: 55.35\n",
      "epoch: 99/200 | step: 100/232 | trn loss: 1.6082 | val loss: 1.5206 | acc: 58.02\n",
      "epoch: 99/200 | step: 200/232 | trn loss: 1.6233 | val loss: 1.6352 | acc: 55.71\n",
      "epoch: 100/200 | step: 100/232 | trn loss: 1.5870 | val loss: 1.6073 | acc: 56.56\n",
      "epoch: 100/200 | step: 200/232 | trn loss: 1.6083 | val loss: 1.6007 | acc: 56.44\n",
      "epoch: 101/200 | step: 100/232 | trn loss: 1.4206 | val loss: 1.3388 | acc: 62.15\n",
      "epoch: 101/200 | step: 200/232 | trn loss: 1.2774 | val loss: 1.3347 | acc: 61.54\n",
      "epoch: 102/200 | step: 100/232 | trn loss: 1.2579 | val loss: 1.3419 | acc: 62.76\n",
      "epoch: 102/200 | step: 200/232 | trn loss: 1.2708 | val loss: 1.3129 | acc: 63.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 103/200 | step: 100/232 | trn loss: 1.2179 | val loss: 1.3284 | acc: 62.58\n",
      "epoch: 103/200 | step: 200/232 | trn loss: 1.2540 | val loss: 1.3391 | acc: 63.55\n",
      "epoch: 104/200 | step: 100/232 | trn loss: 1.1992 | val loss: 1.2838 | acc: 64.52\n",
      "epoch: 104/200 | step: 200/232 | trn loss: 1.2222 | val loss: 1.2981 | acc: 64.22\n",
      "epoch: 105/200 | step: 100/232 | trn loss: 1.1784 | val loss: 1.3254 | acc: 62.94\n",
      "epoch: 105/200 | step: 200/232 | trn loss: 1.1761 | val loss: 1.2841 | acc: 64.82\n",
      "epoch: 106/200 | step: 100/232 | trn loss: 1.1442 | val loss: 1.3339 | acc: 63.97\n",
      "epoch: 106/200 | step: 200/232 | trn loss: 1.1599 | val loss: 1.3042 | acc: 63.73\n",
      "epoch: 107/200 | step: 100/232 | trn loss: 1.1642 | val loss: 1.3196 | acc: 63.67\n",
      "epoch: 107/200 | step: 200/232 | trn loss: 1.1657 | val loss: 1.2906 | acc: 64.76\n",
      "epoch: 108/200 | step: 100/232 | trn loss: 1.1119 | val loss: 1.2993 | acc: 65.49\n",
      "epoch: 108/200 | step: 200/232 | trn loss: 1.1557 | val loss: 1.2958 | acc: 64.34\n",
      "epoch: 109/200 | step: 100/232 | trn loss: 1.1459 | val loss: 1.2770 | acc: 64.76\n",
      "epoch: 109/200 | step: 200/232 | trn loss: 1.1662 | val loss: 1.2869 | acc: 65.07\n",
      "epoch: 110/200 | step: 100/232 | trn loss: 1.1294 | val loss: 1.2937 | acc: 64.70\n",
      "epoch: 110/200 | step: 200/232 | trn loss: 1.1481 | val loss: 1.2689 | acc: 65.43\n",
      "epoch: 111/200 | step: 100/232 | trn loss: 1.1571 | val loss: 1.2937 | acc: 65.49\n",
      "epoch: 111/200 | step: 200/232 | trn loss: 1.0954 | val loss: 1.2525 | acc: 65.37\n",
      "epoch: 112/200 | step: 100/232 | trn loss: 1.1300 | val loss: 1.2871 | acc: 66.46\n",
      "epoch: 112/200 | step: 200/232 | trn loss: 1.1021 | val loss: 1.2756 | acc: 64.16\n",
      "epoch: 113/200 | step: 100/232 | trn loss: 1.1394 | val loss: 1.2929 | acc: 65.01\n",
      "epoch: 113/200 | step: 200/232 | trn loss: 1.1052 | val loss: 1.3037 | acc: 65.25\n",
      "epoch: 114/200 | step: 100/232 | trn loss: 1.1004 | val loss: 1.2712 | acc: 64.70\n",
      "epoch: 114/200 | step: 200/232 | trn loss: 1.0956 | val loss: 1.3063 | acc: 64.82\n",
      "epoch: 115/200 | step: 100/232 | trn loss: 1.0971 | val loss: 1.2808 | acc: 64.88\n",
      "epoch: 115/200 | step: 200/232 | trn loss: 1.0815 | val loss: 1.3223 | acc: 63.24\n",
      "epoch: 116/200 | step: 100/232 | trn loss: 1.1116 | val loss: 1.2964 | acc: 66.16\n",
      "epoch: 116/200 | step: 200/232 | trn loss: 1.1035 | val loss: 1.2823 | acc: 65.49\n",
      "epoch: 117/200 | step: 100/232 | trn loss: 1.0591 | val loss: 1.2759 | acc: 65.61\n",
      "epoch: 117/200 | step: 200/232 | trn loss: 1.0738 | val loss: 1.2958 | acc: 65.49\n",
      "epoch: 118/200 | step: 100/232 | trn loss: 1.0608 | val loss: 1.2699 | acc: 65.80\n",
      "epoch: 118/200 | step: 200/232 | trn loss: 1.1143 | val loss: 1.2758 | acc: 64.95\n",
      "epoch: 119/200 | step: 100/232 | trn loss: 1.1064 | val loss: 1.3253 | acc: 65.31\n",
      "epoch: 119/200 | step: 200/232 | trn loss: 1.0777 | val loss: 1.2895 | acc: 65.61\n",
      "epoch: 120/200 | step: 100/232 | trn loss: 1.0952 | val loss: 1.2749 | acc: 65.01\n",
      "epoch: 120/200 | step: 200/232 | trn loss: 1.0682 | val loss: 1.3098 | acc: 64.03\n",
      "epoch: 121/200 | step: 100/232 | trn loss: 1.0548 | val loss: 1.3207 | acc: 64.82\n",
      "epoch: 121/200 | step: 200/232 | trn loss: 1.0918 | val loss: 1.3093 | acc: 65.01\n",
      "epoch: 122/200 | step: 100/232 | trn loss: 1.0352 | val loss: 1.3460 | acc: 64.09\n",
      "epoch: 122/200 | step: 200/232 | trn loss: 1.0774 | val loss: 1.3059 | acc: 65.07\n",
      "epoch: 123/200 | step: 100/232 | trn loss: 1.0757 | val loss: 1.2886 | acc: 66.40\n",
      "epoch: 123/200 | step: 200/232 | trn loss: 1.0691 | val loss: 1.3331 | acc: 65.31\n",
      "epoch: 124/200 | step: 100/232 | trn loss: 1.0827 | val loss: 1.2702 | acc: 66.59\n",
      "epoch: 124/200 | step: 200/232 | trn loss: 1.0486 | val loss: 1.2971 | acc: 65.49\n",
      "epoch: 125/200 | step: 100/232 | trn loss: 1.0564 | val loss: 1.2976 | acc: 65.67\n",
      "epoch: 125/200 | step: 200/232 | trn loss: 1.0654 | val loss: 1.3275 | acc: 65.80\n",
      "epoch: 126/200 | step: 100/232 | trn loss: 1.0519 | val loss: 1.3107 | acc: 64.46\n",
      "epoch: 126/200 | step: 200/232 | trn loss: 1.0361 | val loss: 1.3051 | acc: 66.59\n",
      "epoch: 127/200 | step: 100/232 | trn loss: 1.0135 | val loss: 1.3327 | acc: 64.88\n",
      "epoch: 127/200 | step: 200/232 | trn loss: 1.0625 | val loss: 1.2975 | acc: 64.88\n",
      "epoch: 128/200 | step: 100/232 | trn loss: 1.0527 | val loss: 1.3038 | acc: 66.46\n",
      "epoch: 128/200 | step: 200/232 | trn loss: 1.0416 | val loss: 1.2666 | acc: 65.92\n",
      "epoch: 129/200 | step: 100/232 | trn loss: 1.0148 | val loss: 1.2577 | acc: 66.52\n",
      "epoch: 129/200 | step: 200/232 | trn loss: 1.0239 | val loss: 1.2740 | acc: 65.67\n",
      "epoch: 130/200 | step: 100/232 | trn loss: 1.0048 | val loss: 1.3001 | acc: 64.76\n",
      "epoch: 130/200 | step: 200/232 | trn loss: 1.0407 | val loss: 1.3042 | acc: 66.10\n",
      "epoch: 131/200 | step: 100/232 | trn loss: 1.0115 | val loss: 1.2934 | acc: 65.31\n",
      "epoch: 131/200 | step: 200/232 | trn loss: 1.0302 | val loss: 1.2773 | acc: 65.98\n",
      "epoch: 132/200 | step: 100/232 | trn loss: 1.0344 | val loss: 1.3130 | acc: 65.80\n",
      "epoch: 132/200 | step: 200/232 | trn loss: 1.0102 | val loss: 1.2755 | acc: 66.59\n",
      "epoch: 133/200 | step: 100/232 | trn loss: 1.0158 | val loss: 1.2782 | acc: 66.22\n",
      "epoch: 133/200 | step: 200/232 | trn loss: 0.9863 | val loss: 1.2730 | acc: 66.16\n",
      "epoch: 134/200 | step: 100/232 | trn loss: 0.9912 | val loss: 1.2756 | acc: 67.25\n",
      "epoch: 134/200 | step: 200/232 | trn loss: 1.0216 | val loss: 1.3023 | acc: 66.28\n",
      "epoch: 135/200 | step: 100/232 | trn loss: 1.0325 | val loss: 1.2998 | acc: 66.40\n",
      "epoch: 135/200 | step: 200/232 | trn loss: 0.9764 | val loss: 1.3192 | acc: 65.86\n",
      "epoch: 136/200 | step: 100/232 | trn loss: 1.0007 | val loss: 1.3077 | acc: 65.55\n",
      "epoch: 136/200 | step: 200/232 | trn loss: 1.0123 | val loss: 1.2851 | acc: 65.61\n",
      "epoch: 137/200 | step: 100/232 | trn loss: 1.0448 | val loss: 1.2567 | acc: 66.71\n",
      "epoch: 137/200 | step: 200/232 | trn loss: 0.9942 | val loss: 1.2723 | acc: 65.92\n",
      "epoch: 138/200 | step: 100/232 | trn loss: 1.0269 | val loss: 1.2972 | acc: 65.98\n",
      "epoch: 138/200 | step: 200/232 | trn loss: 0.9931 | val loss: 1.3428 | acc: 65.86\n",
      "epoch: 139/200 | step: 100/232 | trn loss: 0.9733 | val loss: 1.2997 | acc: 65.55\n",
      "epoch: 139/200 | step: 200/232 | trn loss: 0.9784 | val loss: 1.3096 | acc: 66.65\n",
      "epoch: 140/200 | step: 100/232 | trn loss: 0.9787 | val loss: 1.3055 | acc: 66.71\n",
      "epoch: 140/200 | step: 200/232 | trn loss: 0.9963 | val loss: 1.2883 | acc: 65.86\n",
      "epoch: 141/200 | step: 100/232 | trn loss: 0.9938 | val loss: 1.3193 | acc: 65.61\n",
      "epoch: 141/200 | step: 200/232 | trn loss: 0.9880 | val loss: 1.2884 | acc: 65.49\n",
      "epoch: 142/200 | step: 100/232 | trn loss: 0.9828 | val loss: 1.3237 | acc: 65.67\n",
      "epoch: 142/200 | step: 200/232 | trn loss: 0.9861 | val loss: 1.3001 | acc: 66.52\n",
      "epoch: 143/200 | step: 100/232 | trn loss: 1.0055 | val loss: 1.2840 | acc: 66.46\n",
      "epoch: 143/200 | step: 200/232 | trn loss: 0.9529 | val loss: 1.3044 | acc: 65.49\n",
      "epoch: 144/200 | step: 100/232 | trn loss: 1.0033 | val loss: 1.2993 | acc: 66.40\n",
      "epoch: 144/200 | step: 200/232 | trn loss: 0.9936 | val loss: 1.2887 | acc: 66.52\n",
      "epoch: 145/200 | step: 100/232 | trn loss: 0.9670 | val loss: 1.3011 | acc: 66.10\n",
      "epoch: 145/200 | step: 200/232 | trn loss: 0.9575 | val loss: 1.3062 | acc: 66.83\n",
      "epoch: 146/200 | step: 100/232 | trn loss: 0.9982 | val loss: 1.2911 | acc: 66.28\n",
      "epoch: 146/200 | step: 200/232 | trn loss: 0.9895 | val loss: 1.3004 | acc: 65.80\n",
      "epoch: 147/200 | step: 100/232 | trn loss: 0.9684 | val loss: 1.3083 | acc: 66.34\n",
      "epoch: 147/200 | step: 200/232 | trn loss: 0.9812 | val loss: 1.3181 | acc: 66.40\n",
      "epoch: 148/200 | step: 100/232 | trn loss: 0.9664 | val loss: 1.3024 | acc: 65.43\n",
      "epoch: 148/200 | step: 200/232 | trn loss: 0.9481 | val loss: 1.3230 | acc: 65.43\n",
      "epoch: 149/200 | step: 100/232 | trn loss: 0.9312 | val loss: 1.3307 | acc: 65.86\n",
      "epoch: 149/200 | step: 200/232 | trn loss: 0.9734 | val loss: 1.3365 | acc: 66.16\n",
      "epoch: 150/200 | step: 100/232 | trn loss: 0.9493 | val loss: 1.3340 | acc: 65.67\n",
      "epoch: 150/200 | step: 200/232 | trn loss: 0.9607 | val loss: 1.3349 | acc: 66.71\n",
      "epoch: 151/200 | step: 100/232 | trn loss: 0.9391 | val loss: 1.3144 | acc: 66.46\n",
      "epoch: 151/200 | step: 200/232 | trn loss: 0.9185 | val loss: 1.3120 | acc: 67.25\n",
      "epoch: 152/200 | step: 100/232 | trn loss: 0.9160 | val loss: 1.2852 | acc: 67.98\n",
      "epoch: 152/200 | step: 200/232 | trn loss: 0.9477 | val loss: 1.3049 | acc: 65.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 153/200 | step: 100/232 | trn loss: 0.9214 | val loss: 1.3187 | acc: 66.34\n",
      "epoch: 153/200 | step: 200/232 | trn loss: 0.9120 | val loss: 1.3118 | acc: 66.52\n",
      "epoch: 154/200 | step: 100/232 | trn loss: 0.9243 | val loss: 1.3133 | acc: 66.46\n",
      "epoch: 154/200 | step: 200/232 | trn loss: 0.9164 | val loss: 1.3135 | acc: 66.59\n",
      "epoch: 155/200 | step: 100/232 | trn loss: 0.9727 | val loss: 1.3169 | acc: 66.04\n",
      "epoch: 155/200 | step: 200/232 | trn loss: 0.9120 | val loss: 1.2913 | acc: 67.44\n",
      "epoch: 156/200 | step: 100/232 | trn loss: 0.9266 | val loss: 1.2759 | acc: 67.74\n",
      "epoch: 156/200 | step: 200/232 | trn loss: 0.9152 | val loss: 1.2876 | acc: 66.83\n",
      "epoch: 157/200 | step: 100/232 | trn loss: 0.9189 | val loss: 1.3076 | acc: 66.46\n",
      "epoch: 157/200 | step: 200/232 | trn loss: 0.9393 | val loss: 1.3204 | acc: 66.65\n",
      "epoch: 158/200 | step: 100/232 | trn loss: 0.9231 | val loss: 1.3249 | acc: 66.40\n",
      "epoch: 158/200 | step: 200/232 | trn loss: 0.8948 | val loss: 1.3021 | acc: 66.89\n",
      "epoch: 159/200 | step: 100/232 | trn loss: 0.9106 | val loss: 1.3263 | acc: 65.61\n",
      "epoch: 159/200 | step: 200/232 | trn loss: 0.9242 | val loss: 1.3220 | acc: 66.52\n",
      "epoch: 160/200 | step: 100/232 | trn loss: 0.9123 | val loss: 1.3210 | acc: 65.98\n",
      "epoch: 160/200 | step: 200/232 | trn loss: 0.9009 | val loss: 1.3135 | acc: 66.52\n",
      "epoch: 161/200 | step: 100/232 | trn loss: 0.9146 | val loss: 1.3198 | acc: 67.07\n",
      "epoch: 161/200 | step: 200/232 | trn loss: 0.9324 | val loss: 1.3048 | acc: 66.65\n",
      "epoch: 162/200 | step: 100/232 | trn loss: 0.9208 | val loss: 1.3063 | acc: 66.40\n",
      "epoch: 162/200 | step: 200/232 | trn loss: 0.9180 | val loss: 1.2888 | acc: 67.80\n",
      "epoch: 163/200 | step: 100/232 | trn loss: 0.9131 | val loss: 1.2641 | acc: 67.62\n",
      "epoch: 163/200 | step: 200/232 | trn loss: 0.9241 | val loss: 1.3015 | acc: 66.95\n",
      "epoch: 164/200 | step: 100/232 | trn loss: 0.9053 | val loss: 1.3164 | acc: 66.16\n",
      "epoch: 164/200 | step: 200/232 | trn loss: 0.9044 | val loss: 1.3116 | acc: 67.13\n",
      "epoch: 165/200 | step: 100/232 | trn loss: 0.9293 | val loss: 1.3401 | acc: 66.46\n",
      "epoch: 165/200 | step: 200/232 | trn loss: 0.9182 | val loss: 1.3232 | acc: 66.83\n",
      "epoch: 166/200 | step: 100/232 | trn loss: 0.9415 | val loss: 1.2993 | acc: 67.31\n",
      "epoch: 166/200 | step: 200/232 | trn loss: 0.8861 | val loss: 1.3400 | acc: 66.46\n",
      "epoch: 167/200 | step: 100/232 | trn loss: 0.8957 | val loss: 1.2958 | acc: 66.34\n",
      "epoch: 167/200 | step: 200/232 | trn loss: 0.9059 | val loss: 1.3058 | acc: 67.31\n",
      "epoch: 168/200 | step: 100/232 | trn loss: 0.8948 | val loss: 1.2910 | acc: 67.74\n",
      "epoch: 168/200 | step: 200/232 | trn loss: 0.9242 | val loss: 1.3135 | acc: 67.07\n",
      "epoch: 169/200 | step: 100/232 | trn loss: 0.9306 | val loss: 1.2832 | acc: 67.80\n",
      "epoch: 169/200 | step: 200/232 | trn loss: 0.9045 | val loss: 1.2833 | acc: 66.65\n",
      "epoch: 170/200 | step: 100/232 | trn loss: 0.9188 | val loss: 1.2689 | acc: 68.47\n",
      "epoch: 170/200 | step: 200/232 | trn loss: 0.9236 | val loss: 1.3116 | acc: 67.44\n",
      "epoch: 171/200 | step: 100/232 | trn loss: 0.8896 | val loss: 1.3037 | acc: 66.89\n",
      "epoch: 171/200 | step: 200/232 | trn loss: 0.9118 | val loss: 1.3211 | acc: 66.28\n",
      "epoch: 172/200 | step: 100/232 | trn loss: 0.9155 | val loss: 1.3160 | acc: 67.50\n",
      "epoch: 172/200 | step: 200/232 | trn loss: 0.9244 | val loss: 1.2889 | acc: 66.71\n",
      "epoch: 173/200 | step: 100/232 | trn loss: 0.9163 | val loss: 1.2971 | acc: 67.01\n",
      "epoch: 173/200 | step: 200/232 | trn loss: 0.8844 | val loss: 1.3445 | acc: 67.01\n",
      "epoch: 174/200 | step: 100/232 | trn loss: 0.9097 | val loss: 1.3157 | acc: 67.56\n",
      "epoch: 174/200 | step: 200/232 | trn loss: 0.9238 | val loss: 1.3052 | acc: 67.13\n",
      "epoch: 175/200 | step: 100/232 | trn loss: 0.9077 | val loss: 1.3030 | acc: 67.01\n",
      "epoch: 175/200 | step: 200/232 | trn loss: 0.8887 | val loss: 1.3037 | acc: 66.77\n",
      "epoch: 176/200 | step: 100/232 | trn loss: 0.9080 | val loss: 1.3495 | acc: 66.16\n",
      "epoch: 176/200 | step: 200/232 | trn loss: 0.9034 | val loss: 1.2985 | acc: 67.62\n",
      "epoch: 177/200 | step: 100/232 | trn loss: 0.9018 | val loss: 1.2753 | acc: 67.92\n",
      "epoch: 177/200 | step: 200/232 | trn loss: 0.8876 | val loss: 1.3088 | acc: 66.10\n",
      "epoch: 178/200 | step: 100/232 | trn loss: 0.9080 | val loss: 1.2730 | acc: 67.01\n",
      "epoch: 178/200 | step: 200/232 | trn loss: 0.9029 | val loss: 1.3029 | acc: 66.65\n",
      "epoch: 179/200 | step: 100/232 | trn loss: 0.9233 | val loss: 1.2848 | acc: 66.71\n",
      "epoch: 179/200 | step: 200/232 | trn loss: 0.8796 | val loss: 1.3139 | acc: 66.28\n",
      "epoch: 180/200 | step: 100/232 | trn loss: 0.8853 | val loss: 1.2933 | acc: 67.50\n",
      "epoch: 180/200 | step: 200/232 | trn loss: 0.9103 | val loss: 1.3277 | acc: 66.89\n",
      "epoch: 181/200 | step: 100/232 | trn loss: 0.9089 | val loss: 1.2965 | acc: 67.50\n",
      "epoch: 181/200 | step: 200/232 | trn loss: 0.8906 | val loss: 1.3012 | acc: 67.31\n",
      "epoch: 182/200 | step: 100/232 | trn loss: 0.9382 | val loss: 1.3169 | acc: 67.01\n",
      "epoch: 182/200 | step: 200/232 | trn loss: 0.9242 | val loss: 1.3033 | acc: 66.65\n",
      "epoch: 183/200 | step: 100/232 | trn loss: 0.9324 | val loss: 1.3428 | acc: 65.92\n",
      "epoch: 183/200 | step: 200/232 | trn loss: 0.9194 | val loss: 1.3385 | acc: 66.16\n",
      "epoch: 184/200 | step: 100/232 | trn loss: 0.9046 | val loss: 1.3039 | acc: 66.65\n",
      "epoch: 184/200 | step: 200/232 | trn loss: 0.9168 | val loss: 1.2941 | acc: 67.19\n",
      "epoch: 185/200 | step: 100/232 | trn loss: 0.9196 | val loss: 1.2907 | acc: 67.62\n",
      "epoch: 185/200 | step: 200/232 | trn loss: 0.9045 | val loss: 1.3118 | acc: 67.01\n",
      "epoch: 186/200 | step: 100/232 | trn loss: 0.9059 | val loss: 1.3274 | acc: 67.50\n",
      "epoch: 186/200 | step: 200/232 | trn loss: 0.9027 | val loss: 1.3128 | acc: 65.92\n",
      "epoch: 187/200 | step: 100/232 | trn loss: 0.9036 | val loss: 1.3032 | acc: 66.95\n",
      "epoch: 187/200 | step: 200/232 | trn loss: 0.9197 | val loss: 1.3168 | acc: 67.25\n",
      "epoch: 188/200 | step: 100/232 | trn loss: 0.9010 | val loss: 1.2977 | acc: 66.95\n",
      "epoch: 188/200 | step: 200/232 | trn loss: 0.9143 | val loss: 1.3101 | acc: 67.13\n",
      "epoch: 189/200 | step: 100/232 | trn loss: 0.9176 | val loss: 1.3113 | acc: 66.71\n",
      "epoch: 189/200 | step: 200/232 | trn loss: 0.8971 | val loss: 1.3181 | acc: 66.83\n",
      "epoch: 190/200 | step: 100/232 | trn loss: 0.8787 | val loss: 1.3242 | acc: 66.04\n",
      "epoch: 190/200 | step: 200/232 | trn loss: 0.8989 | val loss: 1.3253 | acc: 66.40\n",
      "epoch: 191/200 | step: 100/232 | trn loss: 0.8923 | val loss: 1.3349 | acc: 66.46\n",
      "epoch: 191/200 | step: 200/232 | trn loss: 0.8889 | val loss: 1.2823 | acc: 67.31\n",
      "epoch: 192/200 | step: 100/232 | trn loss: 0.9341 | val loss: 1.2925 | acc: 67.31\n",
      "epoch: 192/200 | step: 200/232 | trn loss: 0.8977 | val loss: 1.3099 | acc: 67.01\n",
      "epoch: 193/200 | step: 100/232 | trn loss: 0.8711 | val loss: 1.2930 | acc: 68.41\n",
      "epoch: 193/200 | step: 200/232 | trn loss: 0.8970 | val loss: 1.2741 | acc: 66.46\n",
      "epoch: 194/200 | step: 100/232 | trn loss: 0.8758 | val loss: 1.3017 | acc: 66.34\n",
      "epoch: 194/200 | step: 200/232 | trn loss: 0.9222 | val loss: 1.2669 | acc: 66.95\n",
      "epoch: 195/200 | step: 100/232 | trn loss: 0.9515 | val loss: 1.3102 | acc: 66.40\n",
      "epoch: 195/200 | step: 200/232 | trn loss: 0.9096 | val loss: 1.3181 | acc: 66.34\n",
      "epoch: 196/200 | step: 100/232 | trn loss: 0.9041 | val loss: 1.2885 | acc: 66.65\n",
      "epoch: 196/200 | step: 200/232 | trn loss: 0.9006 | val loss: 1.3110 | acc: 66.95\n",
      "epoch: 197/200 | step: 100/232 | trn loss: 0.8960 | val loss: 1.2835 | acc: 67.19\n",
      "epoch: 197/200 | step: 200/232 | trn loss: 0.9182 | val loss: 1.3424 | acc: 66.59\n",
      "epoch: 198/200 | step: 100/232 | trn loss: 0.9000 | val loss: 1.3278 | acc: 66.71\n",
      "epoch: 198/200 | step: 200/232 | trn loss: 0.9087 | val loss: 1.2993 | acc: 66.59\n",
      "epoch: 199/200 | step: 100/232 | trn loss: 0.9087 | val loss: 1.3230 | acc: 68.10\n",
      "epoch: 199/200 | step: 200/232 | trn loss: 0.9133 | val loss: 1.3282 | acc: 67.92\n",
      "epoch: 200/200 | step: 100/232 | trn loss: 0.9165 | val loss: 1.3103 | acc: 67.31\n",
      "epoch: 200/200 | step: 200/232 | trn loss: 0.9029 | val loss: 1.3311 | acc: 66.59\n",
      "training finished!\n",
      "model saved!\n"
     ]
    }
   ],
   "source": [
    "# loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# backpropagation method\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(vgg16.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0002)\n",
    "# hyper-parameters\n",
    "num_epochs = 200\n",
    "num_batches = len(trainLoader)\n",
    "\n",
    "trn_loss_list = []\n",
    "val_loss_list = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    adjust_learning_rate(optimizer,epoch)\n",
    "    trn_loss = 0.0\n",
    "    for i, data in enumerate(trainLoader):\n",
    "        x, label = data\n",
    "        x = x.to(device)\n",
    "        label = label.to(device)\n",
    "        # grad init\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        model_output = vgg16(x)\n",
    "        # calculate loss\n",
    "        loss = criterion(model_output, label)\n",
    "        # back propagation \n",
    "        loss.backward()\n",
    "        # weight update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # trn_loss summary\n",
    "        trn_loss += loss.item()\n",
    "        # del (memory issue)\n",
    "        del loss\n",
    "        del model_output\n",
    "        \n",
    "        # 학습과정 출력\n",
    "        if (i+1) % 100 == 0: # every 100 mini-batches\n",
    "            with torch.no_grad(): # very very very very important!!!\n",
    "                val_loss = 0.0\n",
    "                corr_num = 0\n",
    "                total_num = 0\n",
    "                for j, val in enumerate(valLoader):\n",
    "                    val_x, val_label = val\n",
    "                    val_x = val_x.to(device)\n",
    "                    val_label =val_label.to(device)\n",
    "                    val_output = vgg16(val_x)\n",
    "                    v_loss = criterion(val_output, val_label)\n",
    "                    val_loss += v_loss\n",
    "                    \n",
    "                    model_label = val_output.argmax(dim=1)\n",
    "                    corr = val_label[val_label == model_label].size(0)\n",
    "                    corr_num += corr\n",
    "                    total_num += val_label.size(0)\n",
    "            \n",
    "            print(\"epoch: {}/{} | step: {}/{} | trn loss: {:.4f} | val loss: {:.4f} | acc: {:.2f}\".format(\n",
    "                epoch+1, num_epochs, i+1, num_batches, trn_loss / 100, val_loss / len(valLoader), (corr_num / total_num) * 100\n",
    "            ))            \n",
    "            \n",
    "            trn_loss_list.append(trn_loss/100)\n",
    "            val_loss_list.append(val_loss/len(valLoader))\n",
    "            trn_loss = 0.0\n",
    "\n",
    "print(\"training finished!\")\n",
    "\n",
    "# save\n",
    "PATH = \"./vgg19.pt\"\n",
    "torch.save(vgg16.state_dict(), PATH)\n",
    "\n",
    "print(\"model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "PATH = \"./vgg16_62.pt\"\n",
    "torch.save(vgg16.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 66.84\n"
     ]
    }
   ],
   "source": [
    "# test acc\n",
    "with torch.no_grad():\n",
    "    corr_num = 0\n",
    "    total_num = 0\n",
    "    for j, val in enumerate(testLoader):\n",
    "        val_x, val_label = val\n",
    "        val_x = val_x.to(device)\n",
    "        val_label =val_label.to(device)\n",
    "        val_output = vgg16(val_x)\n",
    "        model_label = val_output.argmax(dim=1)\n",
    "        corr = val_label[val_label == model_label].size(0)\n",
    "        corr_num += corr\n",
    "        total_num += val_label.size(0)\n",
    "\n",
    "print(\"test_acc: {:.2f}\".format(corr_num / total_num * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "plt.plot(trn_loss_list, label=\"train_loss\")\n",
    "plt.legend()\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 CONDA (NGC/PyTorch 20.06) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
