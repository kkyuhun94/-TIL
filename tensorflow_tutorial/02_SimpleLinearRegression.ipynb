{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# tensorflow 2가 설치되어있지만 1을 쓸수 있도록 해줌 \n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build hypothesis and cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1,2,3,4,5]\n",
    "y_data = [1,2,3,4,5]\n",
    "\n",
    "W = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)\n",
    "\n",
    "# hypothesis = W*x + b\n",
    "hypothesis = W*x_data + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ cost(W,b) = \\frac{1}{m}\\Sigma(H(x^{(i)})-y^{(i)})^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=45.660004>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.5, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.reduce_mean() : 차원이 줄어 들면서 mean을 계산\n",
    "v = [1., 2., 3., 4.] # rank = 1\n",
    "print(tf.reduce_mean(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=9>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.square() : 제곱\n",
    "tf.square(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent : 경사 하강 알고리즘\n",
    "### $cost(W,b)$를 최소화 하는 $W,b$를 찾음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=0.376>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning rate 초기화 \n",
    "learning_rate = 0.01\n",
    "\n",
    "# gradient descent : 한 번의 과정 \n",
    "# tape에 변수에 대한 정보를 기록 \n",
    "with tf.GradientTape() as tape :\n",
    "    hypothesis = W * x_data + b\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "# tape에서 cost의 W,b에 대한 미분값을 각각 반환 (W의 기울기, b의 기울기) \n",
    "W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "\n",
    "# A.assign_sub(B) : A = A -B / A -= B \n",
    "W.assign_sub(learning_rate * W_grad)  # 기울기를 얼마만큼 반영할 것인지 \n",
    "b.assign_sub(learning_rate * b_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |      2.452|     0.376| 45.660004\n",
      "   10 |      1.104|  0.003398|  0.206336\n",
      "   20 |      1.013|  -0.02091|  0.001026\n",
      "   30 |      1.007|  -0.02184|  0.000093\n",
      "   40 |      1.006|  -0.02123|  0.000083\n",
      "   50 |      1.006|  -0.02053|  0.000077\n",
      "   60 |      1.005|  -0.01984|  0.000072\n",
      "   70 |      1.005|  -0.01918|  0.000067\n",
      "   80 |      1.005|  -0.01854|  0.000063\n",
      "   90 |      1.005|  -0.01793|  0.000059\n"
     ]
    }
   ],
   "source": [
    "# 위의 방법을 for 문을 이용해서 반복해서 수행 \n",
    "W = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)\n",
    "\n",
    "for i in range(100) :\n",
    "    # Gradient descent\n",
    "    with tf.GradientTape() as tape :\n",
    "        hypothesis = W * x_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 10 == 0 : # i 값이 10의 배수가 될때마다 출력\n",
    "        print(\"{:5} | {:10.4}|{:10.4}|{:10.6f}\".format(i, W.numpy(), b.numpy(), cost))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |      2.452|     0.376| 45.660004\n",
      "   10 |      1.104|  0.003398|  0.206336\n",
      "   20 |      1.013|  -0.02091|  0.001026\n",
      "   30 |      1.007|  -0.02184|  0.000093\n",
      "   40 |      1.006|  -0.02123|  0.000083\n",
      "   50 |      1.006|  -0.02053|  0.000077\n",
      "   60 |      1.005|  -0.01984|  0.000072\n",
      "   70 |      1.005|  -0.01918|  0.000067\n",
      "   80 |      1.005|  -0.01854|  0.000063\n",
      "   90 |      1.005|  -0.01793|  0.000059\n",
      "  100 |      1.005|  -0.01733|  0.000055\n"
     ]
    }
   ],
   "source": [
    "# Full Code\n",
    "import tensorflow as tf\n",
    "\n",
    "# Data\n",
    "x_data = [1, 2, 3, 4, 5]\n",
    "y_data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# W, b initialize\n",
    "W = tf.Variable(2.9) # 2.9 부터 시작 \n",
    "b = tf.Variable(0.5) # 0.5 부터 시작 \n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(100+1) :\n",
    "    with tf.GradientTape() as tape :\n",
    "        hypothesis = W * x_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    if i % 10 == 0 :\n",
    "        print(\"{:5} | {:10.4}|{:10.4}|{:10.6f}\".format(i, W.numpy(), b.numpy(), cost))\n",
    "        \n",
    "        \n",
    "        # cost가 0에 가까울수록 실제 데이터와 유사하다! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5.00667, shape=(), dtype=float32)\n",
      "tf.Tensor(2.4946702, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(W*5 + b)\n",
    "print(W*2.5 + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
