{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5. 오차역전파법(back propagation)\n",
    "* 수치미분 : 단순하고 구현하기 쉽지만 계산이 오래 걸림 $\\to$ 오차역전파법 : 효율적으로 계산 \n",
    "    * \"CS231n\" 수업을 참고\n",
    "* 오차역전파법(역전파) : backward propagation of errors (back propagation)\n",
    "\n",
    "#### 5.1 계산 그래프\n",
    "* 계산 그래프(computational graph) : 계산과정을 그래프로 나타낸 것\n",
    "    * 노드(node)\n",
    "    * 에지(edge) : 노드 사이의 직선 \n",
    "\n",
    "* 계산 그래프를 이용한 문제풀이 흐름\n",
    "    1. 계산 그래프를 구성\n",
    "    2. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행 (순전파)\n",
    "        * 역전파는 이후에 미분을 계산할 때 중요한 역할을 함\n",
    "    \n",
    "* 계산 그래프의 특징 - 국소적 계산 : 자신과 직접 관계된 작은 범위를 계산\n",
    "    * 전체에서 어떤 일이 벌어지든 상관없이 자신과 관계된 정도만으로 결과를 출력할 수 있음\n",
    "    * 노드에서 국소적 계산을 한후 다음 노드로 전달 \n",
    "\n",
    "* 왜 계산 그래프로 푸는가?\n",
    "    * 계산 그래프의 이점\n",
    "        1. 전체가 아무리 복잡해도 각 노드에서는 단순한 계산에 집중하여 문제를 단순화 할 수 있음 \n",
    "        2. 중간 계산 결과를 모두 보관할 수 있음\n",
    "        3. **역전파를 통해 미분을 효율적으로 계산할 수 있다**\n",
    "            \n",
    "#### 5.2 연쇄 법칙\n",
    "* 계산 그래프의 역전파\n",
    "    * 순전파 : 계산 결과를 왼쪽에서 오른쪽으로 전달\n",
    "    * 역전파 : '국소적인 미분'을 순방향과는 반대인 오른쪽에서 왼쪽으로 전달\n",
    "        * '국소적인 미분'의 전달원리 : 연쇄법칙(chain rule)\n",
    "            * 반대방향으로 '국소적 미분'을 곱한다\n",
    "    \n",
    "* 연쇄법칙(chain rule)이란?\n",
    "    * 합성 함수 : 여러 함수로 구성된 함수\n",
    "        * ex. $z = (x+y)^2 \\to z=t^2, t=x+y$\n",
    "        * $\\frac{\\partial z}{\\partial t} = 2t ,\\frac{\\partial t}{\\partial x} = 1$\n",
    "        * $\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t}\\frac{\\partial t}{\\partial x} = 2t \\centerdot 1 = 2(x+y)$\n",
    "        * **합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 구할 수 있음** \n",
    "\n",
    "* 계산그래프의 역전파 : 오른쪽에서 왼쪽으로 국소적 미분(편미분)을 곱한후 다음 노드로 전달 \n",
    "\n",
    "#### 5.3 역전파\n",
    "* 덧셈 노드의 역전파 : $z=x+y$\n",
    "    * $\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t}\\frac{\\partial t}{\\partial x} = 1 \\centerdot 1 = 1 $\n",
    "    * 역방향 신호 그대로 1을 곱해서 다음 노드로 보냄\n",
    "        * 순방향 입력신호가 불필요\n",
    "* 곱셈 노드의 역전파 : $z=xy$\n",
    "    * $\\frac{\\partial z}{\\partial x} = y, \\frac{\\partial z}{\\partial y} = x $\n",
    "    * 상류의 값에 순전파 입력 신호들을 '서로 바꾼 값'을 곱해서 하류로 보냄\n",
    "        ex. x 입력값 10, y입력값이 5이고, 역전파 상류에서 1.3의 값이 흘러나온다면 x방향으로 1.3 x 5, y방향으로 1.3 x 10의 값을 흘려보냄\n",
    "        * 덧셈 노드와 달리 순방향 입력신호 값이 필요\n",
    "        \n",
    "* 사과 쇼핑의 예 : 변수 : 사과의 가격, 사과의 개수, 소비세\n",
    "    * 사과 가격에 대한 지불 금액의 미분, 사과 개수에 대한 지불 금액의 미분, 소비세에 대한 지불금액의 미분을 구해야 함\n",
    "    * 빈칸에 값 채우기 풀어보기 \n",
    "\n",
    "#### 5.4 단순한 계층 구현하기\n",
    "* 신경망을 구성하는 계층 각각을 클래스로 구현\n",
    "    * 계층 : 신경망의 기능 단위\n",
    "        * Sigmoid,Affine등의 기능을 계층 단위로 구현\n",
    "    * 곱셈 계층(MulLayer) : 초기화 필요\n",
    "    * 덧셈 계층(AddLayer) : 초기화가 굳이 필요 없음\n",
    "    \n",
    "#### 5.5 활성화 함수 계층 구현하기\n",
    "* 계산 그래프를 신경망에 적용\n",
    "    * 활성화 함수 구현\n",
    "        * ReLU 계층\n",
    "            * $ y = x(x>0) $\n",
    "            * $ y=0(x <= 0)$\n",
    "            * $\\frac{\\partial y}{\\partial x}=1(x>0), 0(x<=0) $\n",
    "                * 역전파 : x가 0보다 클때는 미분값을 전달, x가 0이거나 작을때는 0을 전달 해야함\n",
    "        * Sigmoid 계층\n",
    "            * $y = \\frac{1}{1+exp(-x)}$를 순차적으로 미분\n",
    "                1. '/' 노드 (1+exp(-x)를 분모로): $y=\\frac{1}{x}$을 미분 : $\\frac{\\partial y}{\\partial x} = - \\frac{1}{x^2} = -y^2$ \n",
    "                2. '+' 노드 (1 ,exp(-x) -> 1+exp(-x)) : 여과 없이 하류로 내보냄 \n",
    "                3. 'exp' 노드 (-x -> exp(-x)) : $y=exp(x)$을 미분 : $\\frac{\\partial y}{\\partial x} = exp(x)$ \n",
    "                4. 'X' 노드 (-1,x -> -x)  : 순전파 때의 값을 바꿔 곱함 \n",
    "            * Sigmoid 노드로 통합 : $\\frac{\\partial L}{\\partial y}y^2exp(-x) = \\frac{\\partial L}{\\partial y}y(1-y)$\n",
    "                * 중간 과정을 생략, 순전파의 출력만으로 역전파를 계산 가능\n",
    "                \n",
    "        \n",
    "#### 5.6 Affine/Softmax 계층 구현하기\n",
    "* Affine 계층\n",
    "    * 신경망의 순전파 : 가중치 신호의 총합을 계산 (Y = np.dot(X,W)+B)\n",
    "    * 어파인 변환 (Affine transformer) : 기하학에서 행렬의 곱을 뜻함 \n",
    "    * 노드사이에 스칼라 값이 아닌 행렬 값이 흐르는 그래프 \n",
    "    * 'dot', '+'노드 존재 \n",
    "        * 행렬의 형상에 주의 : 차원의 원소수를 일치시켜야 곱할 수 있음  \n",
    "            \n",
    "* 배치용 Affine 계층\n",
    "    1. $\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\centerdot W^T$\n",
    "    2. $\\frac{\\partial L}{\\partial W} = X^T \\centerdot \\frac{\\partial L}{\\partial Y}$\n",
    "    3. $\\frac{\\partial L}{\\partial B} = \\frac{\\partial L}{\\partial Y}$의 첫 번째 축(0축,열방향)의 합\n",
    "\n",
    "* Softmax-with-Loss 계층\n",
    "    * Softmax : 출력층에서 사용\n",
    "        * 입력값을 정규화하여(0~1사이의 확률값으로) 출력\n",
    "        * 손글씨예시 : 10클래스 분류(0~10)이면 입력,출력값도 10개\n",
    "    * Softmax-with-Loss : softmax함수와 손실함수인 교차엔트로피 오차를 포함하여 계층으로 표현\n",
    "        * 역전파 : $(y_1-t_1,y_2-t_2,y_3-t_3)$ 정답레이블의 차분(출력과 정답레이블의 오차를 그대로 표현)\n",
    "            * $(y_1,y_2,y_3)$ : Softmax 계층의 출력\n",
    "            * $(t_1,t_2,t_3)$ : 정답레이블   \n",
    "            \n",
    "#### 5.7 오차역전파법 구현하기\n",
    "* 앞 절에서 구현한 계층을 조합, 신경망을 구축\n",
    "\n",
    "* 신경망 학습의 전체 그림\n",
    "    * 전제 : 신경망에 적응 가능한 가중치와 편향이 존재, 이것들을 훈련 데이터에 학습시킴(알맞게 조정)\n",
    "    * 신경망 학습 4단계\n",
    "        1. 미니배치 : 훈련 데이터 중 일부를 무작위로 가져옴. 미니배치의 손실함수 값을 줄이는 것이 목표 \n",
    "        2. 기울기 산출 : 미니배치의 손실함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구하고, 손실함수를 최소화 하는 값을 찾음\n",
    "            * 오차 역전파법을 기울기 산출에 사용 \n",
    "        3. 매개변수 갱신 : 가중치 매개변수를 기울기 방향으로 조금씩 갱신\n",
    "        4. 반복 : 1~3을 반복 \n",
    "    \n",
    "* 오차역전파법을 적용한 신경망 구현\n",
    "\n",
    "* 오차역전파법으로 구한 기울기 검증\n",
    "    * 수치 미분과 오차역전파법의 결과 오차가 0이 되는 일은 드뭄 : 컴퓨터의 정밀도가 유한하기 때문 \n",
    "        * 올바르게 구현할 경우 0에 가까운 작은 값이 나오는 것이 정상\n",
    "        \n",
    "* 오차역전파법을 사용한 학습 구현 \n",
    "\n",
    "\n",
    "#### 5.8 정리\n",
    "* 계산 그래프를 이용하면 계산 과정을 시각적으로 파악할 수 있음\n",
    "* 계산 그래프의 노드는 국소적 계산으로 구성. 국소적 계산을 조합해 전체 계산을 구성\n",
    "* 계산 그래프의 순전파는 통상의 계산을 수행. 계산 그래프의 역전파로는 각 노드의 미분을 구함\n",
    "* 신경망의 구성 요소를 계층으로 구현하여 기울기를 효율적으로 계산할 수 있음(오차역전파법)\n",
    "* 수치 미분과 오차역전파법의 결과를 비교하면 오차역전파법의 구현에 잘못이 없는지 확인가능(기울기 확인)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 곱셈 계층 : x,y 바꿔서 구하기\n",
    "class MulLayer :\n",
    "    def __init__(self) : # x,y를 초기화 : 순전파 시의 입력값을 유지하기 위해 사용 ? \n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, x, y) :\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x*y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout) : # dout: 상류에서 넘어온 미분\n",
    "        dx = dout * self.y # x와 y를 바꿔서 곱해줌\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "# 사과 예시\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들 \n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "# 각 변수에 대한 미분\n",
    "# backward()가 받는 인수 : 순전파의 출력에 대한 미분 \n",
    "\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 덧셈 계층 \n",
    "class AddLayer:\n",
    "    def __init__(self) : \n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y) :\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout) :\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx,dy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# 계층 \n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "# 역전파 \n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(price)\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax) # 이 결과로 뭘 할 수 있단거? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU\n",
    "# mask 인스턴스 변수 : 0,1로 구성 \n",
    "class Relu :\n",
    "    def __init__(self) : \n",
    "        self.mask = None \n",
    "    \n",
    "    def forward(self, x) :\n",
    "        self.mask = (x <= 0)   # x가 0보다 작으면 True, 크면 False \n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0     # x가 0 보다 작은 값은 0으로 반환 \n",
    "    \n",
    "    def backward(self, dout) : # 0보다 작은 값일때(True)일때는 미분값이 0을 전달해야함 \n",
    "        dout[self.mask] = 0    # 0보다 작은 값의 미분값을 0으로 반환 \n",
    "        dx = dout\n",
    "        \n",
    "        return dx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.5, -2. ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1.0, -0.5],[-2.0, 3.0]])\n",
    "print(x)\n",
    "mask = (x<=0)\n",
    "print(mask)\n",
    "out = x.copy()\n",
    "out[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid\n",
    "# 순전파의 출력을 인스턴스 변수 out에 보관했다가, 역전파 계산 때 그 값을 사용 \n",
    "\n",
    "class Sigmoid :\n",
    "    def __init__(self) :\n",
    "        self.out = None\n",
    "        \n",
    "    def foward(self,x) :\n",
    "        out = 1 / (1+ np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout) :\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "# 배치용 Affine 예시\n",
    "X_dot_W = np.array([[0,0,0],[10,10,10]])\n",
    "B = np.array([1,2,3])\n",
    "\n",
    "print(X_dot_W)\n",
    "print(X_dot_W+B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "## 배치용 Affine 예시\n",
    "dY = np.array([[1,2,3],[4,5,6]])\n",
    "print(dY)\n",
    "\n",
    "dB = np.sum(dY, axis = 0)\n",
    "print(dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affine 계층 구현\n",
    "class Affine:\n",
    "    def __init__(self, W ,b) :\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout) :\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis = 0)\n",
    "        \n",
    "        return dx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SoftmaxWithLoss !!\n",
    "class SoftmaxWithLoss :\n",
    "    def __init__(self) :\n",
    "        self.loss = None # 손실\n",
    "        self.y = None # Softmax 출력\n",
    "        self.t = None # 정답레이블 (one-hot vector)\n",
    "        \n",
    "    def forward(self, x, t) :\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1) :\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오차역전파법을 적용한 신경망 구현 \n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import * \n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict # 순서가 있는 딕셔너리(순서를 기억)\n",
    "\n",
    "\n",
    "class TwoLayerNet :\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01) :\n",
    "        # 가중치 초기화 \n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "    \n",
    "    \n",
    "    def predict(self, x) :\n",
    "        for layer in self.layers.values() :\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t) :\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y,t)\n",
    "    \n",
    "    \n",
    "    def accuracy(self, x, t) :\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    # x : 입력 데이터, t : 정답 레이블 \n",
    "    def numerical_gradient(self, x, t) :\n",
    "        loss_W = lambda W : self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "\n",
    "    \n",
    "    def gradient(self, x, t) :\n",
    "        # 순전파\n",
    "        self.loss(x,t)\n",
    "        \n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        \n",
    "        for layer in layers :\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        \n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:4.297928451198766e-10\n",
      "b1:2.3482079938375382e-09\n",
      "W2:5.962502743780398e-09\n",
      "b2:1.404064551360773e-07\n"
     ]
    }
   ],
   "source": [
    "# 오차역전파법으로 구한 기울기 검증\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from data.mnist import load_mnist\n",
    "\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train),(x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch,t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 가중치의 차이의 절대값을 구한 후, 그 절댓값들의 평균을 냄\n",
    "for key in grad_numerical.keys() :\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14366666666666666 0.1496\n",
      "0.9030333333333334 0.9076\n",
      "0.9207833333333333 0.9216\n",
      "0.9341 0.9328\n",
      "0.94245 0.9386\n",
      "0.9497333333333333 0.9437\n",
      "0.9558166666666666 0.9505\n",
      "0.9594166666666667 0.9572\n",
      "0.9640833333333333 0.9603\n",
      "0.9673833333333334 0.9609\n",
      "0.9710833333333333 0.9646\n",
      "0.9731666666666666 0.9659\n",
      "0.97475 0.967\n",
      "0.9764833333333334 0.968\n",
      "0.9774833333333334 0.9683\n",
      "0.9775833333333334 0.9684\n",
      "0.9812166666666666 0.9698\n"
     ]
    }
   ],
   "source": [
    "# 오차 역전파법을 사용한 학습 구현하기\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "\n",
    "from data.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num) :\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 오차 역전파법으로 기울기를 구한다\n",
    "    grad = network.gradient(x_batch,t_batch)\n",
    "    \n",
    "    # 갱신 \n",
    "    for key in ('W1','b1','W2','b2') :\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
