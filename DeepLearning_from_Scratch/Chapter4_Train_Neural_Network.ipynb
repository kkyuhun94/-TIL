{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4. 신경망 학습\n",
    "\n",
    "* 학습 : 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득 \n",
    "    * 손실함수의 결괏값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 학습의 목표 \n",
    "        * 손실함수 : 신경망이 학습할 수 있도록 해주는 지표\n",
    "        * 경사법 : 함수의 기울기를 활용해 손실함수를 작게 만드는 기법 \n",
    "    \n",
    "#### 4.1 데이터에서 학습한다!\n",
    "* 데이터 주도 학습\n",
    "    * 신경망의 특징 : 데이터를 보고 학습할 수 있음\n",
    "        * 가중치 매개 변수의 값을 데이터를 보고 자동으로 결정\n",
    "    * 직선으로 분리할 수 있는 문제라면 데이터로 부터 자동으로 학습 할 수 있음    \n",
    "        * 퍼셉트론 수렴 정리 : 선형 분리 가능 문제는 유한 번의 학습을 통해 풀수 있음 \n",
    "\n",
    "* 기계 학습 : 데이터에서 답을 찾고, 패턴을 발견하고, 데이터로 이야기를 만듬 \n",
    "    * 딥러닝 : 기존 기계학습에서 사용하던 방법보다 사람의 개입을 더욱 배제할 수 있게 해주는 중요한 특성을 지님 \n",
    "\n",
    "* ex. 손글씨 5를 제대로 분류하는 프로그램\n",
    "    * 이미지에서 특징(feature)을 추출, 그 특징의 패턴을 기계학습 기술로 학습하는 방법\n",
    "        * feature(특징) : 입력 데이터(입력 이미지)에서 본질적인 데이터(중요한 데이터)를 정확하게 추출할 수 있도록 변환기\n",
    "        * 이미지의 특징은 주로 벡터로 기술한다.\n",
    "        * 컴퓨터 비전 분야에서는 SIFT, SURF, HOG등의 특징을 많이 사용함(사람이 생각한 특징)\n",
    "            * 이런 특징을 이용해 데이터를 벡터로 변환\n",
    "        * 변환된 벡터를 가지고 지도학습 방식의 대표 분류 기법 SVM,KNN등으로 학습(기계 학습)\n",
    "            * 이와 같은 기계학습 : 모아진 데이터로부터 규칙을 찾아내는 역할을 '기계'가 담당\n",
    "            * 이미지를 벡터로 변환할 때 사용하는 특징은 여전히 '사람'이 설계하는 것\n",
    "                * 문제에 적합한 특징을 쓰지 않으면 (혹은 특징을 설계하지 않으면) 좋은 결과를 얻기 힘듬\n",
    "    * 신경망 : 이미지를 있는 그대로 학습\n",
    "        * 이미지에 포함된 중요한 특징까지도 '기계'가 학습\n",
    "        * 딥러닝을 종단간 기계학습(end-to-end machine learning)이라고 함\n",
    "            * 처음부터 끝까지 데이터(입력)에서 목표한 결과(출력)를 사람의 개입 없이 얻음\n",
    "    \n",
    "* 훈련 데이터와 시험 데이터\n",
    "    * 훈련데이터 : 학습을 통해 최적의 매개변수를 찾음\n",
    "    * 시험데이터 : 앞서 훈련한 모델의 실력을 평가\n",
    "    * 훈련 데이터와 시험 데이터를 나눠야 하는 이유 : 범용적으로 사용할 수 있는 모델이어야 하기 때문\n",
    "        * 범용 능력을 제대로 평가하기 위해 훈련 데이터, 시험 데이터를 분리\n",
    "            * 범용 능력 : 아직보지 못한 데이터로도 문제를 올바르게 풀어내는 능력 -> 기계학습의 최종 목표 \n",
    "        * 데이터셋 하나로만 매개변수의 학습과 평가를 수행하면 올바른 평가가 될 수 없음 \n",
    "            * 수중의 데이터셋은 제대로 맞히더라도 다른 데이터셋에 엉망인 경우 : overfitting(오버피팅)\n",
    "\n",
    "#### 4.2 손실 함수\n",
    "* 하나의 '지표'를 기준으로 최적의 매개변수 값을 탐색함 \n",
    "    * 신경망에서는 손실함수(loss function)를 지표로 사용 \n",
    "        * 임의의 함수를 사용가능 \n",
    "            * 오차 제곱합\n",
    "            * 교차 엔트로피 오차\n",
    "\n",
    "* 오차제곱합(sum of squares for error, SSE)\n",
    "    * $E = \\frac{1}{2} \\Sigma (y_k-t_k)^2$\n",
    "    * $y_k$:신경망의 출력(신경망이 추정한 값) ,  $t_k$:정답 레이블 ,   $k$: 데이터 차원 수\n",
    "    \n",
    "* 교차 엔트로피 오차(cross entropy error, CEE)\n",
    "    * $E = -\\Sigma t_k \\log y_k$\n",
    "    * $\\log$ : 밑이 e인 자연로그 ,$y_k$: 신경망의 출력 ,$t_k$ : 정답레이블(정답에 해당하는 인덱스의 원소만 1, 나머지 0)\n",
    "    * 정답에 해당하는 출력이 커질수록 0에 다가가다가, 그 출력이 1일 때 0이 됨\n",
    "    \n",
    "* 기계학습 문제 : 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아냄\n",
    "    * 모든 훈련 데이터를 대상으로 손실 함수 값을 구해야함, 총 합을 지표로 삼아야함\n",
    "    * $E=-\\frac{1}{N} \\Sigma \\Sigma t_{nk}\\log y_{mk}$\n",
    "    * $N$: 데이터 개수 , $t_{nk}$ : 정답레이블 n번째 데이터 k번째 값 , $y_{nk}$ :신경망의 출력 n번째 데이터 k번째 값\n",
    "    * N개의 손실함수를 모두 더하고 N으로 나누어서 평균 손실 함수를 구함\n",
    "        * 훈련 데이터 개수와 관계없이 항상 통일된 지표를 얻을 수 있음 \n",
    "            \n",
    "* 데이터가 너무 많은 경우 일일이 손실함수를 계산하는데 시간이 걸림\n",
    "    * 미니 배치(mini) : 데이터의 일부 \n",
    "    * 미니 배치 학습 : 무작위로 일부만 뽑아서 학습\n",
    "    \n",
    "* (배치용) 교차 엔트로피 오차 구현하기\n",
    "    * 원-핫 인코딩 일때\n",
    "    * 원-핫 인코딩이 아닐 때\n",
    "    \n",
    "* 왜 손실 함수를 설정하는가?\n",
    "    * 왜 '정확도'라는 지표를 두고 '손실 함수의 값'이라는 우회적인 방법을 택할까?\n",
    "        * 신경망 학습에서의 '미분'의 역할에 주목 : 손실함수의 값을 가능한 작게 만드는 최적의 매개변수(가중치와 편향)을 탐색\n",
    "            * 미분 값의 음,양으로 매개변수 값을 갱신 -> 미분값이 0이 되면 더이상 손실함수가 줄어들지 않는다\n",
    "        * 정확도를 지표로 삼으면 미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없음\n",
    "            * 대부분의 장소에서 0이 되는 이유 : 매개 변수를 약간만 조정해서는 정확도가 크게 바뀌지않고 유지되어서 변화율이 0 또는 불연속적으로 변함\n",
    "        * 손실 함수를 지표로 삼는 다면 : 연속적인 수치로 나타남 (계단 함수를 활성화 함수로 사용하지 않는 이유와 유사)\n",
    "            \n",
    "#### 4.3 수치 미분(numeriacl differentiaion)\n",
    "* 미분 : 한 순간의 변화량\n",
    "    * $\\frac{df(x)}{dx} = \\lim(h\\to0) \\frac{f(x+h)-f(x)}{h}$\n",
    "    * 수치 미분을 코드로 구현할 때는 반올림 오차(rounding error)를 포함함 \n",
    "    * 중심 차분(중앙 차분) : 오차를 줄이기 위해 (x+h)와 (x-h)일 때의 함수 f의 차분을 계산 \n",
    "        * h는 아주 작은값으로 설정\n",
    "        * 수치 미분 : 아주 작은 차분으로 미분(근사치) $\\leftrightarrow$ 해석적 미분(analytic) : 오차를 포함하지 않는 진정한 미분\n",
    "        \n",
    "* 편미분 : 변수가 2개이상일 때\n",
    "    * $f(x_0,x_1)=x_0^2+x_1^2$\n",
    "    * 변수가 하나인 미분과 마찬가지로 특정 장소의 기울기를 구함\n",
    "        * 단, 여러 변수 중 목표 변수 하나에 초점을 맞추고 다른 변수는 값을 고정해야함 \n",
    "  \n",
    "#### 4.4 기울기\n",
    "* 편미분 : 변수별로 따로 계산\n",
    "    * 양쪽의 편미분을 묶어서 계산한다면 : 기울기(gradient)\n",
    "    * 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향 \n",
    "    \n",
    "* 경사법(경사 하강법)\n",
    "    * 신경망 : 최적의 매개 변수(가중치와 편향)를 학습 시에 찾아야 함\n",
    "        * 손실 함수가 최솟값이 될때 매개변수가 최적\n",
    "        * 일반적인 손실 함수는 매우 복잡해서 파악하기 어려움 \n",
    "            * 기울기를 이용해서 최솟값(또는 가능한 한 작은 값)을 찾음\n",
    "            * 함수의 값을 낮추는 방안을 제시하는 지표 = '기울기'\n",
    "                * 정말 함수의 최솟값이 항상 있는 것은 보장할 수 없음\n",
    "                    * 함수가 극소, 최소, 안장점, 고원 등에서 기울기가 0이므로 \n",
    "    * $x_0=x_0 - \\eta \\frac{\\partial f}{\\partial x_0}$\n",
    "    * $x_1=x_1 - \\eta \\frac{\\partial f}{\\partial x_1}$\n",
    "        * $\\eta \\to$  학습률(learning rate) : 한 번의 학습으로 얼마만큼 학습해야 할지, 매개변수 값을 얼마나 갱신할지 결정\n",
    "        * 학습률은 0.01,0.001 등 특정 값으로 미리 정해두어야 함(너무 크거나 작으면 좋은 장소를 찾아갈 수 없음)\n",
    "            * 학습률이 너무 크면 큰 값으로 발산\n",
    "            * 너무 작으면 거의 갱신 되지 않고 끝남\n",
    "    * 하이퍼파라미터(hyper parameter, 초매개변수) : 가중치, 편향 같은 신경망의 매개변수와는 성질이 다른 매개변수\n",
    "        * 사람이 직접 설정해야하는 매개 변수 $\\to$ 여러 후보 값 중에서 시험을 통해 가장 잘 학습하는 값을 찾아야 함\n",
    "\n",
    "* 신경망에서의 기울기\n",
    "    * 가중치 매개변수에 대한 손실 함수의 기울기\n",
    "    * 가중치가 $W$, 손싫함수가 $L$인 신경망의 경사 : $\\frac{\\partial L}{\\partial W}$\n",
    "    * 가중치 $W$가 2x3, 손실함수가 L 인 경우 기울기 $\\frac{\\partial L}{\\partial W}$도 2x3의 형상\n",
    "    * 신경망의 기울기를 구한 다음 경사법에 따라 가중치 매개변수를 갱신해야함\n",
    "\n",
    "#### 4.5 학습 알고리즘 구현하기\n",
    "* 신경망 학습 순서 - 확률적 경사하강법(SGD : stochastic gradient descent)\n",
    "    * 전제 : 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련데이터에 적응하도록 조정하는 과정을 학습이라고 한다.\n",
    "    1. 미니배치 : 훈련 데이터 중 일부를 무작위로 가져옴 \n",
    "    2. 기울기 산출 : 미니배치의 손실 합수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구합니\n",
    "    3. 매개변수 갱신 : 가중치 매개변수를 기울기 방향으로 아주 조금 갱신합니다. \n",
    "    4. 반복 : 1~3 단계를 반복\n",
    "\n",
    "* 미니배치 학습 : 훈련 데이터 중 일부를 무작위로 꺼내고(미니배치), 그 미니배치에 대해서 경사법으로 매개변수를 갱신\n",
    "    \n",
    "\n",
    "\n",
    "* 시험 데이터로 평가하기\n",
    "\n",
    "#### 4.6 정리 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오차 제곱합(SSE)\n",
    "def sum_squares_error(y, t) :\n",
    "    return 0.5*np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차 엔트로피 오차(CEE)\n",
    "def cross_entorpy_error(y,t) :\n",
    "    delta = 1e-7        # np.log()함수에 0을 입력하면 마이너스 무한대(-inf)가 되어서 아주작은 값을 더해주어서 계산을 가능하게함 \n",
    "    return -np.sum(t*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# mini batch\n",
    "# data load\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from data.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) =  load_mnist(normalize=True, one_hot_label=True) \n",
    "\n",
    "# onehot인코딩: 정답 위치의 원소만 1 나머지 0\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7363 24267 37270 23718 42960 47724 57842 47425 24398  4356]\n"
     ]
    }
   ],
   "source": [
    "# 무작위로 10장만 빼내기\n",
    "train_size = x_train.shape[0] #60000\n",
    "batch_size = 10  \n",
    "batch_mask = np.random.choice(train_size, batch_size) #60000중에 10개 뽑기 \n",
    "print(batch_mask)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치용 교차 엔트로피 오차 구현\n",
    "# 정답 레이블이 원핫 인코딩일 때\n",
    "\n",
    "def cross_entropy_error(y,t) :\n",
    "    if y.ndim == 1 :\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1,y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답레이블이 원-핫 인코딩이 아닐때 : t가 0인 원소는 교차 엔트로피 오차도 0이므로 계산을 무시\n",
    "def cross_entropy_error(y,t) :\n",
    "    if y.ndim == 1 :\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7))/ batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치 미분\n",
    "# 중앙 차분 : (x+h),(x-h)사이 차분 이용\n",
    "def numerical_diff(f,x) :\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h)-f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치 미분 예시 \n",
    "# y = 0.01x^2+0.1x\n",
    "\n",
    "def function_1(x) :\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 함수 시각화\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1) # 0~20 까지 0.1간격\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2\n",
      "0.2999999999986347\n",
      "0.30000000000000004\n"
     ]
    }
   ],
   "source": [
    "# f(x)의 미분 (x=5,10일떄)\n",
    "print(numerical_diff(function_1,5))\n",
    "print(2*0.01*5+0.1)\n",
    "print(numerical_diff(function_1,10))\n",
    "print(2*0.01*10+0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수가 2개인 함수\n",
    "def function_2(x) :\n",
    "    return x[0]**2 + x[1]**2\n",
    "    # x가 numpy배열 : return np.sum(x**2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 편미분 : x0 =3, x1=4 x0에 대한 편미분 \n",
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2.0\n",
    "\n",
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 편미분 : x0=3, x1=4, x1에 대한 편미분 \n",
    "def function_tmp2(x1) :\n",
    "    return 3*3+ x1**2\n",
    "\n",
    "numerical_diff(function_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 기울기\n",
    "def numerical_gradient(f, x) :\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size) :\n",
    "        tmp_val = x[idx]\n",
    "        # f(x+h) 계산 \n",
    "        \n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "    \n",
    "    return grad \n",
    "\n",
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num= 100) :\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num) :           \n",
    "        grad = numerical_gradient(f,x)     \n",
    "        x -= lr*grad\n",
    "        print(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.4  3.2]\n",
      "[-1.92  2.56]\n",
      "[-1.536  2.048]\n",
      "[-1.2288  1.6384]\n",
      "[-0.98304  1.31072]\n",
      "[-0.786432  1.048576]\n",
      "[-0.6291456  0.8388608]\n",
      "[-0.50331648  0.67108864]\n",
      "[-0.40265318  0.53687091]\n",
      "[-0.32212255  0.42949673]\n",
      "[-0.25769804  0.34359738]\n",
      "[-0.20615843  0.27487791]\n",
      "[-0.16492674  0.21990233]\n",
      "[-0.1319414   0.17592186]\n",
      "[-0.10555312  0.14073749]\n",
      "[-0.08444249  0.11258999]\n",
      "[-0.06755399  0.09007199]\n",
      "[-0.0540432   0.07205759]\n",
      "[-0.04323456  0.05764608]\n",
      "[-0.03458765  0.04611686]\n",
      "[-0.02767012  0.03689349]\n",
      "[-0.02213609  0.02951479]\n",
      "[-0.01770887  0.02361183]\n",
      "[-0.0141671   0.01888947]\n",
      "[-0.01133368  0.01511157]\n",
      "[-0.00906694  0.01208926]\n",
      "[-0.00725355  0.00967141]\n",
      "[-0.00580284  0.00773713]\n",
      "[-0.00464228  0.0061897 ]\n",
      "[-0.00371382  0.00495176]\n",
      "[-0.00297106  0.00396141]\n",
      "[-0.00237684  0.00316913]\n",
      "[-0.00190148  0.0025353 ]\n",
      "[-0.00152118  0.00202824]\n",
      "[-0.00121694  0.00162259]\n",
      "[-0.00097356  0.00129807]\n",
      "[-0.00077884  0.00103846]\n",
      "[-0.00062308  0.00083077]\n",
      "[-0.00049846  0.00066461]\n",
      "[-0.00039877  0.00053169]\n",
      "[-0.00031901  0.00042535]\n",
      "[-0.00025521  0.00034028]\n",
      "[-0.00020417  0.00027223]\n",
      "[-0.00016334  0.00021778]\n",
      "[-0.00013067  0.00017422]\n",
      "[-0.00010453  0.00013938]\n",
      "[-8.36277945e-05  1.11503726e-04]\n",
      "[-6.69022356e-05  8.92029808e-05]\n",
      "[-5.35217885e-05  7.13623846e-05]\n",
      "[-4.28174308e-05  5.70899077e-05]\n",
      "[-3.42539446e-05  4.56719262e-05]\n",
      "[-2.74031557e-05  3.65375409e-05]\n",
      "[-2.19225246e-05  2.92300327e-05]\n",
      "[-1.75380196e-05  2.33840262e-05]\n",
      "[-1.40304157e-05  1.87072210e-05]\n",
      "[-1.12243326e-05  1.49657768e-05]\n",
      "[-8.97946606e-06  1.19726214e-05]\n",
      "[-7.18357285e-06  9.57809713e-06]\n",
      "[-5.74685828e-06  7.66247770e-06]\n",
      "[-4.59748662e-06  6.12998216e-06]\n",
      "[-3.67798930e-06  4.90398573e-06]\n",
      "[-2.94239144e-06  3.92318858e-06]\n",
      "[-2.35391315e-06  3.13855087e-06]\n",
      "[-1.88313052e-06  2.51084069e-06]\n",
      "[-1.50650442e-06  2.00867256e-06]\n",
      "[-1.20520353e-06  1.60693804e-06]\n",
      "[-9.64162827e-07  1.28555044e-06]\n",
      "[-7.71330261e-07  1.02844035e-06]\n",
      "[-6.17064209e-07  8.22752279e-07]\n",
      "[-4.93651367e-07  6.58201823e-07]\n",
      "[-3.94921094e-07  5.26561458e-07]\n",
      "[-3.15936875e-07  4.21249167e-07]\n",
      "[-2.52749500e-07  3.36999333e-07]\n",
      "[-2.02199600e-07  2.69599467e-07]\n",
      "[-1.61759680e-07  2.15679573e-07]\n",
      "[-1.29407744e-07  1.72543659e-07]\n",
      "[-1.03526195e-07  1.38034927e-07]\n",
      "[-8.28209562e-08  1.10427942e-07]\n",
      "[-6.62567649e-08  8.83423532e-08]\n",
      "[-5.30054119e-08  7.06738826e-08]\n",
      "[-4.24043296e-08  5.65391061e-08]\n",
      "[-3.39234636e-08  4.52312849e-08]\n",
      "[-2.71387709e-08  3.61850279e-08]\n",
      "[-2.17110167e-08  2.89480223e-08]\n",
      "[-1.73688134e-08  2.31584178e-08]\n",
      "[-1.38950507e-08  1.85267343e-08]\n",
      "[-1.11160406e-08  1.48213874e-08]\n",
      "[-8.89283245e-09  1.18571099e-08]\n",
      "[-7.11426596e-09  9.48568795e-09]\n",
      "[-5.69141277e-09  7.58855036e-09]\n",
      "[-4.55313022e-09  6.07084029e-09]\n",
      "[-3.64250417e-09  4.85667223e-09]\n",
      "[-2.91400334e-09  3.88533778e-09]\n",
      "[-2.33120267e-09  3.10827023e-09]\n",
      "[-1.86496214e-09  2.48661618e-09]\n",
      "[-1.49196971e-09  1.98929295e-09]\n",
      "[-1.19357577e-09  1.59143436e-09]\n",
      "[-9.54860614e-10  1.27314749e-09]\n",
      "[-7.63888491e-10  1.01851799e-09]\n",
      "[-6.11110793e-10  8.14814391e-10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f(x_0,x_1)=x_0^2+x_1^2\n",
    "def function_2(x) :\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "\n",
    "init_x = np.array([-3.0,4.0])\n",
    "gradient_descent(function_2, init_x = init_x, lr=0.1, step_num=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 57. -76.]\n",
      "[-1083.00000003  1444.00000007]\n",
      "[ 20576.99999267 -27436.00002124]\n",
      "[-390963.00759858  521284.00215314]\n",
      "[ 7428296.63595611 -9904396.53983905]\n",
      "[-1.41137328e+08  1.88183103e+08]\n",
      "[ 2.68126267e+09 -3.57501690e+09]\n",
      "[-5.09763373e+10  6.79001831e+10]\n",
      "[ 9.45170863e+11 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.58983747e+13, -1.29524862e+12])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 큰 예시 \n",
    "init_x = np.array([-3.0,4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.  4.]\n",
      "[-3.  4.]\n",
      "[-3.  4.]\n",
      "[-3.  4.]\n",
      "[-3.  4.]\n",
      "[-3.  4.]\n",
      "[-3.          3.99999999]\n",
      "[-3.          3.99999999]\n",
      "[-2.99999999  3.99999999]\n",
      "[-2.99999999  3.99999999]\n",
      "[-2.99999999  3.99999999]\n",
      "[-2.99999999  3.99999999]\n",
      "[-2.99999999  3.99999999]\n",
      "[-2.99999999  3.99999999]\n",
      "[-2.99999999  3.99999999]\n",
      "[-2.99999999  3.99999999]\n",
      "[-2.99999999  3.99999999]\n",
      "[-2.99999999  3.99999999]\n",
      "[-2.99999999  3.99999998]\n",
      "[-2.99999999  3.99999998]\n",
      "[-2.99999999  3.99999998]\n",
      "[-2.99999999  3.99999998]\n",
      "[-2.99999999  3.99999998]\n",
      "[-2.99999999  3.99999998]\n",
      "[-2.99999998  3.99999998]\n",
      "[-2.99999998  3.99999998]\n",
      "[-2.99999998  3.99999998]\n",
      "[-2.99999998  3.99999998]\n",
      "[-2.99999998  3.99999998]\n",
      "[-2.99999998  3.99999998]\n",
      "[-2.99999998  3.99999998]\n",
      "[-2.99999998  3.99999997]\n",
      "[-2.99999998  3.99999997]\n",
      "[-2.99999998  3.99999997]\n",
      "[-2.99999998  3.99999997]\n",
      "[-2.99999998  3.99999997]\n",
      "[-2.99999998  3.99999997]\n",
      "[-2.99999998  3.99999997]\n",
      "[-2.99999998  3.99999997]\n",
      "[-2.99999998  3.99999997]\n",
      "[-2.99999998  3.99999997]\n",
      "[-2.99999997  3.99999997]\n",
      "[-2.99999997  3.99999997]\n",
      "[-2.99999997  3.99999996]\n",
      "[-2.99999997  3.99999996]\n",
      "[-2.99999997  3.99999996]\n",
      "[-2.99999997  3.99999996]\n",
      "[-2.99999997  3.99999996]\n",
      "[-2.99999997  3.99999996]\n",
      "[-2.99999997  3.99999996]\n",
      "[-2.99999997  3.99999996]\n",
      "[-2.99999997  3.99999996]\n",
      "[-2.99999997  3.99999996]\n",
      "[-2.99999997  3.99999996]\n",
      "[-2.99999997  3.99999996]\n",
      "[-2.99999997  3.99999996]\n",
      "[-2.99999997  3.99999995]\n",
      "[-2.99999997  3.99999995]\n",
      "[-2.99999996  3.99999995]\n",
      "[-2.99999996  3.99999995]\n",
      "[-2.99999996  3.99999995]\n",
      "[-2.99999996  3.99999995]\n",
      "[-2.99999996  3.99999995]\n",
      "[-2.99999996  3.99999995]\n",
      "[-2.99999996  3.99999995]\n",
      "[-2.99999996  3.99999995]\n",
      "[-2.99999996  3.99999995]\n",
      "[-2.99999996  3.99999995]\n",
      "[-2.99999996  3.99999994]\n",
      "[-2.99999996  3.99999994]\n",
      "[-2.99999996  3.99999994]\n",
      "[-2.99999996  3.99999994]\n",
      "[-2.99999996  3.99999994]\n",
      "[-2.99999996  3.99999994]\n",
      "[-2.99999995  3.99999994]\n",
      "[-2.99999995  3.99999994]\n",
      "[-2.99999995  3.99999994]\n",
      "[-2.99999995  3.99999994]\n",
      "[-2.99999995  3.99999994]\n",
      "[-2.99999995  3.99999994]\n",
      "[-2.99999995  3.99999994]\n",
      "[-2.99999995  3.99999993]\n",
      "[-2.99999995  3.99999993]\n",
      "[-2.99999995  3.99999993]\n",
      "[-2.99999995  3.99999993]\n",
      "[-2.99999995  3.99999993]\n",
      "[-2.99999995  3.99999993]\n",
      "[-2.99999995  3.99999993]\n",
      "[-2.99999995  3.99999993]\n",
      "[-2.99999995  3.99999993]\n",
      "[-2.99999995  3.99999993]\n",
      "[-2.99999994  3.99999993]\n",
      "[-2.99999994  3.99999993]\n",
      "[-2.99999994  3.99999992]\n",
      "[-2.99999994  3.99999992]\n",
      "[-2.99999994  3.99999992]\n",
      "[-2.99999994  3.99999992]\n",
      "[-2.99999994  3.99999992]\n",
      "[-2.99999994  3.99999992]\n",
      "[-2.99999994  3.99999992]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 작은 예 : lr =1e-10\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이전에 만들어 놓은 함수 \n",
    "\n",
    "# softmax \n",
    "def softmax(a) :\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a-c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y\n",
    "\n",
    "def sigmoid(x) :\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 간단한 신경망 예시 \n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.gradient import numerical_gradient # 책의 내용과 함수가 다름\n",
    "\n",
    "\n",
    "class simpleNet :\n",
    "    def __init__(self) :\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화 (랜덤 행렬 2,3)\n",
    "    \n",
    "    def predict(self, x) :\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t) :\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.84847938  2.54405656 -0.74207406]\n",
      " [ 0.84579444 -1.20502867  1.2649686 ]]\n",
      "[1.27030262 0.44190813 0.6932273 ]\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.9052664025473756"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)\n",
    "\n",
    "x = np.array([0.6,0.9])\n",
    "p = net.predict(x) \n",
    "\n",
    "print(p)\n",
    "\n",
    "print(np.argmax(p)) # 최대값의 인덱스 \n",
    "\n",
    "t = np.array([0, 0, 1])\n",
    "net.loss(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.29922906 -0.20658818  0.50581724]\n",
      " [-0.44884359 -0.30988227  0.75872586]]\n"
     ]
    }
   ],
   "source": [
    "# 손실함수\n",
    "def f(W) :\n",
    "    return net.loss(x, t)\n",
    "\n",
    "# 기울기\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.29922906 -0.20658818  0.50581724]\n",
      " [-0.44884359 -0.30988227  0.75872586]]\n"
     ]
    }
   ],
   "source": [
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2층 신경망 클래스 구현하기 \n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet :\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01) :\n",
    "        # 가중치 초기화\n",
    "        self.params = {}  # 매개 변수를 보관하는 딕셔너리 \n",
    "        self.params['W1'] = weight_init_std * \\              # 1번째 층의 가중치\n",
    "                            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)           # 1번째 층의 편향\n",
    "        self.params['W2'] = weight_init_std * \\              \n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def predict(self, x) : # 예측  y = wx + b\n",
    "        W1,W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t) : # 손실함수의 값 y,t의 cross_entropy_error\n",
    "        y = self.predict(x)\n",
    "        return cross_entorpy_error(y,t)\n",
    "    \n",
    "    def accuracy(self, x, t) : # 정확도 \n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        t = np.argmax(t, axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y==t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # x : 입력 데이터, t : 정답 레이블           \n",
    "    def numerical_gradient(self, x, t) :   # 가중치 매개변수의 기울기를 구함 \n",
    "        loss_W = lambda W : self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측 처리 예시 \n",
    "x = np.random.rand(100,784) # 더미 입력 데이터\n",
    "y = net.predict(x)\n",
    "print(y.shape)\n",
    "\n",
    "t = np.random.rand(100,10) # 더미 정답 레이블\n",
    "\n",
    "grads = net.numerical_gradient(x, t) # 기울기 계산\n",
    "\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미니 배치 학습 구현 \n",
    "import numpy as np\n",
    "from data.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# 하이퍼파라미터 \n",
    "iters_num = 10000 # 반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # 미니배치 크기\n",
    "learning_rate = 0.1 \n",
    "network = TwoLayerNet(input_size =784, hidden_size=50, output_size=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
