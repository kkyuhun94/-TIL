{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수  \n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0) # 오버플로를 막기 위해서 exp안에 최댓값을 빼줌 \n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로를 막기 위해서 exp안에 최댓값을 빼줌 \n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "# 배치용 교차 엔트로피 오차 \n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    delta = 1e-7  # 1e-7은 log안에 0이 되지 않도록 더해주는 값\n",
    "    \n",
    "    # 배치의 크기로 나눠 정규화 하고 평균 교차 엔트로피 오차를 계산 \n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + delta)) / batch_size\n",
    "\n",
    "def sigmoid(x) :\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화 함수 \n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None # mask : T/F로 구성된 배열\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0) # 0보다 같거나 작으면 True(1)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0 # fancy index : out[x<=0] = 0 -> 0보다 작은 원소들을 0으로 \n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout): \n",
    "        dout[self.mask] = 0   # 역전파시 0보다 작은 원소들 0으로    \n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "# sigmoid : y = 1 / (1+exp(-x))\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out   # 역전파 시 y(1-y) 곱해줌\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "# Wx + b\n",
    "# 신경망의 순전파 때 수행하는 행렬의 곱을 기하학에서 affine transformation이라고 함 \n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실함수\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "# from common.layers import *\n",
    "# from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size_1,hidden_size_2, output_size, weight_init_std = 0.01):\n",
    "        \n",
    "        # 층 3개  \n",
    "        self.params = {} # 딕셔너리 변수\n",
    "        \n",
    "        # 1번째 층의 가중치와 편향\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size_1)\n",
    "        self.params['b1'] = np.zeros(hidden_size_1)\n",
    "        # 2번쨰 층의 가중치와 편향\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size_1, hidden_size_2) \n",
    "        self.params['b2'] = np.zeros(hidden_size_2)\n",
    "        # 3번째 층의 가중치와 편향\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size_2, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "      \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "      #  self.layers['Sigmoid1'] = Sigmoid()\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "      #  self.layers['Sigmoid2'] = Sigmoid()\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "    \n",
    "    # 예측 함수 \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    # 손실함수의 값을 구함 \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t) # SoftmaxWithLoss y,t의 cross_entropy\n",
    "    \n",
    "    # 정확도 \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "#     def numerical_gradient(self, x, t):\n",
    "#         loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "#         grads = {}\n",
    "#         grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "#         grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "#         grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "#         grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "#         return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1                             # 1로 초기화 \n",
    "        dout = self.lastLayer.backward(dout) # lastLayer : softmax 부터 \n",
    "        \n",
    "        layers = list(self.layers.values()) # layer들의 리스트 \n",
    "        layers.reverse()\n",
    "        for layer in layers:                  # 역전파 \n",
    "            dout = layer.backward(dout) \n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10)\n",
      "(10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 입력층 784, 출력층 10 \n",
    "from data.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "print(x_train.shape, t_train.shape)\n",
    "print(x_test.shape, t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.10218333333333333 test accuracy: 0.101\n",
      "train accuracy: 0.4860333333333333 test accuracy: 0.4879\n",
      "train accuracy: 0.8671 test accuracy: 0.8727\n",
      "train accuracy: 0.9166333333333333 test accuracy: 0.9151\n",
      "train accuracy: 0.93705 test accuracy: 0.934\n",
      "train accuracy: 0.9472833333333334 test accuracy: 0.9456\n",
      "train accuracy: 0.95495 test accuracy: 0.9519\n",
      "train accuracy: 0.96425 test accuracy: 0.9581\n",
      "train accuracy: 0.9664166666666667 test accuracy: 0.9609\n",
      "train accuracy: 0.9686166666666667 test accuracy: 0.9616\n",
      "train accuracy: 0.9732166666666666 test accuracy: 0.9659\n",
      "train accuracy: 0.97435 test accuracy: 0.966\n",
      "train accuracy: 0.97755 test accuracy: 0.9688\n",
      "train accuracy: 0.9775166666666667 test accuracy: 0.9675\n",
      "train accuracy: 0.9807 test accuracy: 0.968\n",
      "train accuracy: 0.9837166666666667 test accuracy: 0.9716\n",
      "train accuracy: 0.9835 test accuracy: 0.9681\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from data.mnist import load_mnist\n",
    "\n",
    "# 데이터 mnist데이터 셋에서 onehot데이터 load\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = MLP(input_size=784, hidden_size_1=50, hidden_size_2 = 25,output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)   # train_size = 60000, batch_size = 100, 60000개 중 임의 100개 \n",
    "    x_batch = x_train[batch_mask]   \n",
    "    t_batch = t_train[batch_mask] \n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch) \n",
    "    train_loss_list.append(loss) \n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train accuracy:\", train_acc,\"test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "np.random.choice(train_size, batch_size)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(5,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2 : \n",
    "    \"\"\"\n",
    "    parameters\n",
    "    input_size : 입력크기\n",
    "    hidden_size_list : 은닉층의 뉴런 수 리스트\n",
    "    output_size : 출력크기 \n",
    "    activation : relu / sigmoid  \n",
    "    weight_init_std : 가중치의 표준편차 지정\n",
    "    weight_decay_lambda : 가중치 감소의 세기\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size_list, output_size, activation ='relu', weight_init_std = 'relu', \n",
    "                 weight_decay_lambda=0) :\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size_list = hidden_size_list\n",
    "        self.hidden_layer_num = len(hidden_size_list)\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "        self.params = {}\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        self.__init_weight(weight_init_std)\n",
    "        \n",
    "        # 계층 생성\n",
    "        activation_layer = {'sigmoid' : Sigmoid, 'relu' : Relu}\n",
    "        self.layers = OrderedDict()\n",
    "        for idx in range(1, self.hidden_layer_num+1) :\n",
    "            self.layers['Affine' + str(idx)] = Affine(self.params['W'+str(idx)], self.params['b'+str(idx)])\n",
    "            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n",
    "        \n",
    "        idx = self.hidden_layer_num + 1\n",
    "        # 어파인층 개수 : 은닉층개수 + 1 \n",
    "        self.layers['Affine'+str(idx)] = Affine(self.params['W'+str(idx)], self.params['b' + str(idx)])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "    \n",
    "    def __init_weight(self, weight_init_std) :\n",
    "        \"\"\"가중치 초기화 \n",
    "        Parameters\n",
    "        ----------\n",
    "        weight_init_std : 가중치의 표준편차\n",
    "            relu, he로 지정 -> He 초깃값\n",
    "            sigmoid, xavier로 지정 -> Xavier 초깃값\n",
    "        \"\"\"\n",
    "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
    "        for idx in range(1, len(all_size_list)) :\n",
    "            scale = weight_init_std\n",
    "            if str(weight_init_std).lower() in ('relu', 'he') :\n",
    "                scale = np.sqrt(2.0 / all_size_list[idx - 1]) # ReLU를 사용할 때의 권장 초깃값 루트 2/n\n",
    "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier') :\n",
    "                scale = np.sqrt(1.0 / all_size_list[idx - 1]) # sigmoid를 사용할 때의 권장 초깃값 루트 1/n\n",
    "            self.params['W'+ str(idx)] = scale*np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
    "            self.params['b'+ str(idx)] = np.zeros(all_size_list[idx])\n",
    "         \n",
    "        \n",
    "    def predict(self, x) :\n",
    "        for layer in self.layers.values() :\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t) :\n",
    "        \"\"\"\n",
    "        손실 함수\n",
    "        parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        weight_decay = 0\n",
    "        for idx in range(1, self.hidden_layer_num + 2) :\n",
    "            W = self.params['W' + str(idx)]\n",
    "            weight_decay += 0.5*self.weight_decay_lambda * np.sum(W**2)\n",
    "        \n",
    "        return self.last_layer.forward(y,t) + weight_decay\n",
    "    \n",
    "    def accuracy(self, x, t) :\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "#     def numerical_gradient(self, x, t) :\n",
    "#         loss_W = lambda W : self.loss(x,t)\n",
    "        \n",
    "#         grads = {}\n",
    "#         for idx in range(1, self.hidden_layer_num + 2) :\n",
    "#             grads['W' + str(idx)] = numerical_gradient(loss_W, self.params['W' + str(idx)])\n",
    "#             grads['b' + str(idx)] = numerical_gradient(loss_W, self.params['b' + str(idx)])\n",
    "        \n",
    "#         return grads\n",
    "    \n",
    "    def gradient(self, x, t) :\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers :\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num + 2) :\n",
    "            # W에는 가중치 감소 \n",
    "            grads['W' + str(idx)] = self.layers['Affine'+ str(idx)].dW + self.weight_decay_lambda * self.layers['Affine' +str(idx)].W \n",
    "            grads['b' + str(idx)] = self.layers['Affine'+ str(idx)].db\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"확률적 경사 하강법（Stochastic Gradient Descent）\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MLP2(input_size=784, hidden_size_list =[250,50], output_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.1082 test accuracy: 0.1144\n",
      "train accuracy: 0.8944833333333333 test accuracy: 0.8969\n",
      "train accuracy: 0.93035 test accuracy: 0.9286\n",
      "train accuracy: 0.94225 test accuracy: 0.9418\n",
      "train accuracy: 0.9480666666666666 test accuracy: 0.9437\n",
      "train accuracy: 0.9510166666666666 test accuracy: 0.9454\n",
      "train accuracy: 0.9607166666666667 test accuracy: 0.9544\n",
      "train accuracy: 0.9651833333333333 test accuracy: 0.9572\n",
      "train accuracy: 0.9669833333333333 test accuracy: 0.9582\n",
      "train accuracy: 0.9674166666666667 test accuracy: 0.9592\n",
      "train accuracy: 0.97185 test accuracy: 0.9622\n",
      "train accuracy: 0.9725666666666667 test accuracy: 0.9588\n",
      "train accuracy: 0.9749333333333333 test accuracy: 0.9648\n",
      "train accuracy: 0.97725 test accuracy: 0.9654\n",
      "train accuracy: 0.9757833333333333 test accuracy: 0.9645\n",
      "train accuracy: 0.978 test accuracy: 0.9653\n",
      "train accuracy: 0.98165 test accuracy: 0.9674\n",
      "train accuracy: 0.9811833333333333 test accuracy: 0.9653\n",
      "train accuracy: 0.9834333333333334 test accuracy: 0.9684\n",
      "train accuracy: 0.9813666666666667 test accuracy: 0.969\n",
      "train accuracy: 0.9844166666666667 test accuracy: 0.9696\n",
      "train accuracy: 0.9868 test accuracy: 0.9719\n",
      "train accuracy: 0.9874 test accuracy: 0.9705\n",
      "train accuracy: 0.9866833333333334 test accuracy: 0.9696\n",
      "train accuracy: 0.9895833333333334 test accuracy: 0.9724\n",
      "train accuracy: 0.98875 test accuracy: 0.9712\n",
      "train accuracy: 0.9896166666666667 test accuracy: 0.9729\n",
      "train accuracy: 0.9882166666666666 test accuracy: 0.9681\n",
      "train accuracy: 0.9916666666666667 test accuracy: 0.9714\n",
      "train accuracy: 0.99165 test accuracy: 0.9724\n",
      "train accuracy: 0.99265 test accuracy: 0.9724\n",
      "train accuracy: 0.9925666666666667 test accuracy: 0.9707\n",
      "train accuracy: 0.99335 test accuracy: 0.9713\n",
      "train accuracy: 0.9943666666666666 test accuracy: 0.9719\n",
      "train accuracy: 0.99535 test accuracy: 0.9728\n",
      "train accuracy: 0.99015 test accuracy: 0.9682\n",
      "train accuracy: 0.9943166666666666 test accuracy: 0.9713\n",
      "train accuracy: 0.99485 test accuracy: 0.9707\n",
      "train accuracy: 0.9945666666666667 test accuracy: 0.9701\n",
      "train accuracy: 0.996 test accuracy: 0.9726\n",
      "train accuracy: 0.9967 test accuracy: 0.973\n",
      "train accuracy: 0.99685 test accuracy: 0.9722\n",
      "train accuracy: 0.99685 test accuracy: 0.9729\n",
      "train accuracy: 0.9960166666666667 test accuracy: 0.9718\n",
      "train accuracy: 0.9978166666666667 test accuracy: 0.9736\n",
      "train accuracy: 0.9984 test accuracy: 0.9731\n",
      "train accuracy: 0.99725 test accuracy: 0.973\n",
      "train accuracy: 0.9973333333333333 test accuracy: 0.9707\n",
      "train accuracy: 0.9983 test accuracy: 0.9724\n",
      "train accuracy: 0.9989 test accuracy: 0.9721\n",
      "train accuracy: 0.9989166666666667 test accuracy: 0.9737\n",
      "train accuracy: 0.9987666666666667 test accuracy: 0.9725\n",
      "train accuracy: 0.9991333333333333 test accuracy: 0.972\n",
      "train accuracy: 0.9991 test accuracy: 0.9726\n",
      "train accuracy: 0.9991 test accuracy: 0.9729\n",
      "train accuracy: 0.9994 test accuracy: 0.9724\n",
      "train accuracy: 0.9994833333333333 test accuracy: 0.9723\n",
      "train accuracy: 0.9994333333333333 test accuracy: 0.9731\n",
      "train accuracy: 0.9996333333333334 test accuracy: 0.972\n",
      "train accuracy: 0.9997 test accuracy: 0.9726\n",
      "train accuracy: 0.9996833333333334 test accuracy: 0.9723\n",
      "train accuracy: 0.99975 test accuracy: 0.9727\n",
      "train accuracy: 0.9997333333333334 test accuracy: 0.9722\n",
      "train accuracy: 0.9995666666666667 test accuracy: 0.9722\n",
      "train accuracy: 0.9997666666666667 test accuracy: 0.9723\n",
      "train accuracy: 0.99985 test accuracy: 0.9724\n",
      "train accuracy: 0.9998333333333334 test accuracy: 0.9723\n",
      "train accuracy: 0.9998666666666667 test accuracy: 0.9726\n",
      "train accuracy: 0.9998833333333333 test accuracy: 0.9723\n",
      "train accuracy: 0.9998666666666667 test accuracy: 0.9719\n",
      "train accuracy: 0.9998833333333333 test accuracy: 0.9729\n",
      "train accuracy: 0.9999166666666667 test accuracy: 0.9726\n",
      "train accuracy: 0.9999333333333333 test accuracy: 0.9721\n",
      "train accuracy: 0.9999166666666667 test accuracy: 0.9727\n",
      "train accuracy: 0.9999666666666667 test accuracy: 0.9727\n",
      "train accuracy: 0.9999833333333333 test accuracy: 0.9731\n",
      "train accuracy: 0.9999666666666667 test accuracy: 0.9723\n",
      "train accuracy: 0.99995 test accuracy: 0.9719\n",
      "train accuracy: 0.99995 test accuracy: 0.9725\n",
      "train accuracy: 0.9999666666666667 test accuracy: 0.972\n",
      "train accuracy: 0.9999833333333333 test accuracy: 0.9717\n",
      "train accuracy: 0.9999666666666667 test accuracy: 0.9721\n",
      "train accuracy: 0.99995 test accuracy: 0.972\n",
      "train accuracy: 0.9999833333333333 test accuracy: 0.9727\n"
     ]
    }
   ],
   "source": [
    "network = MLP2(input_size = 784, hidden_size_list = [100, 100, 100, 100, 100, 100], output_size = 10)\n",
    "optimizer = SGD(lr = 0.01) # 학습률이 0.01인 SGD로 매개변수 갱신\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "for i in range(50000) :\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grads = network.gradient(x_batch ,t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "    \n",
    "    if i%iter_per_epoch == 0 :\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train accuracy:\", train_acc,\"test accuracy:\", test_acc)\n",
    "        \n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs :\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
