{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0) # 오버플로를 막기 위해서 exp안에 최댓값을 빼줌 \n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로를 막기 위해서 exp안에 최댓값을 빼줌 \n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "# 배치용 교차 엔트로피 오차 \n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    delta = 1e-7  # 1e-7은 log안에 0이 되지 않도록 더해주는 값\n",
    "    \n",
    "    # 배치의 크기로 나눠 정규화 하고 평균 교차 엔트로피 오차를 계산 \n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + delta)) / batch_size\n",
    "\n",
    "def sigmoid(x) :\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화 함수 \n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None # mask : T/F로 구성된 배열\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0) # 0보다 같거나 작으면 True(1)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0 # fancy index : out[x<=0] = 0 -> 0보다 작은 원소들을 0으로 \n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout): \n",
    "        dout[self.mask] = 0   # 역전파시 0보다 작은 원소들 0으로    \n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "# sigmoid : y = 1 / (1+exp(-x))\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out   # 역전파 시 y(1-y) 곱해줌\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "# Wx + b\n",
    "# 신경망의 순전파 때 수행하는 행렬의 곱을 기하학에서 affine transformation이라고 함 \n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 텐서 대응\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실함수\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "# from common.layers import *\n",
    "# from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class MLPnetwork:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size_1,hidden_size_2, output_size, weight_init_std = 0.01):\n",
    "        \n",
    "        # 층 3개  \n",
    "        self.params = {} # 딕셔너리 변수\n",
    "        \n",
    "        # 1번째 층의 가중치와 편향\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size_1)\n",
    "        self.params['b1'] = np.zeros(hidden_size_1)\n",
    "        # 2번쨰 층의 가중치와 편향\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size_1, hidden_size_2) \n",
    "        self.params['b2'] = np.zeros(hidden_size_2)\n",
    "        # 3번째 층의 가중치와 편향\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size_2, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "      \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "      #  self.layers['Sigmoid1'] = Sigmoid()\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "      #  self.layers['Sigmoid2'] = Sigmoid()\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "    \n",
    "    # 예측 함수 \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    # 손실함수의 값을 구함 \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t) # SoftmaxWithLoss y,t의 cross_entropy\n",
    "    \n",
    "    # 정확도 \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "#     def numerical_gradient(self, x, t):\n",
    "#         loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "#         grads = {}\n",
    "#         grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "#         grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "#         grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "#         grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "#         return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values()) # layer들의 리스트 \n",
    "        layers.reverse()\n",
    "        for layer in layers:                  # 역전파 \n",
    "            dout = layer.backward(dout) \n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10)\n",
      "(10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 입력층 784, 출력층 10 \n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "print(x_train.shape, t_train.shape)\n",
    "print(x_test.shape, t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.124 test accuracy: 0.1258\n",
      "train accuracy: 0.31883333333333336 test accuracy: 0.3158\n",
      "train accuracy: 0.84345 test accuracy: 0.8497\n",
      "train accuracy: 0.9133166666666667 test accuracy: 0.91\n",
      "train accuracy: 0.93365 test accuracy: 0.9302\n",
      "train accuracy: 0.9461 test accuracy: 0.9429\n",
      "train accuracy: 0.9594666666666667 test accuracy: 0.9525\n",
      "train accuracy: 0.9657333333333333 test accuracy: 0.9606\n",
      "train accuracy: 0.96615 test accuracy: 0.9593\n",
      "train accuracy: 0.9712 test accuracy: 0.9636\n",
      "train accuracy: 0.9716333333333333 test accuracy: 0.963\n",
      "train accuracy: 0.9749 test accuracy: 0.9655\n",
      "train accuracy: 0.9760666666666666 test accuracy: 0.9657\n",
      "train accuracy: 0.98125 test accuracy: 0.969\n",
      "train accuracy: 0.9828666666666667 test accuracy: 0.9704\n",
      "train accuracy: 0.9821833333333333 test accuracy: 0.9677\n",
      "train accuracy: 0.9824666666666667 test accuracy: 0.9694\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from data.mnist import load_mnist\n",
    "\n",
    "# 데이터 mnist데이터 셋에서 onehot데이터 load\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size_1=50, hidden_size_2 = 25,output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train accuracy:\", train_acc,\"test accuracy:\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
