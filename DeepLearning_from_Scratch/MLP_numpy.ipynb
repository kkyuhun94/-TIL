{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0) # 오버플로를 막기 위해서 exp안에 최댓값을 빼줌 \n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로를 막기 위해서 exp안에 최댓값을 빼줌 \n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "# 배치용 교차 엔트로피 오차 \n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    delta = 1e-7  # 1e-7은 log안에 0이 되지 않도록 더해주는 값\n",
    "    \n",
    "    # 배치의 크기로 나눠 정규화 하고 평균 교차 엔트로피 오차를 계산 \n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + delta)) / batch_size\n",
    "\n",
    "def sigmoid(x) :\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화 함수 \n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None # mask : T/F로 구성된 배열\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0) # 0보다 같거나 작으면 True(1)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0 # fancy index : out[x<=0] = 0 -> 0보다 작은 원소들을 0으로 \n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout): \n",
    "        dout[self.mask] = 0   # 역전파시 0보다 작은 원소들 0으로    \n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "# sigmoid : y = 1 / (1+exp(-x))\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out   # 역전파 시 y(1-y) 곱해줌\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "# Wx + b\n",
    "# 신경망의 순전파 때 수행하는 행렬의 곱을 기하학에서 affine transformation이라고 함 \n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 텐서 대응\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실함수\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "# from common.layers import *\n",
    "# from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size_1,hidden_size_2, output_size, weight_init_std = 0.01):\n",
    "        \n",
    "        # 층 3개  \n",
    "        self.params = {} # 딕셔너리 변수\n",
    "        \n",
    "        # 1번째 층의 가중치와 편향\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size_1)\n",
    "        self.params['b1'] = np.zeros(hidden_size_1)\n",
    "        # 2번쨰 층의 가중치와 편향\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size_1, hidden_size_2) \n",
    "        self.params['b2'] = np.zeros(hidden_size_2)\n",
    "        # 3번째 층의 가중치와 편향\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size_2, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "      \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "      #  self.layers['Sigmoid1'] = Sigmoid()\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "      #  self.layers['Sigmoid2'] = Sigmoid()\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "    \n",
    "    # 예측 함수 \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    # 손실함수의 값을 구함 \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t) # SoftmaxWithLoss y,t의 cross_entropy\n",
    "    \n",
    "    # 정확도 \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "#     def numerical_gradient(self, x, t):\n",
    "#         loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "#         grads = {}\n",
    "#         grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "#         grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "#         grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "#         grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "#         return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1                             # 1로 초기화 \n",
    "        dout = self.lastLayer.backward(dout) # lastLayer : softmax 부터 \n",
    "        \n",
    "        layers = list(self.layers.values()) # layer들의 리스트 \n",
    "        layers.reverse()\n",
    "        for layer in layers:                  # 역전파 \n",
    "            dout = layer.backward(dout) \n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10)\n",
      "(10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 입력층 784, 출력층 10 \n",
    "from data.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "print(x_train.shape, t_train.shape)\n",
    "print(x_test.shape, t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 / Loss : 2.3013 / Train Acc : 11.2367% / Test Acc : 11.3500%\n",
      "Epoch : 1 / Loss : 2.2983 / Train Acc : 11.2367% / Test Acc : 11.3500%\n",
      "Epoch : 2 / Loss : 2.2875 / Train Acc : 21.6200% / Test Acc : 21.7800%\n",
      "Epoch : 3 / Loss : 2.1598 / Train Acc : 33.6833% / Test Acc : 33.6100%\n",
      "Epoch : 4 / Loss : 1.5045 / Train Acc : 66.5883% / Test Acc : 67.5500%\n",
      "Epoch : 5 / Loss : 0.8305 / Train Acc : 77.2683% / Test Acc : 78.0100%\n",
      "Epoch : 6 / Loss : 0.6564 / Train Acc : 79.8433% / Test Acc : 80.6100%\n",
      "Epoch : 7 / Loss : 0.5686 / Train Acc : 84.4167% / Test Acc : 84.6300%\n",
      "Epoch : 8 / Loss : 0.4988 / Train Acc : 85.9283% / Test Acc : 86.2400%\n",
      "Epoch : 9 / Loss : 0.4544 / Train Acc : 87.3700% / Test Acc : 87.4800%\n",
      "Epoch : 10 / Loss : 0.4217 / Train Acc : 87.9650% / Test Acc : 88.0300%\n",
      "Epoch : 11 / Loss : 0.3955 / Train Acc : 88.8233% / Test Acc : 88.7700%\n",
      "Epoch : 12 / Loss : 0.3727 / Train Acc : 89.5550% / Test Acc : 89.3700%\n",
      "Epoch : 13 / Loss : 0.3526 / Train Acc : 89.9883% / Test Acc : 89.7500%\n",
      "Epoch : 14 / Loss : 0.3352 / Train Acc : 90.5267% / Test Acc : 90.6300%\n",
      "Epoch : 15 / Loss : 0.3195 / Train Acc : 90.9033% / Test Acc : 90.7000%\n",
      "Epoch : 16 / Loss : 0.3049 / Train Acc : 91.2817% / Test Acc : 90.9600%\n",
      "Epoch : 17 / Loss : 0.2914 / Train Acc : 91.6617% / Test Acc : 91.5400%\n",
      "Epoch : 18 / Loss : 0.2791 / Train Acc : 92.1200% / Test Acc : 91.9900%\n",
      "Epoch : 19 / Loss : 0.2674 / Train Acc : 92.3967% / Test Acc : 92.3100%\n",
      "Epoch : 20 / Loss : 0.2560 / Train Acc : 92.7233% / Test Acc : 92.6300%\n",
      "Epoch : 21 / Loss : 0.2449 / Train Acc : 92.9983% / Test Acc : 92.8700%\n",
      "Epoch : 22 / Loss : 0.2355 / Train Acc : 93.3400% / Test Acc : 93.4300%\n",
      "Epoch : 23 / Loss : 0.2259 / Train Acc : 93.5583% / Test Acc : 93.6900%\n",
      "Epoch : 24 / Loss : 0.2171 / Train Acc : 93.8650% / Test Acc : 93.8900%\n",
      "Epoch : 25 / Loss : 0.2087 / Train Acc : 93.9933% / Test Acc : 94.0300%\n",
      "Epoch : 26 / Loss : 0.2008 / Train Acc : 94.3033% / Test Acc : 94.4100%\n",
      "Epoch : 27 / Loss : 0.1932 / Train Acc : 94.4433% / Test Acc : 94.3300%\n",
      "Epoch : 28 / Loss : 0.1864 / Train Acc : 94.7117% / Test Acc : 94.6400%\n",
      "Epoch : 29 / Loss : 0.1797 / Train Acc : 94.8700% / Test Acc : 94.8700%\n",
      "Epoch : 30 / Loss : 0.1738 / Train Acc : 95.0383% / Test Acc : 94.8600%\n",
      "Epoch : 31 / Loss : 0.1680 / Train Acc : 95.0567% / Test Acc : 95.0800%\n",
      "Epoch : 32 / Loss : 0.1630 / Train Acc : 95.3733% / Test Acc : 95.2100%\n",
      "Epoch : 33 / Loss : 0.1580 / Train Acc : 95.3983% / Test Acc : 95.1300%\n",
      "Epoch : 34 / Loss : 0.1531 / Train Acc : 95.5667% / Test Acc : 95.3800%\n",
      "Epoch : 35 / Loss : 0.1489 / Train Acc : 95.6967% / Test Acc : 95.4300%\n",
      "Epoch : 36 / Loss : 0.1447 / Train Acc : 95.7717% / Test Acc : 95.5100%\n",
      "Epoch : 37 / Loss : 0.1408 / Train Acc : 95.9567% / Test Acc : 95.6300%\n",
      "Epoch : 38 / Loss : 0.1367 / Train Acc : 96.0733% / Test Acc : 95.7000%\n",
      "Epoch : 39 / Loss : 0.1329 / Train Acc : 96.1633% / Test Acc : 95.7700%\n",
      "Epoch : 40 / Loss : 0.1297 / Train Acc : 96.2450% / Test Acc : 95.8800%\n",
      "Epoch : 41 / Loss : 0.1259 / Train Acc : 96.2017% / Test Acc : 95.7600%\n",
      "Epoch : 42 / Loss : 0.1230 / Train Acc : 96.4133% / Test Acc : 96.1200%\n",
      "Epoch : 43 / Loss : 0.1197 / Train Acc : 96.4850% / Test Acc : 96.0900%\n",
      "Epoch : 44 / Loss : 0.1169 / Train Acc : 96.5450% / Test Acc : 96.0700%\n",
      "Epoch : 45 / Loss : 0.1143 / Train Acc : 96.6133% / Test Acc : 96.2700%\n",
      "Epoch : 46 / Loss : 0.1114 / Train Acc : 96.7850% / Test Acc : 96.3600%\n",
      "Epoch : 47 / Loss : 0.1089 / Train Acc : 96.8283% / Test Acc : 96.3100%\n",
      "Epoch : 48 / Loss : 0.1065 / Train Acc : 96.8500% / Test Acc : 96.3400%\n",
      "Epoch : 49 / Loss : 0.1038 / Train Acc : 96.9833% / Test Acc : 96.5000%\n",
      "Epoch : 50 / Loss : 0.1013 / Train Acc : 97.0583% / Test Acc : 96.3500%\n",
      "Epoch : 51 / Loss : 0.0991 / Train Acc : 97.1050% / Test Acc : 96.4500%\n",
      "Epoch : 52 / Loss : 0.0969 / Train Acc : 97.2283% / Test Acc : 96.6300%\n",
      "Epoch : 53 / Loss : 0.0946 / Train Acc : 97.2100% / Test Acc : 96.6200%\n",
      "Epoch : 54 / Loss : 0.0925 / Train Acc : 97.3183% / Test Acc : 96.6000%\n",
      "Epoch : 55 / Loss : 0.0903 / Train Acc : 97.4167% / Test Acc : 96.7400%\n",
      "Epoch : 56 / Loss : 0.0885 / Train Acc : 97.2983% / Test Acc : 96.6500%\n",
      "Epoch : 57 / Loss : 0.0865 / Train Acc : 97.4983% / Test Acc : 96.7700%\n",
      "Epoch : 58 / Loss : 0.0846 / Train Acc : 97.5017% / Test Acc : 96.7500%\n",
      "Epoch : 59 / Loss : 0.0829 / Train Acc : 97.5067% / Test Acc : 96.6600%\n",
      "Epoch : 60 / Loss : 0.0811 / Train Acc : 97.6767% / Test Acc : 96.8900%\n",
      "Epoch : 61 / Loss : 0.0792 / Train Acc : 97.6317% / Test Acc : 96.6800%\n",
      "Epoch : 62 / Loss : 0.0776 / Train Acc : 97.7367% / Test Acc : 96.8900%\n",
      "Epoch : 63 / Loss : 0.0759 / Train Acc : 97.7433% / Test Acc : 96.8800%\n",
      "Epoch : 64 / Loss : 0.0745 / Train Acc : 97.7967% / Test Acc : 96.9900%\n",
      "Epoch : 65 / Loss : 0.0731 / Train Acc : 97.8717% / Test Acc : 97.0300%\n",
      "Epoch : 66 / Loss : 0.0715 / Train Acc : 97.9267% / Test Acc : 97.0200%\n",
      "Epoch : 67 / Loss : 0.0701 / Train Acc : 97.9483% / Test Acc : 96.9900%\n",
      "Epoch : 68 / Loss : 0.0686 / Train Acc : 97.9933% / Test Acc : 97.1100%\n",
      "Epoch : 69 / Loss : 0.0674 / Train Acc : 98.0750% / Test Acc : 97.1100%\n",
      "Epoch : 70 / Loss : 0.0659 / Train Acc : 98.0617% / Test Acc : 97.0300%\n",
      "Epoch : 71 / Loss : 0.0646 / Train Acc : 98.1333% / Test Acc : 97.1100%\n",
      "Epoch : 72 / Loss : 0.0633 / Train Acc : 98.1533% / Test Acc : 97.1300%\n",
      "Epoch : 73 / Loss : 0.0620 / Train Acc : 98.2017% / Test Acc : 97.1400%\n",
      "Epoch : 74 / Loss : 0.0607 / Train Acc : 98.2583% / Test Acc : 97.1200%\n",
      "Epoch : 75 / Loss : 0.0596 / Train Acc : 98.2950% / Test Acc : 97.1500%\n",
      "Epoch : 76 / Loss : 0.0584 / Train Acc : 98.2950% / Test Acc : 97.1900%\n",
      "Epoch : 77 / Loss : 0.0574 / Train Acc : 98.3883% / Test Acc : 97.2500%\n",
      "Epoch : 78 / Loss : 0.0562 / Train Acc : 98.4017% / Test Acc : 97.3300%\n",
      "Epoch : 79 / Loss : 0.0551 / Train Acc : 98.4733% / Test Acc : 97.2100%\n",
      "Epoch : 80 / Loss : 0.0541 / Train Acc : 98.4983% / Test Acc : 97.2700%\n",
      "Epoch : 81 / Loss : 0.0530 / Train Acc : 98.5333% / Test Acc : 97.3500%\n",
      "Epoch : 82 / Loss : 0.0520 / Train Acc : 98.5317% / Test Acc : 97.2700%\n",
      "Epoch : 83 / Loss : 0.0509 / Train Acc : 98.6033% / Test Acc : 97.3800%\n",
      "Epoch : 84 / Loss : 0.0499 / Train Acc : 98.6200% / Test Acc : 97.4300%\n",
      "Epoch : 85 / Loss : 0.0491 / Train Acc : 98.6467% / Test Acc : 97.3800%\n",
      "Epoch : 86 / Loss : 0.0483 / Train Acc : 98.7217% / Test Acc : 97.4200%\n",
      "Epoch : 87 / Loss : 0.0472 / Train Acc : 98.6933% / Test Acc : 97.4500%\n",
      "Epoch : 88 / Loss : 0.0464 / Train Acc : 98.6550% / Test Acc : 97.3900%\n",
      "Epoch : 89 / Loss : 0.0454 / Train Acc : 98.7750% / Test Acc : 97.4400%\n",
      "Epoch : 90 / Loss : 0.0446 / Train Acc : 98.7583% / Test Acc : 97.4200%\n",
      "Epoch : 91 / Loss : 0.0439 / Train Acc : 98.8367% / Test Acc : 97.4600%\n",
      "Epoch : 92 / Loss : 0.0429 / Train Acc : 98.8100% / Test Acc : 97.5400%\n",
      "Epoch : 93 / Loss : 0.0423 / Train Acc : 98.8700% / Test Acc : 97.5500%\n",
      "Epoch : 94 / Loss : 0.0415 / Train Acc : 98.8183% / Test Acc : 97.3900%\n",
      "Epoch : 95 / Loss : 0.0408 / Train Acc : 98.8567% / Test Acc : 97.5100%\n",
      "Epoch : 96 / Loss : 0.0401 / Train Acc : 98.9767% / Test Acc : 97.6000%\n",
      "Epoch : 97 / Loss : 0.0393 / Train Acc : 98.9133% / Test Acc : 97.5600%\n",
      "Epoch : 98 / Loss : 0.0384 / Train Acc : 98.9767% / Test Acc : 97.5900%\n",
      "Epoch : 99 / Loss : 0.0378 / Train Acc : 98.9783% / Test Acc : 97.5900%\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from data.mnist import load_mnist\n",
    "\n",
    "# 데이터 mnist데이터 셋에서 onehot데이터 load\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = MLP(input_size=784, hidden_size_1=256, hidden_size_2=128, output_size=10)\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "idx_arr = np.arange(len(x_train))\n",
    "\n",
    "batch_size = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    train_loss_list = []\n",
    "    np.random.shuffle(idx_arr)\n",
    "    for idx in idx_arr.reshape(-1, batch_size):\n",
    "        \n",
    "        x_batch, t_batch = x_train[idx], t_train[idx]\n",
    "        grad = network.gradient(x_batch, t_batch)\n",
    "        \n",
    "        for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "            network.params[key] -= learning_rate * grad[key]\n",
    "            \n",
    "        loss = network.loss(x_batch, t_batch)\n",
    "        train_loss_list.append(loss)\n",
    "    \n",
    "    train_acc = network.accuracy(x_train, t_train)\n",
    "    test_acc = network.accuracy(x_test, t_test)\n",
    "    \n",
    "    print('Epoch : {} / Loss : {:.4f} / Train Acc : {:.4f}% / Test Acc : {:.4f}%'.format(epoch_i, np.mean(train_loss_list),\n",
    "                                                                                         train_acc*100, test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.10218333333333333 test accuracy: 0.101\n",
      "train accuracy: 0.4860333333333333 test accuracy: 0.4879\n",
      "train accuracy: 0.8671 test accuracy: 0.8727\n",
      "train accuracy: 0.9166333333333333 test accuracy: 0.9151\n",
      "train accuracy: 0.93705 test accuracy: 0.934\n",
      "train accuracy: 0.9472833333333334 test accuracy: 0.9456\n",
      "train accuracy: 0.95495 test accuracy: 0.9519\n",
      "train accuracy: 0.96425 test accuracy: 0.9581\n",
      "train accuracy: 0.9664166666666667 test accuracy: 0.9609\n",
      "train accuracy: 0.9686166666666667 test accuracy: 0.9616\n",
      "train accuracy: 0.9732166666666666 test accuracy: 0.9659\n",
      "train accuracy: 0.97435 test accuracy: 0.966\n",
      "train accuracy: 0.97755 test accuracy: 0.9688\n",
      "train accuracy: 0.9775166666666667 test accuracy: 0.9675\n",
      "train accuracy: 0.9807 test accuracy: 0.968\n",
      "train accuracy: 0.9837166666666667 test accuracy: 0.9716\n",
      "train accuracy: 0.9835 test accuracy: 0.9681\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from data.mnist import load_mnist\n",
    "\n",
    "# 데이터 mnist데이터 셋에서 onehot데이터 load\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = MLP(input_size=784, hidden_size_1=50, hidden_size_2 = 25,output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)   # train_size = 60000, batch_size = 100, 60000개 중 임의 100개 \n",
    "    x_batch = x_train[batch_mask]   \n",
    "    t_batch = t_train[batch_mask] \n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch) \n",
    "    train_loss_list.append(loss) \n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train accuracy:\", train_acc,\"test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "np.random.choice(train_size, batch_size)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
