{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8. 딥러닝\n",
    "* 딥러닝 : 층을 깊게 한 심층 신경망\n",
    "\n",
    "#### 8.1 더 깊게 \n",
    "* 더 깊은 신경망으로 : 손글씨 숫자를 인식하는 심층 CNN\n",
    "    * 합성곱 계층(Conv) : 모두 3 x 3크기의 작은 필터, 층이 깊어지면서 채널수가 더 늘어남(채널수 : 16,16,32,32,64,64)\n",
    "    * 풀링 계층(Pool) : 중간 데이터의 공간 크기를 점차 줄여감\n",
    "    * 드롭아웃(Dropout) : 마지막 단 완전 연결 계층에서 드롭아웃 계층을 사용\n",
    "    * 활성화 함수 : ReLU, 초기값 : He초기값\n",
    "    * 가중치 매개병수 갱신 : Adam을 사용해 최적화\n",
    "* ![nn](deep_cnn.jpeg)\n",
    "\n",
    "* What is the class of this image? : 다양한 데이터셋을 대상으로 그동안 논문 등에서 발표한 기법들의 정확도 순위를 정리한 사이트\n",
    "    * 정확도를 더 높일 수 있는 기술이나 힌트를 발견할 수 있음\n",
    "        * 앙상블 학습\n",
    "        * 학습률 감소\n",
    "        * 데이터 확장 : 손쉽고 정확도 개선에 효과적\n",
    "\n",
    "* 데이터 확장(data augmentation) : 입력 이미지(훈련 이미지)를 알고리즘을 동원해 '인위적'으로 확장\n",
    "    * 입력 이미지를 회전하거나 세로로 이동, 미세한 변화를 주어 이미지의 개수를 늘림\n",
    "    * 데이터가 부족할 때 특히 효과적\n",
    "    * crop : 이미지 일부를 잘라냄\n",
    "    * flip : 이미지 좌우를 뒤집음\n",
    "    * 밝기 등의 외형 변화, 확대, 축소 등의 스케일 변화도 효과적\n",
    "\n",
    "* 층을 깊게 하는 이유 \n",
    "    * 이미지 인식 대회의 결과 : 상위권을 차지한 기법 대부분이 딥러닝 기반, 신경망을 더 깊게 만드는 방향으로 가고 있음\n",
    "    * 층을 깊게 할 때의 이점 : 신경망의 매개변수 수가 줄어듬 -> 넓은 수용 영역(receptive field)을 소화 가능\n",
    "        * 층을 깊게 한 신경망은 깊지 않은 경우보다 적은 매개변수로 같거나 이상의 표현력을 달성할 수 있음\n",
    "            * ex. 3 x 3의 합성곱 연산을 2회 수행으로 5 x 5의 합성곱 연산 1회를 대체 가능\n",
    "                * 3 x 3 연산 2회 : 매개변수(2x3x3=18)\n",
    "                * 5 x 5 연산 1회 : 매개변수(5x5=25)\n",
    "        * 수용영역 : 뉴런에 변화를 일으키는 국소적인 공간 영역\n",
    "        * 층을 거듭 할 수록 ReLU등의 활성화 함수를 계층사이에 배치함으로써 신경망의 표현력이 개선됨\n",
    "            * 활성화 함수가 신경망에 '비선형' 힘을 가하고, 비선형 함수가 겹치면서 더 복잡한 것도 표현할 수 있게 되기 때문\n",
    "        * 학습의 효율성 : 학습 데이터의 양을 줄여 학습을 고속으로 수행할 수 있음\n",
    "            * 층이 깊어 질 수록 단순한 것 보다 복잡한 것(텍스처, 사물의 일부)에 반응\n",
    "        * 신경망을 깊게 하면 학습할 문제를 계층적으로 분해할 수 있음\n",
    "            * 각 층이 학습해야 할 문제를 더 단순한 문제로 대체\n",
    "                * ex. 처음 층에서는 개가 등장하는 이미지보다 에지를 포함한 이미지가 많기 때문에 에지 학습을 먼저 실시\n",
    "            * 정보를 계층적으로 전달할 수 있음\n",
    "                * ex. 에지를 추출한 층의 다음 층은 에지 정보를 쓸 수 있고, 더 고도의 패턴을 효과적으로 학습할 것으로 기대 \n",
    "                * 풀기 쉬운 단순한 문제로 분해, 효율적인 학습\n",
    "        * 층의 심화는 층이 깊어도 제대로 학습할 수 있도록 해주는 새로운 기술과 환경(빅데이터, 컴퓨터 연산 능력)이 뒷받침되어 나타난 현상\n",
    "                \n",
    "                \n",
    "#### 8.2 딥러닝의 초기 역사\n",
    "* 딥러닝이 주목을 받게 된 계기 : ILSVRC(imageNet Large Scale Visual Recognition Challenge)에서 AlexNet의 우승\n",
    "* 이미지넷 : 100만 짱이 넘는 이미지를 담고 있는 데이터 셋, ILSVRC 대회에서 사용 \n",
    "    * 시험 항목\n",
    "        * 분류 : 1000개의 클래스를 제대로 분류하는지를 겨룸\n",
    "* VGG : 합성곱 계층과 풀링 계층으로 구성되는 기본적인 CNN \n",
    "    * VGG16/VGG19 : 비중 있는 층(합성곱 계층, 완전연결 계층)을 모두 16층(혹은 19층)으로 심화한 것이 특징\n",
    "    * 3 x 3의 작은 필터를 사용한 합성곱 계층을 연속으로 거침\n",
    "        * 합성곱 계층을 2~4회 연속으로 풀링 계층을 두어 크기를 절반으로 줄이는 처리를 반복\n",
    "        * 마지막에는 완전 연결 계층을 통과시켜 결과를 출력\n",
    "* GoogLeNet : 사각형이 합성곱 계층과 풀링 계층 등의 계층을 나타냄\n",
    "    * 세로 방향 깊이뿐 아니라 가로 방향도 깊다는 점이 특징\n",
    "        * 가로 방향에 '폭'이 존재 : 인셉션 구조\n",
    "        * 인셉션 구조 : 크기가 다른 필터(와 풀링)을 여러개 적용하여 그 결과를 결합\n",
    "    * 인셉션 구조를 하나의 빌딩 블록(구성요소)로 사용\n",
    "    * 1 x 1 의 합성곱 계층을 많은 곳에서 사용\n",
    "        * 채널 쪽으로 크기를 줄이는 역할 , 매개변수 제거와 고속처리에 기여\n",
    "* ResNet(Residual Network) : 마이크로소프트의 팀이 개발한 네트워크\n",
    "    * 층이 지나치게 깊으면 학습이 잘 되지 않고 성능이 떨어지는 경우가 발생 : '스킵 연결'으로 해결\n",
    "    * 스킵 연결(skip connection) : 입력 데이터를 합성곱 계층 까지 건너 뛰어 출력에 바로 더하는 구조 \n",
    "        * 층의 깊이에 비례해 성능을 향상시킬 수 있게 한 핵심\n",
    "        * F(x) -> F(x) + x\n",
    "        * 역전파 때도 상류의 기울기를 그대로 흘려보냄\n",
    "    * VGG 신경망 기반으로 스킵연결을 도입하여 층을 깊게함\n",
    "    \n",
    "* 이미지넷이 제공하는 거대한 데이터셋으로 학습한 가중치 값들은 실제 제품에 활용해도 효과적이고, 실제로 많이 이용\n",
    "    * 이를 전이학습(transfer learning) : 학습된 가중치(혹은 그 일부)를 다른 신경망에 복사한 다음, 그상태로 재학습을 수행\n",
    "    * 구성이 같은 신경망을 준비, 미리 학습된 가중치를 초깃값으로 설정, 새로운 데이터 셋을 대상으로 재학습(fine tuning)을 수행\n",
    "    * 보유한 데이터 셋이 적을 때 특히 유용 \n",
    "    \n",
    "#### 8.3 더 빠르게 (딥러닝 고속화)\n",
    "* 딥러닝 프레임워크 대부분은 GPU(Graphics Proccessing Unit)을 활용, 대량의 연산을 고속으로 처리\n",
    "    * 최근 프레임 워크 : 학습을 복수의 GPU와 여러 기기로 분산 수행 \n",
    "    * AlexNet에서는 오랜 시간을 합성곱 계층에서 소요\n",
    "        * 합성곱 계층에서 이뤄지는 연산을 어떻게 고속으로 효율적으로 하느냐가 딥러닝의 과제\n",
    "    \n",
    "* GPU를 활용한 고속화 : 과거에는 그래픽 전용 보드에 이용했지만 최근에는 범용 수치 연산에도 사용 \n",
    "    * GPU : 병렬 수치 연산을 고속으로 처리할 수 있음\n",
    "    * GPU 컴퓨팅 : GPU로 범용 수치 연산을 수행 \n",
    "        * CPU는 연속적인 복잡한 계산을 잘 처리하지만 GPU는 대량 병렬 연산을 잘 처리\n",
    "    * GPU는 주로 엔비디아, AMD 두 회사가 제공\n",
    "        * 대부분의 딥러닝 프레임워크는 엔비디아 GPU에서만 혜택을 받을 수 있음\n",
    "            * 엔비디아의 GPU 컴퓨팅용 통합 개발 환경인 CUDA를 사용하기 때문 \n",
    "    * cuDNN : CUDA 위에서 동작하는 라이브러리, 딥러닝에 최적화된 함수 등이 구현되어 있음\n",
    "        \n",
    "* 분산 학습 : 1회 학습에 걸리는 시간을 최대한 단축하기 위해서 딥러닝 학습을 수평확장(scale out)\n",
    "    * 다수의 GPU와 기기로 계산을 분산\n",
    "    * 컴퓨터 사이의 통신, 데이터 동기화, 쉽게 해결할 수 없는 문제들이 존재\n",
    "        * 텐서 플로와 같은 프레임워크에 맡기는 것이 좋음\n",
    "    \n",
    "* 연산 정밀도와 비트 줄이기 : 메모리 용량과 버스 대역폭 등이 딥러닝 고속화에 병목이 될 수 있을 때 조치\n",
    "    * 메모리 용량 : 대량의 가중치 매개변수와 중간 데이터를 저장해야함\n",
    "    * 버스 대역폭 : GPU(CPU)의 버스를 흐르는 데이터가 많아져 한계를 넘어서면 병목이 발생\n",
    "    * 네트워크로 주고받는 데이터의 비트 수를 최소로 만드는 것이 바람직함\n",
    "        * 주로 64비트나 32비트 부동소수점 수를 사용해 실수를 표현\n",
    "        * 많은 비트를 사용할수록 계산 오차는 줄어들지만, 비용과 메모리 사용량이 늘고 버스 대역폭에 부담을 줌\n",
    "        * 딥러닝은 많은 수치 정밀도(수치를 몇 비트로 표현하느냐)를 요구하지 않음 : 신경망의 견고성에 따른 특성\n",
    "            * 입력이미지에 노이즈가 조금 섞여 있어도 출력 결과가 잘 달라지지 않음\n",
    "            * 신경망을 흐르는 데이터를 퇴화시켜도 출력에 주는 영향이 적음\n",
    "    * 컴퓨터에서 실수를 표현하는 방식\n",
    "        * 32비트 단정밀도, 64비트 배정밀도, 부동소수점 등\n",
    "        * 16비트 단정밀도만 사용해도 학습에 문제가 없다고 알려져 있음 \n",
    "    * 파이썬에서는 일반적으로 64비트 배정밀도 부동소수점 수를 사용\n",
    "        * 넘파이의 경우 16비트 반정밀도 부동소수점을 지원\n",
    "    * Binarized Neural Network : 최근에 등장한 가중치와 중간 데이터를 1비트로 표현하는 방법\n",
    "    * 딥러닝의 비트 수를 줄이는 연구는 딥러닝을 임베디드 용으로 이용할 때 중요한 주제! \n",
    "\n",
    "#### 8.4 딥러닝의 활용\n",
    "* 딥러닝의 활용 : 사물인식, 이미지, 음성, 자연어 등 \n",
    "* 컴퓨터 비전 분야에서의 활용\n",
    "    * 사물 검출 : 이미지 속에 담긴 사물의 위치와 종류(클래스)를 알아내는 기술\n",
    "        * R-CNN(Region with Convolutional Neural Network)이 유명\n",
    "        * Faster R-CNN\n",
    "    * 분할(segmentation) : 이미지를 픽셀 수준에서 분류하는 문제\n",
    "        * FCN(Fully Convolutional Network) : 단 한번의 forward 처리로 모든 픽셀의 클래스를 분류해주는 기법\n",
    "    * 사진 캡션 생성 : 컴퓨터 비전과 자연어를 융합한 연구\n",
    "        * 사진 캡션 문장을 자동으로 생성\n",
    "        * NIC(Neural Iamage Caption)모델이 대표적 : 순환신경망(Recurrent Neural Network, RNN)과 CNN을 조합한 구성 \n",
    "        * RNN : 자연어, 시계열, 데이터 등의 연속된 데이터를 다룰 때 많이 활용\n",
    "        * 멀티 모달 처리(multimodal processing) : 사진이나 자연어와 같은 여러 종류의 정보를 조합하고 처리\n",
    "    \n",
    "#### 8.5 딥러닝의 미래\n",
    "* 딥러닝의 가능성과 미래를 느낄 만한 연구\n",
    "    * 이미지 스타일(화풍) 변환\n",
    "        * 콘텐츠 이미지, 스타일 이미지를 조합해 새로운 그림을 그림\n",
    "        * 관련 논문 : A Neural Algorithm of Artistic Style\n",
    "        * 네트워크의 중간 데이터가 콘텐츠 이미지의 중간 데이터와 비슷해지도록 학습 -> 입력 이미지를 콘텐츠 이미지의 형태로 흉내냄\n",
    "        * 스타일 행렬 : 스타일 이미지의 화풍을 흡수하기 위한 개념, 스타일 행렬의 오차를 줄이도록 학습하여 입력이미지의 스타일을 비슷하게 만듬\n",
    "    * 이미지 생성\n",
    "        * 아무런 입력 이미지 없이도 새로운 이미지를 그려내는 연구 \n",
    "            * 대량의 이미지를 사용하여 학습한 후 , 입력 이미지 없이 새로운 이미지를 생성\n",
    "            * 비지도 학습(unsupervised learning) : 정답 레이블(지도용 데이터)이 주어져 있지 않음\n",
    "        * DCGAN(Deep Convolutional Generative Adversarial Network)\n",
    "        * GAN(Generative Adeversarial Network)\n",
    "            * 생성자(Generator) : 진짜와 똑같은 이미지를 생성\n",
    "            * 식별자(Discriminator) : 그것이 진짜인지 판정(생성자가 생성한 이미지인지, 아니면 실제로 촬영된 이미지인지)\n",
    "            * 둘을 서로 겨루도록 학습, 생성자는 더 정교해지고 식별자는 더 정확하게 성장\n",
    "            \n",
    "    * 자율 주행\n",
    "        * 경로 계획 : 주행 경로를 정하는 기술(path plan)\n",
    "        * 탐사 기술 : 카메라, 레이저 등을 이용한 기술\n",
    "        * 주위의 환경을 인식하는 기술이 가장 중요 \n",
    "            * SegNet : CNN기반 신경망, 주변 환경을 인식하는데 사용\n",
    "           \n",
    "    * Deep Q-Network(DQN)\n",
    "        * 강화학습(reinforcement learning) : 시행착오를 겪으며 스스로 학습\n",
    "            * 가르침에 의존하는 지도 학습과는 다름\n",
    "                * 에이전트가 더 좋은 보상을 받기 위해 스스로 학습\n",
    "            * 에이전트 : 환경에 맞게 행동을 선택함, 그 행동에 의해서 환경이 변함\n",
    "                * 환경이 변화하면 에이전트가 보상을 얻음 \n",
    "                * 더 나은 보상을 받는 쪽으로 행동 지침을 바로 잡음\n",
    "                    * 보상 : 정해진 것이 아닌 예상 보상 \n",
    "        * DQN : Q학습이라는 강화학습 알고리즘을 사용한 딥러닝 알고리즘\n",
    "            * 딥러닝(CNN)으로 Q학습을 흉내내어 비슷하게 사용하는 것\n",
    "            * Q학습 : 최적 행동 가치 함수로 최적인 행동을 정함\n",
    "            * 입력 데이터 : 비디오, 게임의 영상\n",
    "            * 각 움직임, 제어, 동작의 가치를 출력해서 더 나은 게임 조작을 실현\n",
    "        * 알파고 : 구글의 DeepMind가 진행한 연구 \n",
    "#### 8.6 정리\n",
    "* 수 많은 문제에서 신경망을 더 깊게 하여 성능을 개선할 수 있다\n",
    "* 이미지 인식 기술 대회인 ILSVRC에서는 최근 딥러닝 기반 기법이 상위권을 독점하고 있으며, 그 깊이도 더 깊어지는 추세\n",
    "* 유명한 신경망으로는 VGG, GoogLeNet, ResNet이 있음\n",
    "* GPU와 분산 학습, 비트 정밀도 감소 등으로 딥러닝을 고속화 할 수 있다.\n",
    "* 딥러닝(신경망)은 사물 인식뿐 아니라 사물 검출과 분할에도 이용 가능\n",
    "* 딥러닝의 응용 분야로는 사진의 캡션 생성, 이미지 생성, 강화학습, 자율주행 등이 존재"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
