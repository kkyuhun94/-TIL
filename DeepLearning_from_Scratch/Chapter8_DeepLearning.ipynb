{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8. 딥러닝\n",
    "* 딥러닝 : 층을 깊게 한 심층 신경망\n",
    "\n",
    "#### 8.1 더 깊게 \n",
    "* 더 깊은 신경망으로 : 손글씨 숫자를 인식하는 심층 CNN\n",
    "    * 합성곱 계층(Conv) : 모두 3 x 3크기의 작은 필터, 층이 깊어지면서 채널수가 더 늘어남(채널수 : 16,16,32,32,64,64)\n",
    "    * 풀링 계층(Pool) : 중간 데이터의 공간 크기를 점차 줄여감\n",
    "    * 드롭아웃(Dropout) : 마지막 단 완전 연결 계층에서 드롭아웃 계층을 사용\n",
    "    * 활성화 함수 : ReLU, 초기값 : He초기값\n",
    "    * 가중치 매개병수 갱신 : Adam을 사용해 최적화\n",
    "* ![nn](deep_cnn.jpeg)\n",
    "\n",
    "* What is the class of this image? : 다양한 데이터셋을 대상으로 그동안 논문 등에서 발표한 기법들의 정확도 순위를 정리한 사이트\n",
    "    * 정확도를 더 높일 수 있는 기술이나 힌트를 발견할 수 있음\n",
    "        * 앙상블 학습\n",
    "        * 학습률 감소\n",
    "        * 데이터 확장 : 손쉽고 정확도 개선에 효과적\n",
    "\n",
    "* 데이터 확장(data augmentation) : 입력 이미지(훈련 이미지)를 알고리즘을 동원해 '인위적'으로 확장\n",
    "    * 입력 이미지를 회전하거나 세로로 이동, 미세한 변화를 주어 이미지의 개수를 늘림\n",
    "    * 데이터가 부족할 때 특히 효과적\n",
    "    * crop : 이미지 일부를 잘라냄\n",
    "    * flip : 이미지 좌우를 뒤집음\n",
    "    * 밝기 등의 외형 변화, 확대, 축소 등의 스케일 변화도 효과적\n",
    "\n",
    "* 층을 깊게 하는 이유 \n",
    "    * 이미지 인식 대회의 결과 : 상위권을 차지한 기법 대부분이 딥러닝 기반, 신경망을 더 깊게 만드는 방향으로 가고 있음\n",
    "    * 층을 깊게 할 때의 이점 : 신경망의 매개변수 수가 줄어듬 -> 넓은 수용 영역(receptive field)을 소화 가능\n",
    "        * 층을 깊게 한 신경망은 깊지 않은 경우보다 적은 매개변수로 같거나 이상의 표현력을 달성할 수 있음\n",
    "            * ex. 3 x 3의 합성곱 연산을 2회 수행으로 5 x 5의 합성곱 연산 1회를 대체 가능\n",
    "                * 3 x 3 연산 2회 : 매개변수(2x3x3=18)\n",
    "                * 5 x 5 연산 1회 : 매개변수(5x5=25)\n",
    "        * 수용영역 : 뉴런에 변화를 일으키는 국소적인 공간 영역\n",
    "        * 층을 거듭 할 수록 ReLU등의 활성화 함수를 계층사이에 배치함으로써 신경망의 표현력이 개선됨\n",
    "            * 활성화 함수가 신경망에 '비선형' 힘을 가하고, 비선형 함수가 겹치면서 더 복잡한 것도 표현할 수 있게 되기 때문\n",
    "        * 학습의 효율성 : 학습 데이터의 양을 줄여 학습을 고속으로 수행할 수 있음\n",
    "            * 층이 깊어 질 수록 단순한 것 보다 복잡한 것(텍스처, 사물의 일부)에 반응\n",
    "        * 신경망을 깊게 하면 학습할 문제를 계층적으로 분해할 수 있음\n",
    "            * 각 층이 학습해야 할 문제를 더 단순한 문제로 대체\n",
    "                * ex. 처음 층에서는 개가 등장하는 이미지보다 에지를 포함한 이미지가 많기 때문에 에지 학습을 먼저 실시\n",
    "            * 정보를 계층적으로 전달할 수 있음\n",
    "                * ex. 에지를 추출한 층의 다음 층은 에지 정보를 쓸 수 있고, 더 고도의 패턴을 효과적으로 학습할 것으로 기대 \n",
    "                * 풀기 쉬운 단순한 문제로 분해, 효율적인 학습\n",
    "        * 층의 심화는 층이 깊어도 제대로 학습할 수 있도록 해주는 새로운 기술과 환경(빅데이터, 컴퓨터 연산 능력)이 뒷받침되어 나타난 현상\n",
    "                \n",
    "                \n",
    "#### 8.2 딥러닝의 초기 역사\n",
    "* 딥러닝이 주목을 받게 된 계기 : ILSVRC(imageNet Large Scale Visual Recognition Challenge)에서 AlexNet의 우승\n",
    "* 이미지넷 : 100만 짱이 넘는 이미지를 담고 있는 데이터 셋, ILSVRC 대회에서 사용 \n",
    "    * 시험 항목\n",
    "        * 분류 : 1000개의 클래스를 제대로 분류하는지를 겨룸\n",
    "* VGG : 합성곱 계층과 풀링 계층으로 구성되는 기본적인 CNN \n",
    "    * VGG16/VGG19 : 비중 있는 층(합성곱 계층, 완전연결 계층)을 모두 16층(혹은 19층)으로 심화한 것이 특징\n",
    "    * 3 x 3의 작은 필터를 사용한 합성곱 계층을 연속으로 거침\n",
    "        * 합성곱 계층을 2~4회 연속으로 풀링 계층을 두어 크기를 절반으로 줄이는 처리를 반복\n",
    "        * 마지막에는 완전 연결 계층을 통과시켜 결과를 출력\n",
    "* GoogLeNet : 사각형이 합성곱 계층과 풀링 계층 등의 계층을 나타냄\n",
    "    * 세로 방향 깊이뿐 아니라 가로 방향도 깊다는 점이 특징\n",
    "        * 가로 방향에 '폭'이 존재 : 인셉션 구조\n",
    "        * 인셉션 구조 : 크기가 다른 필터(와 풀링)을 여러개 적용하여 그 결과를 결합\n",
    "    * 인셉션 구조를 하나의 빌딩 블록(구성요소)로 사용\n",
    "    * 1 x 1 의 합성곱 계층을 많은 곳에서 사용\n",
    "        * 채널 쪽으로 크기를 줄이는 역할 , 매개변수 제거와 고속처리에 기여\n",
    "* ResNet(Residual Network) : 마이크로소프트의 팀이 개발한 네트워크\n",
    "    * 층이 지나치게 깊으면 학습이 잘 되지 않고 성능이 떨어지는 경우가 발생 : '스킵 연결'으로 해결\n",
    "    * 스킵 연결(skip connection) : 입력 데이터를 합성곱 계층 까지 건너 뛰어 출력에 바로 더하는 구조 \n",
    "        * 층의 깊이에 비례해 성능을 향상시킬 수 있게 한 핵심\n",
    "        * F(x) -> F(x) + x\n",
    "        * 역전파 때도 상류의 기울기를 그대로 흘려보냄\n",
    "    * VGG 신경망 기반으로 스킵연결을 도입하여 층을 깊게함\n",
    "    \n",
    "* 이미지넷이 제공하는 거대한 데이터셋으로 학습한 가중치 값들은 실제 제품에 활용해도 효과적이고, 실제로 많이 이용\n",
    "    * 이를 전이학습(transfer learning) : 학습된 가중치(혹은 그 일부)를 다른 신경망에 복사한 다음, 그상태로 재학습을 수행\n",
    "    * 구성이 같은 신경망을 준비, 미리 학습된 가중치를 초깃값으로 설정, 새로운 데이터 셋을 대상으로 재학습(fine tuning)을 수행\n",
    "    * 보유한 데이터 셋이 적을 때 특히 유용 \n",
    "    \n",
    "#### 8.3 더 빠르게 (딥러닝 고속화)\n",
    "* 딥러닝 프레임워크 대부분은 GPU(Graphics Proccessing Unit)을 활용, 대량의 연산을 고속으로 처리\n",
    "    * 최근 프레임 워크 : 학습을 복수의 GPU와 여러 기기로 분산 수행 \n",
    "    * AlexNet에서는 오랜 시간을 합성곱 계층에서 소요\n",
    "        * 합성곱 계층에서 이뤄지는 연산을 어떻게 고속으로 효율적으로 하느냐가 딥러닝의 과제\n",
    "    \n",
    "* GPU를 활용한 고속화 : 과거에는 그래픽 전용 보드에 이용했지만 최근에는 범용 수치 연산에도 사용 \n",
    "    * GPU : 병렬 수치 연산을 고속으로 처리할 수 있음\n",
    "    * GPU 컴퓨팅 : GPU로 범용 수치 연산을 수행 \n",
    "        * CPU는 연속적인 복잡한 계산을 잘 처리하지만 GPU는 대량 병렬 연산을 잘 처리\n",
    "    * GPU는 주로 엔비디아, AMD 두 회사가 제공\n",
    "        * 대부분의 딥러닝 프레임워크는 엔비디아 GPU에서만 혜택을 받을 수 있음\n",
    "            * 엔비디아의 GPU 컴퓨팅용 통합 개발 환경인 CUDA를 사용하기 때문 \n",
    "    * cuDNN : CUDA 위에서 동작하는 라이브러리, 딥러닝에 최적화된 함수 등이 구현되어 있음\n",
    "        \n",
    "* 분산 학습 : 1회 학습에 걸리는 시간을 최대한 단축하기 위해서 딥러닝 학습을 수평확장(scale out)\n",
    "    * 다수의 GPU와 기기로 계산을 분산\n",
    "    * 컴퓨터 사이의 통신, 데이터 동기화, 쉽게 해결할 수 없는 문제들이 존재\n",
    "        * 텐서 플로와 같은 프레임워크에 맡기는 것이 좋음\n",
    "    \n",
    "* 연산 정밀도와 비트 줄이기 :  \n",
    "\n",
    "#### 8.4 딥러닝의 활용\n",
    "* 사물 검출\n",
    "* 분할\n",
    "* 사진 캡션 생성\n",
    "\n",
    "#### 8.5 딥러닝의 미래\n",
    "* 이미지 스타일(화풍) 변환\n",
    "* 이미지 생성\n",
    "* 자율 주행\n",
    "* Deep Q-Network(강화학습)\n",
    "\n",
    "#### 8.6 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
